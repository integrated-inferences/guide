<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Identification | Causal Models: Guide to CausalQueries</title>
  <meta name="description" content="Model based strategies for integrating qualitative and quantitative inferences." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Identification | Causal Models: Guide to CausalQueries" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Model based strategies for integrating qualitative and quantitative inferences." />
  <meta name="github-repo" content="rstudio/causalmodels" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Identification | Causal Models: Guide to CausalQueries" />
  
  <meta name="twitter:description" content="Model based strategies for integrating qualitative and quantitative inferences." />
  

<meta name="author" content="Macartan Humphreys and Alan Jacobs" />


<meta name="date" content="2020-06-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="process-tracing.html"/>
<link rel="next" href="mixing-methods.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="headers\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Model based causal inference: A guide to gbiqq</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Causal Models</b></span></li>
<li class="chapter" data-level="1" data-path="cm.html"><a href="cm.html"><i class="fa fa-check"></i><b>1</b> What and why</a><ul>
<li class="chapter" data-level="1.1" data-path="cm.html"><a href="cm.html#two-approaches-to-inference"><i class="fa fa-check"></i><b>1.1</b> Two approaches to inference</a></li>
<li class="chapter" data-level="1.2" data-path="cm.html"><a href="cm.html#recovering-the-ate-with-difference-in-means"><i class="fa fa-check"></i><b>1.2</b> Recovering the ATE with Difference in Means</a></li>
<li class="chapter" data-level="1.3" data-path="cm.html"><a href="cm.html#recovering-the-ate-with-a-causal-model"><i class="fa fa-check"></i><b>1.3</b> Recovering the ATE with a Causal Model</a></li>
<li class="chapter" data-level="1.4" data-path="cm.html"><a href="cm.html#going-further"><i class="fa fa-check"></i><b>1.4</b> Going further</a></li>
</ul></li>
<li class="part"><span><b>II The Package</b></span></li>
<li class="chapter" data-level="2" data-path="package.html"><a href="package.html"><i class="fa fa-check"></i><b>2</b> Installation</a></li>
<li class="chapter" data-level="3" data-path="defining-models.html"><a href="defining-models.html"><i class="fa fa-check"></i><b>3</b> Defining models</a><ul>
<li class="chapter" data-level="3.1" data-path="defining-models.html"><a href="defining-models.html#getting-going"><i class="fa fa-check"></i><b>3.1</b> Getting going</a></li>
<li class="chapter" data-level="3.2" data-path="defining-models.html"><a href="defining-models.html#structure"><i class="fa fa-check"></i><b>3.2</b> Causal structure</a><ul>
<li class="chapter" data-level="3.2.1" data-path="defining-models.html"><a href="defining-models.html#nodal-types"><i class="fa fa-check"></i><b>3.2.1</b> Nodal types</a></li>
<li class="chapter" data-level="3.2.2" data-path="defining-models.html"><a href="defining-models.html#causal-types"><i class="fa fa-check"></i><b>3.2.2</b> Causal types</a></li>
<li class="chapter" data-level="3.2.3" data-path="defining-models.html"><a href="defining-models.html#parameters-dataframe"><i class="fa fa-check"></i><b>3.2.3</b> Parameters dataframe</a></li>
<li class="chapter" data-level="3.2.4" data-path="defining-models.html"><a href="defining-models.html#parameter-matrix"><i class="fa fa-check"></i><b>3.2.4</b> Parameter matrix</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="defining-models.html"><a href="defining-models.html#restrictions"><i class="fa fa-check"></i><b>3.3</b> Setting restrictions</a></li>
<li class="chapter" data-level="3.4" data-path="defining-models.html"><a href="defining-models.html#confounding"><i class="fa fa-check"></i><b>3.4</b> Allowing confounding</a></li>
<li class="chapter" data-level="3.5" data-path="defining-models.html"><a href="defining-models.html#priors"><i class="fa fa-check"></i><b>3.5</b> Setting Priors</a><ul>
<li class="chapter" data-level="3.5.1" data-path="defining-models.html"><a href="defining-models.html#custom-priors"><i class="fa fa-check"></i><b>3.5.1</b> Custom priors</a></li>
<li class="chapter" data-level="3.5.2" data-path="defining-models.html"><a href="defining-models.html#prior-warnings"><i class="fa fa-check"></i><b>3.5.2</b> Prior warnings</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="defining-models.html"><a href="defining-models.html#parameters"><i class="fa fa-check"></i><b>3.6</b> Setting Parameters</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="updating-models-with-stan.html"><a href="updating-models-with-stan.html"><i class="fa fa-check"></i><b>4</b> Updating models with <code>stan</code></a><ul>
<li class="chapter" data-level="4.1" data-path="updating-models-with-stan.html"><a href="updating-models-with-stan.html#data-for-stan"><i class="fa fa-check"></i><b>4.1</b> Data for <code>stan</code></a></li>
<li class="chapter" data-level="4.2" data-path="updating-models-with-stan.html"><a href="updating-models-with-stan.html#stan-code"><i class="fa fa-check"></i><b>4.2</b> <code>stan</code> code</a></li>
<li class="chapter" data-level="4.3" data-path="updating-models-with-stan.html"><a href="updating-models-with-stan.html#implementation"><i class="fa fa-check"></i><b>4.3</b> Implementation</a></li>
<li class="chapter" data-level="4.4" data-path="updating-models-with-stan.html"><a href="updating-models-with-stan.html#extensions"><i class="fa fa-check"></i><b>4.4</b> Extensions</a><ul>
<li class="chapter" data-level="4.4.1" data-path="updating-models-with-stan.html"><a href="updating-models-with-stan.html#arbitrary-parameters"><i class="fa fa-check"></i><b>4.4.1</b> Arbitrary parameters</a></li>
<li class="chapter" data-level="4.4.2" data-path="updating-models-with-stan.html"><a href="updating-models-with-stan.html#non-binary-data"><i class="fa fa-check"></i><b>4.4.2</b> Non binary data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="querying-models.html"><a href="querying-models.html"><i class="fa fa-check"></i><b>5</b> Querying models</a><ul>
<li class="chapter" data-level="5.1" data-path="querying-models.html"><a href="querying-models.html#case-level-queries"><i class="fa fa-check"></i><b>5.1</b> Case level queries</a></li>
<li class="chapter" data-level="5.2" data-path="querying-models.html"><a href="querying-models.html#posterior-queries"><i class="fa fa-check"></i><b>5.2</b> Posterior queries</a></li>
<li class="chapter" data-level="5.3" data-path="querying-models.html"><a href="querying-models.html#query-distribution"><i class="fa fa-check"></i><b>5.3</b> Query distribution</a></li>
<li class="chapter" data-level="5.4" data-path="querying-models.html"><a href="querying-models.html#token-and-general-causation"><i class="fa fa-check"></i><b>5.4</b> Token and general causation</a></li>
<li class="chapter" data-level="5.5" data-path="querying-models.html"><a href="querying-models.html#complex-queries"><i class="fa fa-check"></i><b>5.5</b> Complex queries</a></li>
</ul></li>
<li class="part"><span><b>III Applications</b></span></li>
<li class="chapter" data-level="6" data-path="applications.html"><a href="applications.html"><i class="fa fa-check"></i><b>6</b> Basic Models</a><ul>
<li class="chapter" data-level="6.1" data-path="applications.html"><a href="applications.html#the-ladder-of-causation-in-an-x-rightarrow-y-model"><i class="fa fa-check"></i><b>6.1</b> The ladder of causation in an <span class="math inline">\(X \rightarrow Y\)</span> model</a></li>
<li class="chapter" data-level="6.2" data-path="applications.html"><a href="applications.html#x-causes-y-with-unmodelled-confounding"><i class="fa fa-check"></i><b>6.2</b> <span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span>, with unmodelled confounding</a></li>
<li class="chapter" data-level="6.3" data-path="applications.html"><a href="applications.html#x-causes-y-with-confounding-modeled"><i class="fa fa-check"></i><b>6.3</b> <span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span>, with confounding modeled</a></li>
<li class="chapter" data-level="6.4" data-path="applications.html"><a href="applications.html#simple-mediation-model"><i class="fa fa-check"></i><b>6.4</b> Simple mediation model</a></li>
<li class="chapter" data-level="6.5" data-path="applications.html"><a href="applications.html#simple-moderator-model"><i class="fa fa-check"></i><b>6.5</b> Simple moderator model</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="explanation.html"><a href="explanation.html"><i class="fa fa-check"></i><b>7</b> Explanation</a><ul>
<li class="chapter" data-level="7.1" data-path="explanation.html"><a href="explanation.html#tightening-bounds-on-causes-of-effects-using-an-unobserved-covariate"><i class="fa fa-check"></i><b>7.1</b> Tightening bounds on causes of effects using an unobserved covariate</a></li>
<li class="chapter" data-level="7.2" data-path="explanation.html"><a href="explanation.html#Billy"><i class="fa fa-check"></i><b>7.2</b> Actual Causation: Billy and Suzy’s moderator and mediation model</a></li>
<li class="chapter" data-level="7.3" data-path="explanation.html"><a href="explanation.html#diagnosis-inferring-a-cause-from-symptoms"><i class="fa fa-check"></i><b>7.3</b> Diagnosis: Inferring a cause from symptoms</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="process-tracing.html"><a href="process-tracing.html"><i class="fa fa-check"></i><b>8</b> Process tracing</a><ul>
<li class="chapter" data-level="8.1" data-path="process-tracing.html"><a href="process-tracing.html#what-to-infer-from-what"><i class="fa fa-check"></i><b>8.1</b> What to infer from what</a></li>
<li class="chapter" data-level="8.2" data-path="process-tracing.html"><a href="process-tracing.html#probative-value-and-d-separation"><i class="fa fa-check"></i><b>8.2</b> Probative value and <span class="math inline">\(d\)</span>-separation</a></li>
<li class="chapter" data-level="8.3" data-path="process-tracing.html"><a href="process-tracing.html#foundations-for-van-everas-tests"><i class="fa fa-check"></i><b>8.3</b> Foundations for Van Evera’s tests</a></li>
<li class="chapter" data-level="8.4" data-path="process-tracing.html"><a href="process-tracing.html#clue-selection-clues-at-the-center-of-chains-can-be-more-informative"><i class="fa fa-check"></i><b>8.4</b> Clue selection: clues at the center of chains can be more informative</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="identification.html"><a href="identification.html"><i class="fa fa-check"></i><b>9</b> Identification</a><ul>
<li class="chapter" data-level="9.1" data-path="identification.html"><a href="identification.html#illustration-of-the-backdoor-criterion"><i class="fa fa-check"></i><b>9.1</b> Illustration of the backdoor criterion</a></li>
<li class="chapter" data-level="9.2" data-path="identification.html"><a href="identification.html#identification-instruments"><i class="fa fa-check"></i><b>9.2</b> Identification: Instruments</a></li>
<li class="chapter" data-level="9.3" data-path="identification.html"><a href="identification.html#identification-through-the-frontdoor"><i class="fa fa-check"></i><b>9.3</b> Identification through the frontdoor</a></li>
<li class="chapter" data-level="9.4" data-path="identification.html"><a href="identification.html#simple-sample-selection-bias"><i class="fa fa-check"></i><b>9.4</b> Simple sample selection bias</a></li>
<li class="chapter" data-level="9.5" data-path="identification.html"><a href="identification.html#addressing-both-sample-selection-bias-and-confounding"><i class="fa fa-check"></i><b>9.5</b> Addressing both sample selection bias and confounding</a></li>
<li class="chapter" data-level="9.6" data-path="identification.html"><a href="identification.html#learning-from-a-collider"><i class="fa fa-check"></i><b>9.6</b> Learning from a collider!</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="mixing-methods.html"><a href="mixing-methods.html"><i class="fa fa-check"></i><b>10</b> Mixing methods</a><ul>
<li class="chapter" data-level="10.1" data-path="mixing-methods.html"><a href="mixing-methods.html#using-within-case-data-to-help-with-identification"><i class="fa fa-check"></i><b>10.1</b> Using within case data to help with identification</a></li>
<li class="chapter" data-level="10.2" data-path="mixing-methods.html"><a href="mixing-methods.html#distinguishing-paths"><i class="fa fa-check"></i><b>10.2</b> Distinguishing paths</a></li>
<li class="chapter" data-level="10.3" data-path="mixing-methods.html"><a href="mixing-methods.html#nothing-from-nothing"><i class="fa fa-check"></i><b>10.3</b> Nothing from nothing</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="external-validity-and-inference-aggregation.html"><a href="external-validity-and-inference-aggregation.html"><i class="fa fa-check"></i><b>11</b> External validity and inference aggregation</a><ul>
<li class="chapter" data-level="11.1" data-path="external-validity-and-inference-aggregation.html"><a href="external-validity-and-inference-aggregation.html#transportation-of-findings-across-contexts"><i class="fa fa-check"></i><b>11.1</b> Transportation of findings across contexts</a></li>
<li class="chapter" data-level="11.2" data-path="external-validity-and-inference-aggregation.html"><a href="external-validity-and-inference-aggregation.html#combining-observational-and-experimental-data"><i class="fa fa-check"></i><b>11.2</b> Combining observational and experimental data</a></li>
<li class="chapter" data-level="11.3" data-path="external-validity-and-inference-aggregation.html"><a href="external-validity-and-inference-aggregation.html#a-jigsaw-puzzle-learning-across-populations"><i class="fa fa-check"></i><b>11.3</b> A jigsaw puzzle: Learning across populations</a></li>
</ul></li>
<li class="part"><span><b>IV Notation</b></span></li>
<li class="chapter" data-level="12" data-path="notation.html"><a href="notation.html"><i class="fa fa-check"></i><b>12</b> Notation and syntax</a><ul>
<li class="chapter" data-level="12.1" data-path="notation.html"><a href="notation.html#notation-1"><i class="fa fa-check"></i><b>12.1</b> Notation</a><ul>
<li class="chapter" data-level="12.1.1" data-path="notation.html"><a href="notation.html#parents"><i class="fa fa-check"></i><b>12.1.1</b> Parents, children, and all that</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="notation.html"><a href="notation.html#syntax"><i class="fa fa-check"></i><b>12.2</b> Causal syntax</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/macartan/gbiqq/" target="blank">Uses gbiqq</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Causal Models: Guide to <code>CausalQueries</code></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="identification" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Identification</h1>
<div id="illustration-of-the-backdoor-criterion" class="section level2">
<h2><span class="header-section-number">9.1</span> Illustration of the backdoor criterion</h2>
<p>Perhaps the most common approach to identifying causal effects in observational research is to condition on possible confounders. The “backdoor” criterion for identifying an effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> involves finding a set of nodes to condition on that collectively block all “backdoor paths” between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The intuition is that if these paths are blocked, then any systematic correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> reflects the effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>.</p>
<p>To illustrate the backdoor criterion we want to show that estimates of the effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> are identified if we have data on a node that blocks a backdoor path—<span class="math inline">\(C\)</span>—but not otherwise. With <code>CausalQueries</code> models however, rather than conditioning on <span class="math inline">\(C\)</span> we simply include data on <span class="math inline">\(C\)</span> in our model and update as usual.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb96-1" title="1">model &lt;-<span class="st"> </span><span class="kw">make_model</span>(<span class="st">&quot;C -&gt; X -&gt; Y &lt;- C&quot;</span>)  <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb96-2" title="2"><span class="st">         </span><span class="kw">set_restrictions</span>(<span class="st">&quot;(Y[C=1]&lt;Y[C=0])&quot;</span>)</a>
<a class="sourceLine" id="cb96-3" title="3"></a>
<a class="sourceLine" id="cb96-4" title="4"></a>
<a class="sourceLine" id="cb96-5" title="5"><span class="co"># Four types of data: Large, small, door open, door closed</span></a>
<a class="sourceLine" id="cb96-6" title="6">N &lt;-<span class="st"> </span><span class="dv">10000</span></a>
<a class="sourceLine" id="cb96-7" title="7">df_closed_door_large &lt;-<span class="st"> </span><span class="kw">make_data</span>(model, <span class="dt">n =</span> N)</a>
<a class="sourceLine" id="cb96-8" title="8">df_open_door_large   &lt;-<span class="st"> </span><span class="kw">mutate</span>(df_closed_door_large, <span class="dt">C =</span> <span class="ot">NA</span>)</a>
<a class="sourceLine" id="cb96-9" title="9">df_closed_door_small &lt;-<span class="st"> </span>df_closed_door_large[<span class="kw">sample</span>(N, <span class="dv">200</span>), ]</a>
<a class="sourceLine" id="cb96-10" title="10">df_open_door_small   &lt;-<span class="st"> </span>df_open_door_large[<span class="kw">sample</span>(N, <span class="dv">200</span>), ]</a></code></pre></div>
<p><img src="causalmodels_files/figure-html/unnamed-chunk-93-1.png" width="672" /></p>
<p>We see that with small <span class="math inline">\(n\)</span> (200 units), closing the backdoor (by including data on <span class="math inline">\(C\)</span>) produces a tighter distribution on the ATE. With large <span class="math inline">\(N\)</span> (10,000 units) the distribution around the estimand collapses when the backdoor is closed but not when it is open.</p>
</div>
<div id="identification-instruments" class="section level2">
<h2><span class="header-section-number">9.2</span> Identification: Instruments</h2>
<p>We illustrate how you can learn about whether <span class="math inline">\(X=1\)</span> caused <span class="math inline">\(Y=1\)</span> by taking advantage of an “instrument,” <span class="math inline">\(Z\)</span>.</p>
<p>We start with a model that builds in the instrumental variables exclusion restriction (no unobserved confounding between <span class="math inline">\(Z\)</span> and <span class="math inline">\(Y\)</span>, no paths between <span class="math inline">\(Z\)</span> and <span class="math inline">\(Y\)</span> except through <span class="math inline">\(X\)</span>) but does not include a monotonicity restriction (no negative effect of <span class="math inline">\(Z\)</span> on <span class="math inline">\(X\)</span>).</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb97-1" title="1">model &lt;-<span class="st"> </span><span class="kw">make_model</span>(<span class="st">&quot;Z -&gt; X -&gt; Y&quot;</span>)  <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb97-2" title="2"><span class="st">         </span><span class="kw">set_confound</span>(<span class="dt">confound =</span> <span class="kw">list</span>(<span class="dt">X =</span> <span class="st">&quot;Y[X=1]==1&quot;</span>)) </a>
<a class="sourceLine" id="cb97-3" title="3"></a>
<a class="sourceLine" id="cb97-4" title="4"><span class="kw">plot</span>(model)</a></code></pre></div>
<p><img src="causalmodels_files/figure-html/appinstruments-1.png" width="672" /></p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb98-1" title="1">result &lt;-<span class="st"> </span><span class="kw">query_model</span>(</a>
<a class="sourceLine" id="cb98-2" title="2">    updated, </a>
<a class="sourceLine" id="cb98-3" title="3">    <span class="dt">queries =</span> <span class="kw">list</span>(<span class="dt">ATE =</span> <span class="st">&quot;c(Y[X=1] - Y[X=0])&quot;</span>), </a>
<a class="sourceLine" id="cb98-4" title="4">    <span class="dt">given =</span> <span class="kw">list</span>(<span class="ot">TRUE</span>, <span class="st">&quot;X[Z=1] &gt; X[Z=0]&quot;</span>,  <span class="st">&quot;X==0&quot;</span>,  <span class="st">&quot;X==1&quot;</span>),</a>
<a class="sourceLine" id="cb98-5" title="5">    <span class="dt">using =</span> <span class="st">&quot;posteriors&quot;</span>)</a></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">Query</th>
<th align="left">Given</th>
<th align="left">Using</th>
<th align="right">mean</th>
<th align="right">sd</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ATE</td>
<td align="left">-</td>
<td align="left">posteriors</td>
<td align="right">0.602</td>
<td align="right">0.033</td>
</tr>
<tr class="even">
<td align="left">ATE</td>
<td align="left">X[Z=1] &gt; X[Z=0]</td>
<td align="left">posteriors</td>
<td align="right">0.673</td>
<td align="right">0.031</td>
</tr>
<tr class="odd">
<td align="left">ATE</td>
<td align="left">X==0</td>
<td align="left">posteriors</td>
<td align="right">0.618</td>
<td align="right">0.070</td>
</tr>
<tr class="even">
<td align="left">ATE</td>
<td align="left">X==1</td>
<td align="left">posteriors</td>
<td align="right">0.587</td>
<td align="right">0.023</td>
</tr>
</tbody>
</table>
<p>We calculate the average causal effect (a) for all (b) for the compliers and (c) conditional on values of <span class="math inline">\(M\)</span>.</p>
<p>We see here that the effects are strongest for the “compliers”—units for whom <span class="math inline">\(X\)</span> responds positively to <span class="math inline">\(Z\)</span>; in addition they are stronger for the treated than for the untreated. Moreover we see that the posterior variance on the complier average effect is low. If our model also imposed a monotonicity assumption then it would be lower still.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb99-1" title="1">model &lt;-<span class="st"> </span><span class="kw">make_model</span>(<span class="st">&quot;Z -&gt; X -&gt; Y&quot;</span>)  <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb99-2" title="2"><span class="st">         </span><span class="kw">set_restrictions</span>(<span class="kw">decreasing</span>(<span class="st">&quot;Z&quot;</span>, <span class="st">&quot;X&quot;</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb99-3" title="3"><span class="st">         </span><span class="kw">set_confound</span>(<span class="dt">confound =</span> <span class="kw">list</span>(<span class="dt">X =</span> <span class="st">&quot;Y[X=1]==1&quot;</span>)) </a></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">Query</th>
<th align="left">Given</th>
<th align="left">Using</th>
<th align="right">mean</th>
<th align="right">sd</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ATE</td>
<td align="left">-</td>
<td align="left">posteriors</td>
<td align="right">0.603</td>
<td align="right">0.047</td>
</tr>
<tr class="even">
<td align="left">ATE</td>
<td align="left">X[Z=1] &gt; X[Z=0]</td>
<td align="left">posteriors</td>
<td align="right">0.697</td>
<td align="right">0.020</td>
</tr>
<tr class="odd">
<td align="left">ATE</td>
<td align="left">X==0</td>
<td align="left">posteriors</td>
<td align="right">0.618</td>
<td align="right">0.099</td>
</tr>
<tr class="even">
<td align="left">ATE</td>
<td align="left">X==1</td>
<td align="left">posteriors</td>
<td align="right">0.587</td>
<td align="right">0.026</td>
</tr>
</tbody>
</table>
</div>
<div id="identification-through-the-frontdoor" class="section level2">
<h2><span class="header-section-number">9.3</span> Identification through the frontdoor</h2>
<p>A less well known approach to identification uses information on the causal path from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span>. Consider the following model:</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb100-1" title="1">frontdoor &lt;-<span class="st"> </span><span class="kw">make_model</span>(<span class="st">&quot;X -&gt; M -&gt; Y&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb100-2" title="2"><span class="st">  </span></a>
<a class="sourceLine" id="cb100-3" title="3"><span class="st">  </span><span class="kw">set_confound</span>(<span class="kw">list</span>(<span class="dt">X =</span> <span class="st">&quot;Y[M=1]&gt;Y[M=0]&quot;</span>, </a>
<a class="sourceLine" id="cb100-4" title="4">                    <span class="dt">X =</span> <span class="st">&quot;Y[M=1]&lt;Y[M=0]&quot;</span>))</a>
<a class="sourceLine" id="cb100-5" title="5"></a>
<a class="sourceLine" id="cb100-6" title="6"><span class="kw">plot</span>(frontdoor)</a></code></pre></div>
<p><img src="causalmodels_files/figure-html/appfrontdoor-1.png" width="672" /></p>
<p>Although in both the instrumental variables (IV) setup and the frontdoor setup we are trying to deal with confounding between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the two differ in that in the IV set up we make use of a variable that is prior to <span class="math inline">\(X\)</span> whereas in the frontdoor model we make use of a variable between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. In both cases we need other exclusion restrictions: here we see that there is no unobserved confounding between <span class="math inline">\(X\)</span> and <span class="math inline">\(M\)</span> or between <span class="math inline">\(M\)</span> and <span class="math inline">\(Y\)</span>. Importantly too there is no direct path from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span>, only the path that runs through <span class="math inline">\(M\)</span>.</p>
<p>Below we plot posterior distributions given observations on 2000 units, with and without data on <span class="math inline">\(M\)</span>:</p>
<p><img src="causalmodels_files/figure-html/appfrontdoor2-1.png" width="672" /></p>
<p>The spike on the right confirms that we have identification.</p>
</div>
<div id="simple-sample-selection-bias" class="section level2">
<h2><span class="header-section-number">9.4</span> Simple sample selection bias</h2>
<p>Say we are interested in assessing the share of Republicans in a population but Republicans are (possible) systematically likely to be absent from our sample. What inferences can we make given our sample?</p>
<p>We will assume that we know when we have missing data, though of course we do not know the value of the missing data.</p>
<p>To tackle the problem we will include sample selection into our model:</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb101-1" title="1">model &lt;-<span class="st"> </span><span class="kw">make_model</span>(<span class="st">&quot;R -&gt; S&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb101-2" title="2"><span class="st">  </span><span class="kw">set_parameters</span>(<span class="dt">node =</span> <span class="kw">c</span>(<span class="st">&quot;R&quot;</span>, <span class="st">&quot;S&quot;</span>), <span class="dt">parameters =</span> <span class="kw">list</span>(<span class="kw">c</span>(<span class="dv">2</span><span class="op">/</span><span class="dv">3</span>,<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>), <span class="kw">c</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>)))</a>
<a class="sourceLine" id="cb101-3" title="3"></a>
<a class="sourceLine" id="cb101-4" title="4">data &lt;-<span class="st"> </span><span class="kw">make_data</span>(model, <span class="dt">n =</span> <span class="dv">1000</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb101-5" title="5"><span class="st">        </span><span class="kw">mutate</span>(<span class="dt">R =</span> <span class="kw">ifelse</span>(S<span class="op">==</span><span class="dv">0</span>, <span class="ot">NA</span>, R ))</a></code></pre></div>
<p>From this data and model, the priors and posteriors for population and sample quantities are:</p>
<table>
<thead>
<tr class="header">
<th align="left">Query</th>
<th align="left">Given</th>
<th align="left">Using</th>
<th align="right">mean</th>
<th align="right">sd</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Q 1</td>
<td align="left">-</td>
<td align="left">parameters</td>
<td align="right">0.333</td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">Q 1</td>
<td align="left">-</td>
<td align="left">priors</td>
<td align="right">0.501</td>
<td align="right">0.291</td>
</tr>
<tr class="odd">
<td align="left">Q 1</td>
<td align="left">-</td>
<td align="left">posteriors</td>
<td align="right">0.504</td>
<td align="right">0.142</td>
</tr>
<tr class="even">
<td align="left">Q 1</td>
<td align="left">S==1</td>
<td align="left">parameters</td>
<td align="right">0.500</td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="left">Q 1</td>
<td align="left">S==1</td>
<td align="left">priors</td>
<td align="right">0.502</td>
<td align="right">0.307</td>
</tr>
<tr class="even">
<td align="left">Q 1</td>
<td align="left">S==1</td>
<td align="left">posteriors</td>
<td align="right">0.508</td>
<td align="right">0.024</td>
</tr>
</tbody>
</table>
<p>For the population average effect we tightened our posteriors relative to the priors, though credibility intervals remain wide, even with large data, reflecting our uncertainty about the nature of selection. Our posteriors on the sample mean are accurate and tight.</p>
<p>Importantly we would not do so well if our data did not indicate that we had missingness.</p>
<table>
<thead>
<tr class="header">
<th align="left">Query</th>
<th align="left">Given</th>
<th align="left">Using</th>
<th align="right">mean</th>
<th align="right">sd</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Q 1</td>
<td align="left">-</td>
<td align="left">parameters</td>
<td align="right">0.333</td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">Q 1</td>
<td align="left">-</td>
<td align="left">posteriors</td>
<td align="right">0.492</td>
<td align="right">0.008</td>
</tr>
<tr class="odd">
<td align="left">Q 1</td>
<td align="left">S==1</td>
<td align="left">parameters</td>
<td align="right">0.500</td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">Q 1</td>
<td align="left">S==1</td>
<td align="left">posteriors</td>
<td align="right">0.492</td>
<td align="right">0.008</td>
</tr>
</tbody>
</table>
<p>We naively conclude that all cases are sampled and that population effects are the same as sample effects. The problem here arises because the causal model does not encompass the data gathering process.</p>
</div>
<div id="addressing-both-sample-selection-bias-and-confounding" class="section level2">
<h2><span class="header-section-number">9.5</span> Addressing both sample selection bias and confounding</h2>
<p>Consider the following model from <span class="citation">Bareinboim and Pearl (<a href="#ref-bareinboim2016causal" role="doc-biblioref">2016</a>)</span> (their Figure 4C). The key feature is that data is only seen for units with <span class="math inline">\(S=1\)</span> (<span class="math inline">\(S\)</span> for sampling).</p>
<p>In this model the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is confounded. Two strategies work to address confounding: controlling for either <span class="math inline">\(Z\)</span> or for <span class="math inline">\(W1\)</span> <em>and</em> <span class="math inline">\(W2\)</span> works. But only the first strategy addresses the sample selection problem properly. The reason is that <span class="math inline">\(Z\)</span> is independent of <span class="math inline">\(S\)</span> and so variation in <span class="math inline">\(Z\)</span> is not affected by selection on <span class="math inline">\(S\)</span>.</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb102-1" title="1">selection &lt;-<span class="st"> </span><span class="kw">make_model</span>(<span class="st">&quot;X &lt;- W1 -&gt; W2 -&gt; X -&gt; Y &lt;- Z -&gt; W2; W1 -&gt; S&quot;</span>)</a></code></pre></div>
<p><img src="causalmodels_files/figure-html/unnamed-chunk-99-1.png" width="672" /></p>
<p>To keep the parameter and type space small we also impose a set of restrictions: <span class="math inline">\(S\)</span> is non decreasing in <span class="math inline">\(W_1\)</span>, <span class="math inline">\(X\)</span> is not decreasing in either <span class="math inline">\(W1\)</span> or <span class="math inline">\(W2\)</span>, <span class="math inline">\(Y\)</span> is not decreasing <span class="math inline">\(Z\)</span> or <span class="math inline">\(X\)</span> and <span class="math inline">\(X\)</span> affects <span class="math inline">\(Y\)</span> only if <span class="math inline">\(Z=1\)</span>. <span class="math inline">\(W_2=1\)</span> if and only if both <span class="math inline">\(W_1=1\)</span> and <span class="math inline">\(Z=1\)</span>. These all reduce the problem to one with 18 nodal types and 288 causal types.</p>
<p>Worth noting that in this model although selection is related to patterns of confounding, it is not related to causal effects: the effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> is not different from units that are or are not selected.</p>
<p>Given these priors we will assume a true (unknown) data generating process with no effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>, in which <span class="math inline">\(W_1\)</span> arises with a <span class="math inline">\(1/3\)</span> probability but has a strong positive effect on selection into the sample when it does arise.</p>
<p>The estimand values given the true parameters and priors for this model are as shown below.</p>
<table>
<caption><span id="tab:appsimpleselcconf5">Table 9.1: </span>Estimand values</caption>
<thead>
<tr class="header">
<th align="left">Query</th>
<th align="left">Given</th>
<th align="left">Using</th>
<th align="right">mean</th>
<th align="right">sd</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Q 1</td>
<td align="left">-</td>
<td align="left">parameters</td>
<td align="right">0.000</td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">Q 1</td>
<td align="left">-</td>
<td align="left">priors</td>
<td align="right">0.334</td>
<td align="right">0.155</td>
</tr>
</tbody>
</table>
<p>This confirms a zero true effect, though priors are dispersed, centered on a positive effect.</p>
<p>We can see the inference challenge from observational data using regression analysis with and without conditioning on <span class="math inline">\(Z\)</span> and <span class="math inline">\(W_1, W_2\)</span>.</p>
<table style="text-align:center">
<tr>
<td colspan="4" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td colspan="3">
<em>Dependent variable:</em>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="3" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td colspan="3">
Y
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(1)
</td>
<td>
(2)
</td>
<td>
(3)
</td>
</tr>
<tr>
<td colspan="4" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
X
</td>
<td>
0.075<sup>***</sup>
</td>
<td>
0.029<sup>*</sup>
</td>
<td>
-0.006
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(0.008)
</td>
<td>
(0.016)
</td>
<td>
(0.008)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
X:W1_norm
</td>
<td>
</td>
<td>
0.163<sup>***</sup>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
(0.030)
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
X:W2_norm
</td>
<td>
</td>
<td>
0.200<sup>***</sup>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
(0.032)
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
X:Z_norm
</td>
<td>
</td>
<td>
</td>
<td>
-0.020
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
<td>
(0.016)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td colspan="4" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
Observations
</td>
<td>
14,968
</td>
<td>
14,968
</td>
<td>
14,968
</td>
</tr>
<tr>
<td style="text-align:left">
R<sup>2</sup>
</td>
<td>
0.006
</td>
<td>
0.020
</td>
<td>
0.100
</td>
</tr>
<tr>
<td style="text-align:left">
Adjusted R<sup>2</sup>
</td>
<td>
0.006
</td>
<td>
0.019
</td>
<td>
0.100
</td>
</tr>
<tr>
<td style="text-align:left">
Residual Std. Error
</td>
<td>
0.499 (df = 14966)
</td>
<td>
0.495 (df = 14962)
</td>
<td>
0.474 (df = 14964)
</td>
</tr>
<tr>
<td style="text-align:left">
F Statistic
</td>
<td>
83.900<sup>***</sup> (df = 1; 14966)
</td>
<td>
59.800<sup>***</sup> (df = 5; 14962)
</td>
<td>
554.200<sup>***</sup> (df = 3; 14964)
</td>
</tr>
<tr>
<td colspan="4" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
<em>Note:</em>
</td>
<td colspan="3" style="text-align:right">
<sup><em></sup>p&lt;0.1; <sup><strong></sup>p&lt;0.05; <sup></strong></em></sup>p&lt;0.01
</td>
</tr>
</table>
<p>Naive analysis is far off; but even after conditioning on <span class="math inline">\(W_1, W_2\)</span> we still wrongly infer a positive effect.</p>
<p>Bayesian inferences given different data strategies are shown below:</p>
<table>
<thead>
<tr class="header">
<th align="left">data</th>
<th align="right">mean</th>
<th align="right">sd</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">X,Y</td>
<td align="right">0.066</td>
<td align="right">0.013</td>
</tr>
<tr class="even">
<td align="left">X,Y, W1, W2</td>
<td align="right">0.009</td>
<td align="right">0.005</td>
</tr>
<tr class="odd">
<td align="left">X, Y, Z</td>
<td align="right">0.011</td>
<td align="right">0.005</td>
</tr>
</tbody>
</table>
<p>We see the best performance is achieved for the model with data on <span class="math inline">\(Z\)</span>—in this case the mean posterior estimate is closest to the truth–0–and the standard deviation is lowest also. However the gains in choosing <span class="math inline">\(Z\)</span> over <span class="math inline">\(W1, W2\)</span> are not as striking as in the regression estimates since knowledge of the model structure protects us from error.</p>
</div>
<div id="learning-from-a-collider" class="section level2">
<h2><span class="header-section-number">9.6</span> Learning from a collider!</h2>
<p>Conditioning on a collider can be a bad idea as it can introduce a correlation between variables that might not have existed otherwise <span class="citation">(Elwert and Winship <a href="#ref-elwert2014endogenous" role="doc-biblioref">2014</a>)</span>. But that doesn’t mean colliders should be ignored in analysis altogether. For a Bayesian, knowledge of the value of a collider can still be informative.</p>
<p>Pearl describes a model similar to the following as a case for which controlling for covariate <span class="math inline">\(W\)</span> induces bias in the estimation of the effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>, which could otherwise be estimated without bias using simple differences in means.</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb103-1" title="1">model &lt;-<span class="st"> </span><span class="kw">make_model</span>(<span class="st">&quot;X -&gt; Y &lt;- U1 -&gt; W &lt;- U2 -&gt; X&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb103-2" title="2"><span class="st">  </span></a>
<a class="sourceLine" id="cb103-3" title="3"><span class="st">  </span><span class="kw">set_restrictions</span>(<span class="dt">labels =</span> <span class="kw">list</span>(<span class="dt">Y =</span> <span class="kw">c</span>(<span class="st">&quot;0001&quot;</span>, <span class="st">&quot;1111&quot;</span>), <span class="dt">W =</span> <span class="st">&quot;0001&quot;</span>), <span class="dt">keep =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb103-4" title="4"><span class="st">  </span><span class="kw">set_restrictions</span>(<span class="st">&quot;(X[U2=1]&lt;X[U2=0])&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb103-5" title="5"><span class="st">  </span><span class="kw">set_parameters</span>(<span class="dt">node =</span> <span class="kw">c</span>(<span class="st">&quot;U1&quot;</span>, <span class="st">&quot;Y&quot;</span>), <span class="dt">parameters =</span> <span class="kw">list</span>(<span class="kw">c</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">4</span>, <span class="dv">3</span><span class="op">/</span><span class="dv">4</span>), <span class="kw">c</span>(<span class="dv">2</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>)))</a>
<a class="sourceLine" id="cb103-6" title="6"></a>
<a class="sourceLine" id="cb103-7" title="7"><span class="kw">plot</span>(model)</a></code></pre></div>
<p><img src="causalmodels_files/figure-html/applearncoll-1.png" width="672" /></p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb104-1" title="1">data &lt;-<span class="st"> </span><span class="kw">make_data</span>(model, </a>
<a class="sourceLine" id="cb104-2" title="2">                  <span class="dt">n =</span> <span class="dv">25000</span>, </a>
<a class="sourceLine" id="cb104-3" title="3">                  <span class="dt">vars =</span> <span class="kw">c</span>(<span class="st">&quot;W&quot;</span>, <span class="st">&quot;X&quot;</span>, <span class="st">&quot;Y&quot;</span>), </a>
<a class="sourceLine" id="cb104-4" title="4">                  <span class="dt">using =</span> <span class="st">&quot;parameters&quot;</span>)</a></code></pre></div>
<p>The effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> is .5 but average effects as well as the probability of causation, are different for units with <span class="math inline">\(W=0\)</span> and <span class="math inline">\(W=1\)</span> (this, even though <span class="math inline">\(W\)</span> does not affect <span class="math inline">\(Y\)</span>):</p>
<table>
<thead>
<tr class="header">
<th align="left">Query</th>
<th align="left">Given</th>
<th align="left">Using</th>
<th align="right">mean</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Y(1)-Y(0)</td>
<td align="left">-</td>
<td align="left">parameters</td>
<td align="right">0.500</td>
</tr>
<tr class="even">
<td align="left">Y(1)-Y(0)</td>
<td align="left">W==0</td>
<td align="left">parameters</td>
<td align="right">0.400</td>
</tr>
<tr class="odd">
<td align="left">Y(1)-Y(0)</td>
<td align="left">W==1</td>
<td align="left">parameters</td>
<td align="right">0.667</td>
</tr>
<tr class="even">
<td align="left">Y(1)-Y(0)</td>
<td align="left">X==1 &amp; Y==1</td>
<td align="left">parameters</td>
<td align="right">0.600</td>
</tr>
<tr class="odd">
<td align="left">Y(1)-Y(0)</td>
<td align="left">X==1 &amp; Y==1 &amp; W==0</td>
<td align="left">parameters</td>
<td align="right">0.500</td>
</tr>
<tr class="even">
<td align="left">Y(1)-Y(0)</td>
<td align="left">X==1 &amp; Y==1 &amp; W==1</td>
<td align="left">parameters</td>
<td align="right">0.667</td>
</tr>
</tbody>
</table>
<p>These are the quantities we seek to recover. The ATE can be gotten fairly precisely in a simple regression. But controlling for <span class="math inline">\(W\)</span> introduces bias in the estimation of this effect (whether done using a simple control or an interactive model):</p>
<table style="text-align:center">
<tr>
<td colspan="4" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td colspan="3">
<em>Dependent variable:</em>
</td>
</tr>
<tr>
<td>
</td>
<td colspan="3" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td colspan="3">
Y
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(1)
</td>
<td>
(2)
</td>
<td>
(3)
</td>
</tr>
<tr>
<td colspan="4" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
X
</td>
<td>
0.494<sup>***</sup>
</td>
<td>
0.446<sup>***</sup>
</td>
<td>
0.452<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(0.005)
</td>
<td>
(0.005)
</td>
<td>
(0.005)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
W
</td>
<td>
</td>
<td>
0.195<sup>***</sup>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
(0.006)
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
W_norm
</td>
<td>
</td>
<td>
</td>
<td>
-0.005
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
<td>
(0.008)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
X:W_norm
</td>
<td>
</td>
<td>
</td>
<td>
0.347<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
<td>
(0.011)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
Constant
</td>
<td>
0.334<sup>***</sup>
</td>
<td>
0.285<sup>***</sup>
</td>
<td>
0.333<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(0.004)
</td>
<td>
(0.004)
</td>
<td>
(0.004)
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td colspan="4" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
Observations
</td>
<td>
25,000
</td>
<td>
25,000
</td>
<td>
25,000
</td>
</tr>
<tr>
<td style="text-align:left">
R<sup>2</sup>
</td>
<td>
0.250
</td>
<td>
0.284
</td>
<td>
0.311
</td>
</tr>
<tr>
<td style="text-align:left">
Adjusted R<sup>2</sup>
</td>
<td>
0.250
</td>
<td>
0.284
</td>
<td>
0.311
</td>
</tr>
<tr>
<td style="text-align:left">
Residual Std. Error
</td>
<td>
0.427 (df = 24998)
</td>
<td>
0.417 (df = 24997)
</td>
<td>
0.410 (df = 24996)
</td>
</tr>
<tr>
<td style="text-align:left">
F Statistic
</td>
<td>
8,352.000<sup>***</sup> (df = 1; 24998)
</td>
<td>
4,968.000<sup>***</sup> (df = 2; 24997)
</td>
<td>
3,757.000<sup>***</sup> (df = 3; 24996)
</td>
</tr>
<tr>
<td colspan="4" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
<em>Note:</em>
</td>
<td colspan="3" style="text-align:right">
<sup><em></sup>p&lt;0.1; <sup><strong></sup>p&lt;0.05; <sup></strong></em></sup>p&lt;0.01
</td>
</tr>
</table>
<p>How does the Bayesian model do, with and without data on <span class="math inline">\(W\)</span>?</p>
<p>Without <span class="math inline">\(W\)</span> we have:</p>
<table>
<caption><span id="tab:applearncoll6">Table 9.2: </span>Collider excluded from model</caption>
<thead>
<tr class="header">
<th align="left">Query</th>
<th align="left">Given</th>
<th align="left">Using</th>
<th align="right">mean</th>
<th align="right">sd</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Y(1)-Y(0)</td>
<td align="left">-</td>
<td align="left">posteriors</td>
<td align="right">0.496</td>
<td align="right">0.005</td>
</tr>
<tr class="even">
<td align="left">Y(1)-Y(0)</td>
<td align="left">W==0</td>
<td align="left">posteriors</td>
<td align="right">0.362</td>
<td align="right">0.114</td>
</tr>
<tr class="odd">
<td align="left">Y(1)-Y(0)</td>
<td align="left">W==1</td>
<td align="left">posteriors</td>
<td align="right">0.669</td>
<td align="right">0.004</td>
</tr>
<tr class="even">
<td align="left">Y(1)-Y(0)</td>
<td align="left">X==1 &amp; Y==1</td>
<td align="left">posteriors</td>
<td align="right">0.599</td>
<td align="right">0.005</td>
</tr>
<tr class="odd">
<td align="left">Y(1)-Y(0)</td>
<td align="left">X==1 &amp; Y==1 &amp; W==0</td>
<td align="left">posteriors</td>
<td align="right">0.419</td>
<td align="right">0.168</td>
</tr>
<tr class="even">
<td align="left">Y(1)-Y(0)</td>
<td align="left">X==1 &amp; Y==1 &amp; W==1</td>
<td align="left">posteriors</td>
<td align="right">0.669</td>
<td align="right">0.004</td>
</tr>
</tbody>
</table>
<p>Thus we estimate the treatment effect well. What’s more we can estimate the probability of causation when <span class="math inline">\(W=1\)</span> accurately, even though we have not observed <span class="math inline">\(W\)</span>. The reason is that if <span class="math inline">\(W=1\)</span> then, given the model restrictions, we know that both <span class="math inline">\(U_1=1\)</span> and <span class="math inline">\(U_2=1\)</span> which is enough. We are not sure however what to infer when <span class="math inline">\(W=0\)</span> since this could be due to either <span class="math inline">\(U_1=0\)</span> or <span class="math inline">\(U_2=0\)</span>.</p>
<p>When we incorporate data on <span class="math inline">\(W\)</span> our posteriors are:</p>
<table>
<caption><span id="tab:applearncoll8">Table 9.3: </span>Collider included in model</caption>
<thead>
<tr class="header">
<th align="left">Query</th>
<th align="left">Given</th>
<th align="left">Using</th>
<th align="right">mean</th>
<th align="right">sd</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Y(1)-Y(0)</td>
<td align="left">-</td>
<td align="left">posteriors</td>
<td align="right">0.496</td>
<td align="right">0.005</td>
</tr>
<tr class="even">
<td align="left">Y(1)-Y(0)</td>
<td align="left">W==0</td>
<td align="left">posteriors</td>
<td align="right">0.394</td>
<td align="right">0.007</td>
</tr>
<tr class="odd">
<td align="left">Y(1)-Y(0)</td>
<td align="left">W==1</td>
<td align="left">posteriors</td>
<td align="right">0.669</td>
<td align="right">0.004</td>
</tr>
<tr class="even">
<td align="left">Y(1)-Y(0)</td>
<td align="left">X==1 &amp; Y==1</td>
<td align="left">posteriors</td>
<td align="right">0.599</td>
<td align="right">0.005</td>
</tr>
<tr class="odd">
<td align="left">Y(1)-Y(0)</td>
<td align="left">X==1 &amp; Y==1 &amp; W==0</td>
<td align="left">posteriors</td>
<td align="right">0.500</td>
<td align="right">0.008</td>
</tr>
<tr class="even">
<td align="left">Y(1)-Y(0)</td>
<td align="left">X==1 &amp; Y==1 &amp; W==1</td>
<td align="left">posteriors</td>
<td align="right">0.669</td>
<td align="right">0.004</td>
</tr>
</tbody>
</table>
<p>We see including the collider does not induce error in estimation of the ATE, even though it does in a regression framework. Where we do well before we continue to do well. However the new information lets us improve our model and, in particular, we see that we now get a good and tight estimate for the probability that <span class="math inline">\(X=1\)</span> caused <span class="math inline">\(Y=1\)</span> in a case where <span class="math inline">\(W=0\)</span>.</p>
<p>In short, though conditioning on a collider induces error in a regression framework; including the collider as data for updating our causal model doesn’t hurt us and can help us.</p>
<!-- In cases in which $W=0$, $X=1$ causes $Y=1$ when $U2=1$ (and $U1 = 0$). Joint observation of $W$ and $Y$ lets us learn about the probability that $U1=0$ and $U2 =1$ and in particular lets us learn that U1=0 is less common that supposed in the priors. -->
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-bareinboim2016causal">
<p>Bareinboim, Elias, and Judea Pearl. 2016. “Causal Inference and the Data-Fusion Problem.” <em>Proceedings of the National Academy of Sciences</em> 113 (27): 7345–52.</p>
</div>
<div id="ref-elwert2014endogenous">
<p>Elwert, Felix, and Christopher Winship. 2014. “Endogenous Selection Bias: The Problem of Conditioning on a Collider Variable.” <em>Annual Review of Sociology</em> 40: 31–53.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="process-tracing.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mixing-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false,
"google": false,
"instapper": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["causalmodels.pdf"],
"toc": {
"collapse": "section"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

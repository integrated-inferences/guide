% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  12pt,
]{book}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{color}
%\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{color}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{dsfont}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Causal Models: Guide to CausalQueries},
  pdfauthor={Macartan Humphreys and Alan Jacobs},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Causal Models: Guide to \texttt{CausalQueries}}
\author{Macartan Humphreys and Alan Jacobs}
\date{2023-08-31}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

\textbf{Map}

This guide is supplementary material for our book. \href{https://integrated-inferences.github.io/}{\emph{Integrated Inferences}} \citep{ii2023}.

\begin{itemize}
\tightlist
\item
  The \protect\hyperlink{cm}{first part} of the guide provides a brief motivation of causal models.
\item
  The \protect\hyperlink{package}{second part} describes how the package works and how to use it.
\item
  The \protect\hyperlink{applications}{third part} illustrates applications of the package for defining and learning from a set of canonical causal models.
\item
  The short \protect\hyperlink{notation}{last part} has a notation guide.
\end{itemize}

\textbf{Credits}

The approach used in \texttt{CausalQueries} is a generalization of the ``biqq'' models described in ``Mixing Methods: A Bayesian Approach'' \citep{humphreys2015mixing}. The conceptual extension makes use of work on probabilistic causal models described in Pearl's \emph{Causality} \citep{pearl2009causality}. The approach to generating a generic stan function that can take data from arbitrary models was developed in key contributions by \href{http://jasper-cooper.com/}{Jasper Cooper} and \href{http://gsyunyaev.com/}{Georgiy Syunyaev}. \href{https://lilymedina.github.io/}{Lily Medina} did magical work pulling it all together and developing approaches to characterizing confounding and defining estimands. Clara Bicalho helped figure out a nice syntax for causal statements. Julio Solis made many key contributions figuring out how to simplify the specification of priors. Merlin Heidemanns figure out the \texttt{rstantools} integration and made myriad code improvements. Till Tietz revamped the package and improved every part of it.

\hypertarget{part-causal-models}{%
\part{Causal Models}\label{part-causal-models}}

\hypertarget{cm}{%
\chapter{What and why}\label{cm}}

The \texttt{CausalQueries} package is designed to make it easy to \emph{build}, \emph{update}, and \emph{query} causal models defined over binary variables.

The causal models we use are of the form described by \citet{pearl2009causality}, with Bayesian updating on the causal models similar to that described by \citet{cowell1999probabilistic}. Though drawing heavily on the Pearlian framework, the approach specifies parameters using potential outcomes (specifically, using principal stratificaton \citep{frangakis2002principal}) and uses the \texttt{stan} framework \citep{carpenter2017stan} to implement updating.

We will illustrate how to use these models for a number of inferential tasks that are sometimes difficult to implement:

\begin{itemize}
\tightlist
\item
  \textbf{Bayesian process tracing}: How to figure out what to believe about a case-level causal claim given one or more pieces of evidence about a case \citep{bennett2015process}.
\item
  \textbf{Mixed methods}: How to combine within-case (``qualitative'') evidence with cross-case (``quantitative'') evidence to make causal claims about cases or groups \citep{humphreys2015mixing}.
\item
  \textbf{Counterfactual reasoning}: How to estimate the probability that an outcome is due to a cause and other counterfactual queries (effects of causes, probability of sufficiency, explanation) \citep{tian2000probabilities}.
\item
  \textbf{Inference in the absence of causal identification}: What inferences can you draw when causal quantities are not identified? How can you figure out \emph{whether} a causal quantity is identified \citep{manski1995identification}?
\item
  \textbf{Extrapolation, data fusion}: How to draw inferences from mixtures of observational and experimental data? How to draw out-of-sample inferences given a theory of how one place differs from another \citep{bareinboim2016causal}?
\end{itemize}

The functions in the package allow these different kinds of questions to be answered using the same three basic steps --- \texttt{make\_model}, \texttt{update\_model}, and \texttt{query\_model} --- without recourse to specific estimators for specific estimands.

The approach, however, requires thinking about causal inference differently from how many in the experimental tradition are used to.

\hypertarget{two-approaches-to-inference}{%
\section{Two approaches to inference}\label{two-approaches-to-inference}}

We contrast the two approaches to causal inference using the simplest problem: the analysis of data from a two-arm experimental trial to determine the average effect of a treatment on an outcome.

In the canonical experimental trial, a treatment, \(X\), is randomly assigned and outcomes, \(Y\), are measured in both treatment and control groups. The usual interest is in understanding the average effect of \(X\) on \(Y\) (which we will assume are both binary).

The classic approach to answering this question is to take the difference between outcomes in treatment and control groups as an estimate of this average treatment effect. Estimates of uncertainty about this answer can also be generated using information on variation in the treatment and control groups.

It is also possible, however, to answer this question---along with a rich variety of other questions---using a \emph{causal models} approach \citep{pearl2009causality}.

For intuition for how a causal model approach works, say instead of simply taking the differences between treatment and control one were to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  construct a simple model in which (a) \(X\) is randomly assigned, with 50\% probability and (b) we specify some prior beliefs about how \(Y\) responds to \(X\), which could be positively, negatively, or not at all, and possibly different for each unit
\item
  update beliefs about how \(Y\) responds to \(X\) given the data on \(X\) and \(Y\)
\end{enumerate}

Though very simple, this \(X \rightarrow Y\) model adds a great deal of structure to the situation It is, in fact, more of a model than you need if all you want to do is estimate average treatment effects. But it is nevertheless enough of a model to let you estimate quantities---such as causal attribution---that you could not estimate without a model, even given random assignment.

\hypertarget{recovering-the-ate-with-difference-in-means}{%
\section{Recovering the ATE with Difference in Means}\label{recovering-the-ate-with-difference-in-means}}

To illustrate, imagine that \emph{in actuality} (but unknown to us) \(X\) shifts \(Y\) from 0 to 1 in 50\% of cases while in the rest \(Y\) is 0 or 1 regardless.

We imagine we have access to some data in which treatment, \(X\), has been randomly assigned:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fabricate data using fabricatr}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{17022020}\NormalTok{)}
\NormalTok{data  }\OtherTok{\textless{}{-}} \FunctionTok{fabricate}\NormalTok{(}\AttributeTok{N =} \DecValTok{1000}\NormalTok{, }\AttributeTok{X =} \FunctionTok{complete\_ra}\NormalTok{(N), }\AttributeTok{Y =} \DecValTok{1}\SpecialCharTok{*}\NormalTok{(}\FunctionTok{runif}\NormalTok{(N, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{)}\SpecialCharTok{/}\DecValTok{2} \SpecialCharTok{\textless{}}\NormalTok{ X))}
\end{Highlighting}
\end{Shaded}

The classic experimental approach to causal inference is to estimate the effect of \(X\) on \(Y\) using differences in means: taking the difference between the average outcome in treatment and the average outcome in control. Thanks to randomization, in expectation that difference is equal to the average of the differences in outcomes that units would exhibit if they were in treatment versus if they were in control---that is, the causal effect.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{difference\_in\_means}\NormalTok{(Y}\SpecialCharTok{\textasciitilde{}}\NormalTok{X, }\AttributeTok{data =}\NormalTok{ data)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-5}Inferences on the ATE from differences in means}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|) & CI Lower & CI Upper & DF\\
\hline
X & 0.484 & 0.028 & 17.48 & 0 & 0.43 & 0.538 & 996.4\\
\hline
\end{tabular}
\end{table}

This approach gets us an accurate and precise answer, and it's simply done (here using a function from the \texttt{estimatr} package).

\hypertarget{recovering-the-ate-with-a-causal-model}{%
\section{Recovering the ATE with a Causal Model}\label{recovering-the-ate-with-a-causal-model}}

The model-based approach takes a few more lines of code and is implemented in \texttt{CausalQueries} as follows.

First we define a model, like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} Y"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Implicit in the definition of the model is a set of parameters and priors over these parameters. We discuss these in much more detail later, but for now we will just say that priors are uniform over possible causal relations.

Second, we update the model, like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{update\_model}\NormalTok{(model, data) }
\end{Highlighting}
\end{Shaded}

Third, we query the model like this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{query\_model}\NormalTok{(model, }\AttributeTok{using  =} \StringTok{"posteriors"}\NormalTok{, }\AttributeTok{query =} \StringTok{"Y[X=1] {-} Y[X=0]"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-9}Inferences on the ATE from updated model}
\centering
\begin{tabular}[t]{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
Q 1 & - & posteriors & FALSE & 0.4802 & 0.0283 & 0.4242 & 0.5335\\
\hline
\end{tabular}
\end{table}

We see that the answers we get from the differences-in-means approach and the causal-model approach are about the same, as one would hope.

\hypertarget{going-further}{%
\section{Going further}\label{going-further}}

In principle, however, the causal models approach can let you do more. The third section of this guide is full of examples, but for a simple one consider the following: say, instead of wanting to know the average effect of \(X\) on \(Y\) you wanted to know, ``What is the probability that \(X\) caused \(Y\) in a case in which \(X=Y=1\)?''

This is a harder question. Differences-in-means is an estimation strategy tied to a particular estimand, and it does not provide a good answer to this question. However, with a causal model in hand, \emph{we can ask whatever causal question we like}, and get an answer plus estimates of uncertainty around the answer.

Here is the answer we get:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{query\_model}\NormalTok{(model, }
            \AttributeTok{using =} \StringTok{"posteriors"}\NormalTok{, }
            \AttributeTok{query =} \StringTok{"Y[X=0] == 0"}\NormalTok{,}
            \AttributeTok{given =} \StringTok{"X==1 \& Y==1"}\NormalTok{)  }
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-11}Causes of effects estimand}
\centering
\begin{tabular}[t]{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
Q 1 & X==1 \& Y==1 & posteriors & FALSE & 0.7936 & 0.0941 & 0.6296 & 0.9663\\
\hline
\end{tabular}
\end{table}

In this case we are asking \emph{for those cases in which \(X=1\) and \(Y=1\)}, what are the chances that \(Y\) would have been \(0\) if \(X\) were \(0\)?

Note, however, that while the model gave a precise answer to the ATE question, the answer for the causes-of-effects estimand is not precise. Moreover, more data won't reduce the uncertainty substantially. The reason is that this estimand is not identified.

This then is a situation in which we can ask a question about a quantity that is not identified and still learn a lot. We will encounter numerous examples like this as we explore different causal models.

\hypertarget{part-the-package}{%
\part{The Package}\label{part-the-package}}

\hypertarget{package}{%
\chapter{Installation}\label{package}}

You can install the package from Rstudio via:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"CausalQueries"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The package has some dependencies, the most important of which is \texttt{rstan}. Make sure you have \texttt{rstan} installed and working properly before doing anything else. Note that at installation \texttt{CausalQueries} compiles and stores a \texttt{stan} model so if you have problems with \texttt{stan} you won't be able to install \texttt{CausalQueries}.

\begin{itemize}
\tightlist
\item
  See the \href{https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started}{rstan getting started guide} for general installation procedures
\item
  Check that rstan is working by running one of the simple \texttt{stan} examples (see for example \texttt{?stan})
\item
  For windows users you might have to configure according to (these instructions{]}(\url{https://github.com/stan-dev/rstan/wiki/Installing-RStan-from-source-on-Windows\#configuration})
\item
  if all is working you should be able to run a simple model of the form:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make\_model}\NormalTok{(}\StringTok{"X{-}\textgreater{}Y"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{update\_model}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{defining-models}{%
\chapter{Defining models}\label{defining-models}}

\hypertarget{getting-going}{%
\section{Getting going}\label{getting-going}}

Model definition involves:

\begin{itemize}
\tightlist
\item
  Defining a \protect\hyperlink{structure}{causal structure} (required)
\item
  Indicating causal type \protect\hyperlink{restrictions}{restrictions} (optional)
\item
  Indicating possible unobserved \protect\hyperlink{confounding}{confounding} (optional)
\item
  Providing \protect\hyperlink{priors}{priors} and \protect\hyperlink{parameters}{parameters} (required, but defaults are provided)
\end{itemize}

We discuss these in turn.

\hypertarget{structure}{%
\section{Causal structure}\label{structure}}

A simple model is defined in one step using a \texttt{dagitty} syntax in which the structure of the model is provided as a statement.

For instance:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} M {-}\textgreater{} Y \textless{}{-} X"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The statement (in quotes) provides the names of nodes. An arrow (``\texttt{-\textgreater{}}'' or ``\texttt{\textless{}-}'') connecting nodes indicates that one node is a potential cause of another (that is, whether a given node is a ``parent'' or ``child'' of another; see section \ref{parents}).

Formally a statement like this is interpreted as:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Functional equations:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \(Y = f(M, X, \theta^Y)\)
\item
  \(M = f(X, \theta^M)\)
\item
  \(X = \theta^X\)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Distributions on shocks:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \(\Pr(\theta^i = \theta^i_k) = \lambda^i_k\)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Independence assumptions:\\
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \(\theta_i \perp\!\!\! \perp \theta_j, i\neq j\)
\end{itemize}

where function \(f\) maps from the set of possible values of the parents of \(i\) to values of node \(i\) given \(\theta^i\). Units with the same value on \(\theta^i\) react in the same way to the parents of \(i\). Indeed in this discrete world we think of \(\theta^i\) as fully dictating the functional form of \(f\), indicating what outcome is observed on \(i\) for any value of \(i\)'s parents.

In addition, it is also possible to indicate ``unobserved confounding'', that is, the presence of an unobserved variable that might influence observed variables. In this case condition 3 above is relaxed. We describe how this is done in greater detail in section \ref{confounding}.

For instance:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} Y \textless{}{-} W {-}\textgreater{} X; X \textless{}{-}\textgreater{} Y"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note that different segments of the graph can be included in the same statement, separated by semicolons. There are many ways to write the same model. Here is the same model written once using a three part statement and once as a chain (with the same node appearing possibly more than once).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"W {-}\textgreater{} X; W {-}\textgreater{} Y; X {-}\textgreater{} Y"}\NormalTok{)}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} Y \textless{}{-} W {-}\textgreater{} X"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Once defined, a model can be graphed (we use the \texttt{dagitty} package for this):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} Y \textless{}{-} W {-}\textgreater{} X; X \textless{}{-}\textgreater{} Y"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{causalmodels_files/figure-latex/unnamed-chunk-16-1.pdf}
\caption{\label{fig:unnamed-chunk-16}A plotted model. Curved double headed arrows indicate unobserved confounding}
\end{figure}

This is useful to check that you have written the structure down correctly.

When a model is defined, a set of objects are generated. These are the key quantities that are used for all inference. We discuss each in turn. (See the notation guide---section \ref{notation}---for definitions and code pointers).

\hypertarget{nodal-types}{%
\subsection{Nodal types}\label{nodal-types}}

Two units have the same \emph{nodal type} at node \(Y\), \(\theta^Y\), if their outcome at \(Y\) responds in the same ways to parents of \(Y\).

A node with \(k\) parents has \(2^{2^k}\) nodal types. The reason is that with \(k\) parents, there are \(2^k\) possible values of the parents and so \(2^{2^k}\) ways to respond to these possible parental values. As a convention we say that a node with no parents has two nodal types (0 or 1).

When a model is created the full set of ``nodal types'' is identified. These are stored in the model. The subscripts become very long and hard to parse for more complex models so the model object also includes a guide to interpreting nodal type values. You can see them like this.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} Y"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{nodal\_types}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$X
[1] "0" "1"

$Y
[1] "00" "10" "01" "11"

attr(,"interpret")
attr(,"interpret")$X
  node position display interpretation
1    X       NA      X0          X = 0
2    X       NA      X1          X = 1

attr(,"interpret")$Y
  node position display interpretation
1    Y        1   Y[*]*      Y | X = 0
2    Y        2   Y*[*]      Y | X = 1
\end{verbatim}

Note that we use \(\theta^j\) to indicate nodal types because for qualitative analysis the nodal types are often the parameters of interest.

\hypertarget{causal-types}{%
\subsection{Causal types}\label{causal-types}}

Causal types are collections of nodal types. Two units are of the same \emph{causal type} if they have the same nodal type at every node. For example in a \(X \rightarrow M \rightarrow Y\) model, \(\theta = (\theta^X_0, \theta^M_{01}, \theta^Y_{10})\) is a type that has \(X=0\), \(M\) responds positively to \(X\), and \(Y\) responds positively to \(M\).

When a model is created, the full set of causal types is identified. These are stored in the model object:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make\_model}\NormalTok{(}\StringTok{"A {-}\textgreater{} B"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{causal\_types}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       A  B
A0.B00 0 00
A1.B00 1 00
A0.B10 0 10
A1.B10 1 10
A0.B01 0 01
A1.B01 1 01
A0.B11 0 11
A1.B11 1 11
\end{verbatim}

A model with \(n_j\) nodal types at node \(j\) has \(\prod_jn_j\) causal types. Thus the set of causal types can be large. In the model \((X\rightarrow M \rightarrow Y \leftarrow X)\) there are \(2\times 4\times 16 = 128\) causal types.

Knowledge of a causal type tells you what values a unit would take, on all nodes, absent an intervention. For example for a model \(X \rightarrow M \rightarrow Y\) a type \(\theta = (\theta^X_0, \theta^M_{01}, \theta^Y_{10})\) would imply data \((X=0, M=0, Y=1)\). (The converse of this, of course, is the key to updating: observation of data \((X=0, M=0, Y=1)\) result in more weight placed on \(\theta^X_0\), \(\theta^M_{01}\), and \(\theta^Y_{10})\).)

\hypertarget{parameters-dataframe}{%
\subsection{Parameters dataframe}\label{parameters-dataframe}}

When a model is created, \texttt{CausalQueries} attaches a ``parameters dataframe'' which keeps track of model parameters, which belong together in a family, and how they relate to causal types. This becomes especially important for more complex models with confounding that might involve more complicated mappings between parameters and nodal types. In the case with no confounding the nodal types \emph{are} the parameters; in cases with confounding you generally have more parameters than nodal types.

For instance:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make\_model}\NormalTok{(}\StringTok{"X{-}\textgreater{}Y"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{parameters\_df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|r|l|l|l|r|r}
\hline
param\_names & node & gen & param\_set & nodal\_type & given & param\_value & priors\\
\hline
X.0 & X & 1 & X & 0 &  & 0.50 & 1\\
\hline
X.1 & X & 1 & X & 1 &  & 0.50 & 1\\
\hline
Y.00 & Y & 2 & Y & 00 &  & 0.25 & 1\\
\hline
Y.10 & Y & 2 & Y & 10 &  & 0.25 & 1\\
\hline
Y.01 & Y & 2 & Y & 01 &  & 0.25 & 1\\
\hline
Y.11 & Y & 2 & Y & 11 &  & 0.25 & 1\\
\hline
\end{tabular}

Each row in the dataframe corresponds to a single parameter.

The columns of the parameters data frame are understood as follows:

\begin{itemize}
\tightlist
\item
  \texttt{param\_names} gives the name of the parameter, in shorthand. For instance the parameter \(\lambda^X_0 = \Pr(\theta^X = \theta^X_0)\) has \texttt{par\_name} \texttt{X.0}. See section \ref{notation} for a summary of notation.
\item
  \texttt{param\_value} gives the (possibly default) parameter values. These are probabilities.\\
\item
  \texttt{param\_set} indicates which parameters group together to form a simplex. The parameters in a set have parameter values that sum to 1. In this example \(\lambda^X_0 + \lambda^X_1 = 1\).
\item
  \texttt{node} indicates the node associated with the parameter. For parameter \texttt{\textbackslash{}lambda\^{}X\_0} this is \(X\).
\item
  \texttt{nodal\_type} indicates the nodal types associated with the parameter.
\item
  \texttt{gen} indicates the place in the partial causal ordering (generation) of the node associated with the parameter
\item
  \texttt{priors} gives (possibly default) Dirichlet priors arguments for parameters in a set. Values of 1 (.5) for all parameters in a set implies uniform (Jeffrey's) priors over this set.
\end{itemize}

Below we will see examples where the parameter dataframe helps keep track of parameters that are created when confounding is added to a model.

\hypertarget{parameter-matrix}{%
\subsection{Parameter matrix}\label{parameter-matrix}}

The parameters dataframe keeps track of parameter values and priors for parameters but it does not provide a mapping between parameters and the probability of causal types.

The parameter matrix (\(P\) matrix) is added to the model to provide this mapping. The \(P\) matrix has a row for each parameter and a column for each causal type. For instance:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make\_model}\NormalTok{(}\StringTok{"X{-}\textgreater{}Y"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ get\_parameter\_matrix }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  kable}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r|r|r|r|r|r|r}
\hline
  & X0.Y00 & X1.Y00 & X0.Y10 & X1.Y10 & X0.Y01 & X1.Y01 & X0.Y11 & X1.Y11\\
\hline
X.0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0\\
\hline
X.1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1\\
\hline
Y.00 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\
\hline
Y.10 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0\\
\hline
Y.01 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0\\
\hline
Y.11 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1\\
\hline
\end{tabular}

The probability of a causal type is given by the product of the parameters values for parameters whose row in the \(P\) matrix contains a 1.

Below we will see examples where the \(P\) matrix helps keep track of parameters that are created when confounding is added to a model.

\hypertarget{restrictions}{%
\section{Setting restrictions}\label{restrictions}}

When a model is defined, the complete set of possible causal relations are identified. This set can be very large.

Sometimes for theoretical or practical reasons it is useful to constrain the set of types. In \texttt{CausalQueries} this is done at the level of nodal types, with restrictions on causal types following from restrictions on nodal types.

For instance to impose an assumption that \(Y\) is not decreasing in \(X\) we generate a restricted model as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X{-}\textgreater{}Y"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{set\_restrictions}\NormalTok{(}\StringTok{"Y[X=1] \textless{} Y[X=0]"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

or:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X{-}\textgreater{}Y"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{set\_restrictions}\NormalTok{(}\FunctionTok{decreasing}\NormalTok{(}\StringTok{"X"}\NormalTok{, }\StringTok{"Y"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Viewing the resulting parameter matrix we see that both the set of parameters and the set of causal types are now restricted:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get\_parameter\_matrix}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Rows are parameters, grouped in parameter sets

Columns are causal types

Cell entries indicate whether a parameter probability is used
in the calculation of causal type probability

     X0.Y00 X1.Y00 X0.Y01 X1.Y01 X0.Y11 X1.Y11
X.0       1      0      1      0      1      0
X.1       0      1      0      1      0      1
Y.00      1      1      0      0      0      0
Y.01      0      0      1      1      0      0
Y.11      0      0      0      0      1      1

 
 param_set  (P)
 
\end{verbatim}

Here and in general, setting restrictions typically involves using causal syntax; see Section \ref{syntax} for a guide the syntax used by \texttt{CausalQueries}.

Note:

\begin{itemize}
\tightlist
\item
  Restrictions have to operate on nodal types: restrictions on \emph{levels} of endogenous nodes aren't allowed. This, for example, will fail:
  \texttt{make\_model("X-\textgreater{}Y")\ \%\textgreater{}\%\ set\_restrictions(statement\ =\ \ "(Y\ ==\ 1)")}. The reason is that it requests a correlated restriction on nodal types for \texttt{X} and \texttt{Y} which involves undeclared confounding.
\item
  Restrictions implicitly assume fixed values for \emph{all} parents of a node. For instance: \texttt{make\_model("A\ -\textgreater{}\ B\ \textless{}-\ C")\ \%\textgreater{}\%\ set\_restrictions("(B{[}C=1{]}==1)")} is interpreted as shorthand for the restriction \texttt{"B{[}C\ =\ 1,\ A\ =\ 0{]}==1\ \textbar{}\ B{[}C\ =\ 1,\ A\ =\ 1{]}==1"}.
\item
  To place restrictions on multiple nodes at the same time, provide these as a vector of restrictions. This is not permitted: \texttt{set\_restrictions("Y{[}X=1{]}==1\ \&\ X==1")}, since it requests correlated restrictions. This however is allowed: \texttt{set\_restrictions(c("Y{[}X=1{]}==1",\ "X==1"))}.\\
\item
  Use the \texttt{keep} argument to indicate whether nodal types should be dropped (default) or retained.
\item
  Restrictions can be set using nodal type labels. \texttt{make\_model("S\ -\textgreater{}\ C\ -\textgreater{}\ Y\ \textless{}-\ R\ \textless{}-\ X;\ X\ -\textgreater{}\ C\ -\textgreater{}\ R")\ \%\textgreater{}\%\ set\_restrictions(labels\ =\ list(C\ =\ "C1000",\ R\ =\ "R0001",\ Y\ =\ "Y0001"),\ keep\ =\ TRUE)}
\item
  Wild cards can be used in nodal type labels: \texttt{make\_model("X-\textgreater{}Y")\ \%\textgreater{}\%\ set\_restrictions(labels\ =\ list(Y\ =\ "Y?0"))}
\end{itemize}

\hypertarget{confounding}{%
\section{Allowing confounding}\label{confounding}}

(Unobserved) confounding between two nodes arises when the nodal types for the nodes are not independently distributed.

In the \(X \rightarrow Y\) graph, for instance, there are 2 nodal types for \(X\) and 4 for \(Y\). There are thus 8 joint nodal types (or causal types):

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0517}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0690}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3448}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3448}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1897}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\theta^X\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
& & 0 & 1 & Sum \\
\(\theta^Y\) & 00 & \(\Pr(\theta^X_0, \theta^Y_{00})\) & \(\Pr(\theta^X_1, \theta^Y_{00})\) & \(\Pr(\theta^Y_{00})\) \\
& 10 & \(\Pr(\theta^X_0, \theta^Y_{10})\) & \(\Pr(\theta^X_1, \theta^Y_{10})\) & \(\Pr(\theta^Y_{10})\) \\
& 01 & \(\Pr(\theta^X_0, \theta^Y_{01})\) & \(\Pr(\theta^X_1, \theta^Y_{01})\) & \(\Pr(\theta^Y_{01})\) \\
& 11 & \(\Pr(\theta^X_0, \theta^Y_{11})\) & \(\Pr(\theta^X_1, \theta^Y_{11})\) & \(\Pr(\theta^Y_{11})\) \\
& Sum & \(\Pr(\theta^X_0)\) & \(\Pr(\theta^X_1)\) & 1 \\
\end{longtable}

This table has 8 interior elements and so an unconstrained joint distribution would have 7 degrees of freedom. A no confounding assumption means that \(\Pr(\theta^X | \theta^Y) = \Pr(\theta^X)\), or \(\Pr(\theta^X, \theta^Y) = \Pr(\theta^X)\Pr(\theta^Y)\). In this case we just put a distribution on the marginals and there would be 3 degrees of freedom for \(Y\) and 1 for \(X\), totaling \(4\) rather than 7.

\texttt{set\_confounds} lets you relax this assumption by increasing the number of parameters characterizing the joint distribution. Using the fact that \(\Pr(A,B) = \Pr(A)\Pr(B|A)\) new parameters are introduced to capture \(\Pr(B|A=a)\) rather than simply \(\Pr(B)\).

The simplest way to allow for confounding is by adding a bidirected edge, such as via: \texttt{set\_confound(model,\ list("X\ \textless{}-\textgreater{}\ Y"))}. In this case the descendant node has a distribution conditional on the value of the ancestor node. To wit:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{confounded }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X{-}\textgreater{}Y"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_confound}\NormalTok{(}\StringTok{"X \textless{}{-}\textgreater{} Y"}\NormalTok{)}

\NormalTok{confounded}\SpecialCharTok{$}\NormalTok{parameters\_df }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ kable}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|r|l|l|l|r|r}
\hline
param\_names & node & gen & param\_set & nodal\_type & given & param\_value & priors\\
\hline
X.0 & X & 1 & X & 0 &  & 0.50 & 1\\
\hline
X.1 & X & 1 & X & 1 &  & 0.50 & 1\\
\hline
Y.00\_X.0 & Y & 2 & Y.X.0 & 00 & X.0 & 0.25 & 1\\
\hline
Y.10\_X.0 & Y & 2 & Y.X.0 & 10 & X.0 & 0.25 & 1\\
\hline
Y.01\_X.0 & Y & 2 & Y.X.0 & 01 & X.0 & 0.25 & 1\\
\hline
Y.11\_X.0 & Y & 2 & Y.X.0 & 11 & X.0 & 0.25 & 1\\
\hline
Y.00\_X.1 & Y & 2 & Y.X.1 & 00 & X.1 & 0.25 & 1\\
\hline
Y.10\_X.1 & Y & 2 & Y.X.1 & 10 & X.1 & 0.25 & 1\\
\hline
Y.01\_X.1 & Y & 2 & Y.X.1 & 01 & X.1 & 0.25 & 1\\
\hline
Y.11\_X.1 & Y & 2 & Y.X.1 & 11 & X.1 & 0.25 & 1\\
\hline
\end{tabular}

We see here that there are now two parameter families for parameters associated with the node \(Y\). Each family captures the conditional distribution of \(Y\)'s nodal types, given \(X\). For instance the parameter \texttt{Y01\_X.1} can be interpreted as \(\Pr(\theta^Y = \theta^Y _{01} | X=1)\).

To see exactly how the parameters map to causal types we can view the parameter matrix:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get\_parameter\_matrix}\NormalTok{(confounded) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ kable}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r|r|r|r|r|r|r}
\hline
  & X0.Y00 & X1.Y00 & X0.Y10 & X1.Y10 & X0.Y01 & X1.Y01 & X0.Y11 & X1.Y11\\
\hline
X.0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0\\
\hline
X.1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1\\
\hline
Y.00\_X.0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
\hline
Y.10\_X.0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\
\hline
Y.01\_X.0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\
\hline
Y.11\_X.0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\
\hline
Y.00\_X.1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\
\hline
Y.10\_X.1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\
\hline
Y.01\_X.1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\
\hline
Y.11\_X.1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\
\hline
\end{tabular}

Importantly, the \(P\) matrix works as before, despite confounding. We can assess the probability of causal types by multiplying the probabilities of the constituent parameters.

Note:

\begin{itemize}
\item
  Ordering of conditioning can also be controlled however via \texttt{set\_confound(model,\ list(X\ =\ "Y"))} in which case X is given a distribution conditional on nodal types of Y.
\item
  More specific confounding statements are also possible using causal syntax.

  \begin{itemize}
  \tightlist
  \item
    A statement of the form \texttt{list(X\ =\ "Y{[}X=1{]}==1")} can be interpreted as: ``Allow X to have a distinct conditional distribution when \(Y\) has types that involve \(Y(do(X=1))=1\).'' In this case, nodal types for \(Y\) would continue to have 3 degrees of freedom. But there would be parameters assigning the probability of \(X\) when \(\theta^Y = \theta^Y_{01}\) or \(\theta^Y = \theta^Y_{11}\) and other parameters for residual cases. Thus 6 degrees of freedom in all.
  \item
    Similarly a statement of the form \texttt{list(Y\ =\ "X==1")} can be interpreted as: ``Allow Y to have a distinct conditional distribution when X=1.'' In this case there would be two distributions over nodal types for Y, producing 2*3 = 6 degrees of freedom. Nodal types for X would continue to have 1 degree of freedom. Thus 7 degrees of freedom in all, corresponding to a fully unconstrained joint distribution.
  \end{itemize}
\item
  Unlike nodal restrictions, a confounding relation can involve multiple nodal types simultaneously. For instance \texttt{make\_model("X\ -\textgreater{}\ M\ -\textgreater{}\ Y")\ \%\textgreater{}\%\ set\_confound(list(X\ =\ "Y{[}X=1{]}\ \textgreater{}\ Y{[}X=0{]}"))} allows for a parameter that renders \(X\) more or less likely depending on whether \(X\) has a positive effect on \(Y\) whether it runs through a positive or a negative effect on \(M\).
\item
  The parameters needed to capture confounding relations depend on the direction of causal arrows. For example compare:

  \begin{itemize}
  \tightlist
  \item
    \texttt{make\_model("A\ -\textgreater{}\ W\ \textless{}-\ B\ ;\ A\ \textless{}-\textgreater{}\ W;\ B\ \textless{}-\textgreater{}\ W")\$parameters\_df\ \%\textgreater{}\%\ dim} In this case we can decompose shocks on \(A, B, W\) via: \(\Pr(\theta^A, \theta^B, \theta^W) = \Pr(\theta^W | \theta^A, \theta^A)\Pr(\theta^A)\Pr(\theta^B)\), and we have 68 parameters.
  \item
    \texttt{make\_model("A\ \textless{}-\ W\ -\textgreater{}\ B\ ;\ A\ \textless{}-\textgreater{}\ W;\ B\ \textless{}-\textgreater{}\ W")\$parameters\_df\ \%\textgreater{}\%\ dim} In this case we have \(\Pr(\theta^A, \theta^B, \theta^W) = \Pr(\theta^A | \theta^W)\Pr(\theta^B|\theta^W)\Pr(\theta^W)\) and just has just 18 parameters.
  \end{itemize}
\end{itemize}

When confounding is added to a model, a dataframe, \texttt{confounds\_df} is created and added to the model, recording which variables involve confounding. This is then used for plotting:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make\_model}\NormalTok{(}\StringTok{"A \textless{}{-} X {-}\textgreater{} B; A \textless{}{-}\textgreater{} X; B \textless{}{-}\textgreater{} X"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{causalmodels_files/figure-latex/unnamed-chunk-26-1.pdf}

Sometimes the \texttt{confounds\_df} can highlight nonobvious confounding relations:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} M {-}\textgreater{} Y"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_confound}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{X =} \StringTok{"Y[X=1] \textgreater{} Y[X=0]"}\NormalTok{))}
\NormalTok{model}\SpecialCharTok{$}\NormalTok{confounds\_df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
NULL
\end{verbatim}

In this example, the confounding statement implies confounding between \(X\) and \(M\) even though \(M\) is not included explicitly in the confound statement (the reason is that \(X\) can have a positive effect on \(Y\) by having a positive effect on \(M\) and this in turn having a positive effect on \(Y\) \emph{or} by having a negative effect on \(M\) and this in turn having a negative effect on \(Y\)).

\hypertarget{priors}{%
\section{Setting Priors}\label{priors}}

Priors on model parameters can be added to the parameters dataframe. The priors are interpreted as alpha arguments for a Dirichlet distribution. They can be seen using \texttt{get\_priors}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make\_model}\NormalTok{(}\StringTok{"X{-}\textgreater{}Y"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ get\_priors}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 X.0  X.1 Y.00 Y.10 Y.01 Y.11 
   1    1    1    1    1    1 
\end{verbatim}

Here the priors have not been specified and so they default to 1, which corresponds to uniform priors.

Alternatively you could set jeffreys priors like this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make\_model}\NormalTok{(}\StringTok{"X{-}\textgreater{}Y"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{set\_priors}\NormalTok{(}\AttributeTok{distribution =} \StringTok{"jeffreys"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ get\_priors}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
no specific parameters to alter values for specified. Altering all parameters.
\end{verbatim}

\begin{verbatim}
 X.0  X.1 Y.00 Y.10 Y.01 Y.11 
 0.5  0.5  0.5  0.5  0.5  0.5 
\end{verbatim}

\hypertarget{custom-priors}{%
\subsection{Custom priors}\label{custom-priors}}

Custom priors are most simply specified by being added as a vector of numbers using \texttt{set\_priors}. For instance:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make\_model}\NormalTok{(}\StringTok{"X{-}\textgreater{}Y"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{set\_priors}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ get\_priors}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 X.0  X.1 Y.00 Y.10 Y.01 Y.11 
   1    2    3    4    5    6 
\end{verbatim}

The priors here should be interpreted as indicating:

\begin{itemize}
\tightlist
\item
  \(\alpha_X = (1,2)\), which implies a distribution over \((\lambda^X_0, \lambda^X_1)\) centered on \((1/3, 2/3)\).
\item
  \(\alpha_Y = (3,4,5,6)\), which implies a distribution over \((\lambda^Y_{00}, \lambda^Y_{10}, \lambda^Y_{01} \lambda^Y_{11})\) centered on \((3/18, 4/18, 5/18, 6/18)\).
\end{itemize}

For larger models it can be hard to provide priors as a vector of numbers and so \texttt{set\_priors} can allow for more targeted modifications of the parameter vector. For instance:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make\_model}\NormalTok{(}\StringTok{"X{-}\textgreater{}Y"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_priors}\NormalTok{(}\AttributeTok{statement =} \StringTok{"Y[X=1] \textgreater{} Y[X=0]"}\NormalTok{, }\AttributeTok{alphas =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  get\_priors}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 X.0  X.1 Y.00 Y.10 Y.01 Y.11 
   1    1    1    1    3    1 
\end{verbatim}

See \texttt{?set\_priors} and \texttt{?make\_priors} for many examples.

\hypertarget{prior-warnings}{%
\subsection{Prior warnings}\label{prior-warnings}}

``Flat'' priors over parameters in a parameter family put equal weight on each nodal type, but this in turn can translate into strong assumptions on causal quantities of interest.

For instance in an \(X \rightarrow Y\) model model in which negative effects are ruled out, the average causal effect implied by ``flat'' priors is \(1/3\). This can be seen by querying the model:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} Y"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_restrictions}\NormalTok{(}\FunctionTok{decreasing}\NormalTok{(}\StringTok{"X"}\NormalTok{, }\StringTok{"Y"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{query\_model}\NormalTok{(}\StringTok{"Y[X=1] {-} Y[X=0]"}\NormalTok{, }\AttributeTok{n\_draws =} \DecValTok{10000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  kable}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|l|r}
\hline
Query & Given & Using & Case.estimand & mean\\
\hline
Q 1 & - & parameters & FALSE & 0.3333\\
\hline
\end{tabular}

More subtly the \emph{structure} of a model, coupled with flat priors, has substantive importance for priors on causal quantities. For instance with flat priors, priors on the probability that \(X\) has a positive effect on \(Y\) in the model \(X \rightarrow Y\) is centered on \(1/4\). But priors on the probability that \(X\) has a positive effect on \(Y\) in the model \(X \rightarrow M \rightarrow Y\) is centered on \(1/8\).

Again, you can use \texttt{query\_model} to figure out what flat (or other) priors over parameters imply for priors over causal quantities:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} Y"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{query\_model}\NormalTok{(}\StringTok{"Y[X=1] \textgreater{} Y[X=0]"}\NormalTok{, }\AttributeTok{n\_draws =} \DecValTok{10000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  kable}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|l|r}
\hline
Query & Given & Using & Case.estimand & mean\\
\hline
Q 1 & - & parameters & FALSE & 0.25\\
\hline
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} M {-}\textgreater{} Y"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{query\_model}\NormalTok{(}\StringTok{"Y[X=1] \textgreater{} Y[X=0]"}\NormalTok{, }\AttributeTok{n\_draws =} \DecValTok{10000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  kable}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|l|r}
\hline
Query & Given & Using & Case.estimand & mean\\
\hline
Q 1 & - & parameters & FALSE & 0.125\\
\hline
\end{tabular}

Caution regarding priors is particularly important when models are not identified, as is the case for many of the models considered here. In such cases, for some quantities, the marginal posterior distribution can be the same as the marginal prior distribution \citep{poirier1998revising}.

The key point here is to make sure you do not fall into a trap of thinking that ``uninformative'' priors make no commitments regarding the values of causal quantities of interest. They do, and the implications of flat priors for causal quantities can depend on the structure of the model. Moreover for some inferences from causal models the priors can matter a lot even if you have a lot of data. In such cases it can be helpful to know what priors on parameters imply for priors on causal quantities of interest (by using \texttt{query\_model}) and to assess how much conclusions depend on priors (by comparing results across models that vary in their priors).

\hypertarget{parameters}{%
\section{Setting Parameters}\label{parameters}}

By default, models have a vector of parameter values included in the \texttt{parameters\_df} dataframe. These are useful for generating data, or for situations, such as process tracing, when one wants to make inferences about causal types (\(\theta\)), given case level data, under the assumption that the model is known.

Consider the causal model below. It has two parameter sets, X and Y, with six nodal types, two corresponding to X and four corresponding to Y. The key feature of the parameters is that they must sum to 1 within each parameter set.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make\_model}\NormalTok{(}\StringTok{"X{-}\textgreater{}Y"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{get\_parameters}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 X.0  X.1 Y.00 Y.10 Y.01 Y.11 
0.50 0.50 0.25 0.25 0.25 0.25 
\end{verbatim}

Setting parameters can be done using a similar syntax as \texttt{set\_priors}. The main difference is that when a given value is altered the entire set must still always sum to 1. The example below illustrates a change in the value of the parameter \(Y\) in the case it is increasing in \(X\). Here nodal type \texttt{Y.Y01} is set to be .5, while the other nodal types of this parameter set were renormalized so that the parameters in the set still sum to one.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make\_model}\NormalTok{(}\StringTok{"X{-}\textgreater{}Y"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_parameters}\NormalTok{(}\AttributeTok{statement =} \StringTok{"Y[X=1] \textgreater{} Y[X=0]"}\NormalTok{, }\AttributeTok{parameters =}\NormalTok{ .}\DecValTok{5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  get\_parameters}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   X.0    X.1   Y.00   Y.10   Y.01   Y.11 
0.5000 0.5000 0.1667 0.1667 0.5000 0.1667 
\end{verbatim}

Alternatively, instead of setting a particular new value for a parameter you can set a value that then gets renormalized along with all other values. In the example below, if we begin with vector (.25, .25, .25, .25) and request a value of \texttt{Y.Y01} of .5, \emph{without} requesting a renormalization of other variables, then we get a vector (.2, .2, .4, .2) which is itself a renormalization of (.25, .25, .5, .25).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make\_model}\NormalTok{(}\StringTok{"X{-}\textgreater{}Y"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_parameters}\NormalTok{(}\AttributeTok{statement =} \StringTok{"Y[X=1] \textgreater{} Y[X=0]"}\NormalTok{, }\AttributeTok{parameters =}\NormalTok{ .}\DecValTok{5}\NormalTok{, }\AttributeTok{normalize=}\ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  get\_parameters}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 X.0  X.1 Y.00 Y.10 Y.01 Y.11 
 0.5  0.5  0.2  0.2  0.4  0.2 
\end{verbatim}

This normalization behavior can mean that you can control parameters better if they are set in a single step rather than in multiple steps, compare:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make\_model}\NormalTok{(}\StringTok{\textquotesingle{}X {-}\textgreater{} Y\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{make\_parameters}\NormalTok{(}\AttributeTok{statement =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Y[X=1]\textless{}Y[X=0] | Y[X=1]\textgreater{}Y[X=0]\textquotesingle{}}\NormalTok{), }\AttributeTok{parameters =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 X.0  X.1 Y.00 Y.10 Y.01 Y.11 
0.50 0.50 0.25 0.50 0.00 0.25 
\end{verbatim}

Here the .6 in the second vector arises because a two step process is requested (\texttt{statement} is of length 2) and the vector first becomes (1/6, 1/2, 1/6, 1/6) and then becomes (.2, .6, 0, .2) which is a renormalization of (1/6, 1/2, 0, 1/6).

If in doubt, check parameter values.

See \texttt{?\ set\_parameters} for some handy ways to set parameters either manually (\texttt{define}) or using \texttt{prior\_mean}, \texttt{prior\_draw}, \texttt{posterior\_mean}, \texttt{posterior\_draw}.

\hypertarget{updating-models-with-stan}{%
\chapter{\texorpdfstring{Updating models with \texttt{stan}}{Updating models with stan}}\label{updating-models-with-stan}}

When we generate a model we often impose a lot of assumptions on nature of causal relations. This includes ``structure'' regarding what relates to what but also the nature of those relations---how strong the effect of a given variable is and how it interacts with others, for example. The latter features are captured by parameters whose values, fortunately, can be data based.

The approach used by the \texttt{CausalQueries} package to updating parameter values given observed data uses \texttt{stan} and involves the following elements:

\begin{itemize}
\tightlist
\item
  Dirichlet priors over parameters, \(\lambda\) (which, in cases without confounding, correspond to nodal types)
\item
  A mapping from parameters to event probabilities, \(w\)
\item
  A likelihood function that assumes events are distributed according to a multinomial distribution given event probabilities.
\end{itemize}

We provide further details below.

\hypertarget{data-for-stan}{%
\section{\texorpdfstring{Data for \texttt{stan}}{Data for stan}}\label{data-for-stan}}

We use a generic \texttt{stan} model that works for all binary causal models. Rather than writing a new \texttt{stan} model for each causal model we send \texttt{stan} details of each particular causal model as data inputs.

In particular we provide a set of matrices that \texttt{stan} tailor itself to particular models: the parameter matrix (\(P\) ) tells \texttt{stan} how many parameters there are, and how they map into causal types; an ambiguity matrix \(A\) tells \texttt{stan} how causal types map into data types; and an event matrix \(E\) relates data types into patterns of observed data (in cases where there are incomplete observations).

The internal function \texttt{prep\_stan\_data} prepares data for \texttt{stan}. You generally don't need to use this manually, but we show here a sample of what it produces as input for \texttt{stan}.

We provide \texttt{prep\_stan\_data} with data in compact form (listing ``data events'').

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X{-}\textgreater{}Y"}\NormalTok{)}

\NormalTok{data  }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{X =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\ConstantTok{NA}\NormalTok{), }\AttributeTok{Y =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)) }

\NormalTok{compact\_data }\OtherTok{\textless{}{-}}  \FunctionTok{collapse\_data}\NormalTok{(data, model) }

\FunctionTok{kable}\NormalTok{(compact\_data)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|r}
\hline
event & strategy & count\\
\hline
X0Y0 & XY & 1\\
\hline
X1Y0 & XY & 1\\
\hline
X0Y1 & XY & 0\\
\hline
X1Y1 & XY & 1\\
\hline
Y0 & Y & 0\\
\hline
Y1 & Y & 1\\
\hline
\end{tabular}

Note that NAs are interpreted as data not having been sought. So in this case the interpretation is that there are two data strategies: data on \(Y\) and \(X\) was sought in three cases; data on \(Y\) only was sought in just one case.

\texttt{prep\_stan\_data} then returns a list of objects that \texttt{stan} expects to receive. These include indicators to figure out where a parameter set starts (\texttt{l\_starts}, \texttt{l\_ends}) and ends and where a data strategy starts and ends (\texttt{strategy\_starts}, \texttt{strategy\_ends}), as well as the matrices described above.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CausalQueries}\SpecialCharTok{:::}\FunctionTok{prep\_stan\_data}\NormalTok{(model, compact\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
$parmap
     X0Y0 X1Y0 X0Y1 X1Y1
X.0     1    0    1    0
X.1     0    1    0    1
Y.00    1    1    0    0
Y.10    0    1    1    0
Y.01    1    0    0    1
Y.11    0    0    1    1
attr(,"map")
     X0Y0 X1Y0 X0Y1 X1Y1
X0Y0    1    0    0    0
X1Y0    0    1    0    0
X0Y1    0    0    1    0
X1Y1    0    0    0    1

$map
     X0Y0 X1Y0 X0Y1 X1Y1
X0Y0    1    0    0    0
X1Y0    0    1    0    0
X0Y1    0    0    1    0
X1Y1    0    0    0    1

$n_paths
[1] 4

$n_params
[1] 6

$n_param_sets
[1] 2

$n_param_each
X Y 
2 4 

$l_starts
X Y 
1 3 

$l_ends
X Y 
2 6 

$node_starts
X Y 
1 3 

$node_ends
X Y 
2 6 

$n_nodes
[1] 2

$lambdas_prior
 X.0  X.1 Y.00 Y.10 Y.01 Y.11 
   1    1    1    1    1    1 

$n_data
[1] 4

$n_events
[1] 6

$n_strategies
[1] 2

$strategy_starts
[1] 1 5

$strategy_ends
[1] 4 6

$keep_transformed
[1] 1

$E
     X0Y0 X1Y0 X0Y1 X1Y1
X0Y0    1    0    0    0
X1Y0    0    1    0    0
X0Y1    0    0    1    0
X1Y1    0    0    0    1
Y0      1    1    0    0
Y1      0    0    1    1

$Y
[1] 1 1 0 1 0 1

$P

Rows are parameters, grouped in parameter sets

Columns are causal types

Cell entries indicate whether a parameter probability is used
in the calculation of causal type probability

     X0.Y00 X1.Y00 X0.Y10 X1.Y10 X0.Y01 X1.Y01 X0.Y11
X.0       1      0      1      0      1      0      1
X.1       0      1      0      1      0      1      0
Y.00      1      1      0      0      0      0      0
Y.10      0      0      1      1      0      0      0
Y.01      0      0      0      0      1      1      0
Y.11      0      0      0      0      0      0      1
     X1.Y11
X.0       0
X.1       1
Y.00      0
Y.10      0
Y.01      0
Y.11      1

 
 param_set  (P)
 
$n_types
[1] 8
\end{verbatim}

\hypertarget{stan-code}{%
\section{\texorpdfstring{\texttt{stan} code}{stan code}}\label{stan-code}}

Below we show the \texttt{stan} code. This starts off with a block saying what input data is to be expected. Then there is a characterization of parameters and the transformed parameters. Then the likelihoods and priors are provided. \texttt{stan} takes it from there and generates a posterior distribution.

\begin{verbatim}
functions{
  row_vector col_sums(matrix X) {
    row_vector[cols(X)] s ;
    s = rep_row_vector(1, rows(X)) * X ;
    return s ;
  }
}
data {
int<lower=1> n_params;
int<lower=1> n_paths;
int<lower=1> n_types;
int<lower=1> n_param_sets;
int<lower=1> n_nodes;
int<lower=1> n_param_each[n_param_sets];
int<lower=1> n_data;
int<lower=1> n_events;
int<lower=1> n_strategies;
int<lower=0, upper=1> keep_transformed;
vector<lower=0>[n_params] lambdas_prior;
int<lower=1> l_starts[n_param_sets];
int<lower=1> l_ends[n_param_sets];
int<lower=1> node_starts[n_nodes];
int<lower=1> node_ends[n_nodes];
int<lower=1> strategy_starts[n_strategies];
int<lower=1> strategy_ends[n_strategies];
matrix[n_params, n_types] P;
matrix[n_params, n_paths] parmap;
matrix[n_paths, n_data] map;
matrix<lower=0,upper=1>[n_events,n_data] E;
int<lower=0> Y[n_events];
}
parameters {
vector<lower=0>[n_params - n_param_sets] gamma;
}
transformed parameters {
vector<lower=0>[n_params] lambdas;
vector<lower=1>[n_param_sets] sum_gammas;
matrix[n_params, n_paths] parlam;
matrix[n_nodes, n_paths] parlam2;
vector<lower=0, upper=1>[n_paths] w_0;
vector<lower=0, upper=1>[n_data] w;
vector[n_events] w_full;
// Cases in which a parameter set has only one value need special handling
// they have no gamma components and sum_gamma needs to be made manually
for (i in 1:n_param_sets) {
if (l_starts[i] >= l_ends[i]) {
  sum_gammas[i] = 1;
  // syntax here to return unity as a vector
  lambdas[l_starts[i]] = lambdas_prior[1]/lambdas_prior[1];
  }
else if (l_starts[i] < l_ends[i]) {
  sum_gammas[i] =
  1 + sum(gamma[(l_starts[i] - (i-1)):(l_ends[i] - i)]);
  lambdas[l_starts[i]:l_ends[i]] =
  append_row(1, gamma[(l_starts[i] - (i-1)):(l_ends[i] - i)]) / sum_gammas[i];
  }
  }
// Mapping from parameters to data types
parlam  = rep_matrix(lambdas, n_paths) .* parmap; // (usual case): [n_par * n_data] * [n_par * n_data]
// Sum probability over nodes on each path
for (i in 1:n_nodes) {
 parlam2[i,] = col_sums(parlam[(node_starts[i]):(node_ends[i]),]);
 }
// then take product  to get probability of data type on path
for (i in 1:n_paths) {
  w_0[i] = prod(parlam2[,i]);
 }
 // last (if confounding): map to n_data columns instead of n_paths
 w = map'*w_0;
 w = w / sum(w);
 w_full = E * w;
}
model {
// Dirichlet distributions (earlier versions used gamma)
for (i in 1:n_param_sets) {
  target += dirichlet_lpdf(lambdas[l_starts[i]:l_ends[i]]  | lambdas_prior[l_starts[i] :l_ends[i]]);
  target += -n_param_each[i] * log(sum_gammas[i]);
 }
// Multinomials
for (i in 1:n_strategies) {
  target += multinomial_lpmf(
  Y[strategy_starts[i]:strategy_ends[i]] | w_full[strategy_starts[i]:strategy_ends[i]]);
 }
}
// Option to export distribution of causal types
// Note if clause used here to effectively turn off this block if not required
generated quantities{
vector[n_types] prob_of_types;
if (keep_transformed == 1){
for (i in 1:n_types) {
   prob_of_types[i] = prod(P[, i].*lambdas + 1 - P[,i]);
}}
 if (keep_transformed == 0){
    prob_of_types = rep_vector(1, n_types);
 }
}
\end{verbatim}

The \texttt{stan} model works as follows (technical!):

\begin{itemize}
\item
  We are interested in ``sets'' of parameters. For example in the \(X \rightarrow Y\) model we have two parameter sets (\texttt{param\_sets}). The first is \(\lambda^X \in \{\lambda^X_0, \lambda^X_1\}\) whose elements give the probability that \(X\) is 0 or 1. These two probabilities sum to one. The second parameter set is \(\lambda^Y \in \{\lambda^Y_{00}, \lambda^Y_{10}, \lambda^Y_{01} \lambda^Y_{11}\}\). These are also probabilities and their values sum to one. Note in all that we have 6 parameters but just 1 + 3 = 4 degrees of freedom.
\item
  We would like to express priors over these parameters using multiple Dirichlet distributions (two in this case). In practice because we are dealing with multiple simplices of varying length, it is easier to express priors over gamma distributions with a unit scale parameter and shape parameter corresponding to the Dirichlet priors, \(\alpha\). We make use of the fact that \(\lambda^X_0 \sim Gamma(\alpha^X_0,1)\) and \(\lambda^X_1 \sim Gamma(\alpha^X_1,1)\) then \(\frac{1}{\lambda^X_0 +\lambda^X_1}(\lambda^X_0, \lambda^X_1) \sim Dirichlet(\alpha^X_0, \alpha^X_1)\). For a discussion of implementation of this approach in \texttt{stan} see \url{https://discourse.mc-stan.org/t/ragged-array-of-simplexes/1382}.
\item
  For any candidate parameter vector \(\lambda\) we calculate the probability of \emph{causal} types (\texttt{prob\_of\_types}) by taking, for each type \(i\), the product of the probabilities of all parameters (\(\lambda_j\)) that appear in column \(i\) of the parameter matrix \(P\). Thus the probability of a \((X_0,Y_{00})\) case is just \(\lambda^X_0 \times \lambda^Y_{00}\). The implementations in \texttt{stan} uses \texttt{prob\_of\_types\_{[}i{]}} \(= \prod_j \left(P_{j,i} \lambda_j + (1-P_{j,i})\right)\): this multiplies the probability of all parameters involved in the causal type (and substitutes 1s for parameters that are not). (\texttt{P} and \texttt{not\_P} (1-\(P\)) are provided as data to \texttt{stan}).
\item
  The probability of data types, \texttt{w}, is given by summing up the probabilities of all causal types that produce a given data type. For example, the probability of a \(X=0,Y=0\) case, \(w_{00}\) is \(\lambda^X_0\times \lambda^Y_{00} + \lambda^X_0\times \lambda^Y_{01}\). The ambiguity matrix \(A\) is provided to \texttt{stan} to indicate which probabilities need to be summed.
\item
  In the case of incomplete data we first identify the set of ``data strategies'', where a collection of a data strategy might be of the form ``gather data on \(X\) and \(M\), but not \(Y\), for \(n_1\) cases and gather data on \(X\) and \(Y\), but not \(M\), for \(n_2\) cases. The probability of an observed event, within a data strategy, is given by summing the probabilities of the types that could give rise to the incomplete data. For example \(X\) is observed, but \(Y\) is not, then the probability of \(X=0, Y = \text{NA}\) is \(w_{00} +w_{01}\). The matrix \(E\) is passed to \texttt{stan} to figure out which event probabilities need to be combined for events with missing data.
\item
  The probability of a dataset is then given by a multinomial distribution with these event probabilities (or, in the case of incomplete data, the product of multinomials, one for each data strategy). Justification for this approach relies on the likelihood principle and is discussed in Chapter 6.
\end{itemize}

\hypertarget{implementation}{%
\section{Implementation}\label{implementation}}

To update a CausalQueries model with data use:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{update\_model}\NormalTok{(model, data)}
\end{Highlighting}
\end{Shaded}

where the data argument is a dataset containing some or all of the nodes in the model.

Other \texttt{stan} arguments can be passed to \texttt{update\_data}, in particular:

\begin{itemize}
\tightlist
\item
  \texttt{iter} sets the number of iterations and ultimately the number of draws in the posterior
\item
  \texttt{chains} sets the number of chains; doing multiple chains in parallel speeds things up
\item
  lots of other options via \texttt{?rstan::stan}
\end{itemize}

If you have multiple cores you can do parallel processing by including this line before running \texttt{CausalQueries}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{options}\NormalTok{(}\AttributeTok{mc.cores =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

The \texttt{stan} output from a simple model looks like this:

\begin{verbatim}
Inference for Stan model: simplexes.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

                   mean se_mean     sd   2.5%    25%
gamma[1]           3.13    0.13   5.65   0.24   0.85
gamma[2]          11.71    5.15 181.05   0.02   0.27
gamma[3]          14.96    5.85 213.49   0.05   0.50
gamma[4]           7.37    2.86 151.51   0.04   0.36
lambdas[1]         0.40    0.00   0.20   0.06   0.24
lambdas[2]         0.60    0.00   0.20   0.20   0.46
lambdas[3]         0.25    0.00   0.17   0.01   0.12
lambdas[4]         0.21    0.00   0.16   0.01   0.08
lambdas[5]         0.31    0.00   0.19   0.02   0.16
lambdas[6]         0.23    0.00   0.16   0.01   0.10
sum_gammas[1]      4.13    0.13   5.65   1.24   1.85
sum_gammas[2]     35.04   12.63 441.32   1.61   2.74
parlam[1,1]        0.40    0.00   0.20   0.06   0.24
parlam[1,2]        0.00     NaN   0.00   0.00   0.00
parlam[1,3]        0.40    0.00   0.20   0.06   0.24
parlam[1,4]        0.00     NaN   0.00   0.00   0.00
parlam[2,1]        0.00     NaN   0.00   0.00   0.00
parlam[2,2]        0.60    0.00   0.20   0.20   0.46
parlam[2,3]        0.00     NaN   0.00   0.00   0.00
parlam[2,4]        0.60    0.00   0.20   0.20   0.46
parlam[3,1]        0.25    0.00   0.17   0.01   0.12
parlam[3,2]        0.25    0.00   0.17   0.01   0.12
parlam[3,3]        0.00     NaN   0.00   0.00   0.00
parlam[3,4]        0.00     NaN   0.00   0.00   0.00
parlam[4,1]        0.00     NaN   0.00   0.00   0.00
parlam[4,2]        0.21    0.00   0.16   0.01   0.08
parlam[4,3]        0.21    0.00   0.16   0.01   0.08
parlam[4,4]        0.00     NaN   0.00   0.00   0.00
parlam[5,1]        0.31    0.00   0.19   0.02   0.16
parlam[5,2]        0.00     NaN   0.00   0.00   0.00
parlam[5,3]        0.00     NaN   0.00   0.00   0.00
parlam[5,4]        0.31    0.00   0.19   0.02   0.16
parlam[6,1]        0.00     NaN   0.00   0.00   0.00
parlam[6,2]        0.00     NaN   0.00   0.00   0.00
parlam[6,3]        0.23    0.00   0.16   0.01   0.10
parlam[6,4]        0.23    0.00   0.16   0.01   0.10
parlam2[1,1]       0.40    0.00   0.20   0.06   0.24
parlam2[1,2]       0.60    0.00   0.20   0.20   0.46
parlam2[1,3]       0.40    0.00   0.20   0.06   0.24
parlam2[1,4]       0.60    0.00   0.20   0.20   0.46
parlam2[2,1]       0.56    0.00   0.20   0.18   0.42
parlam2[2,2]       0.46    0.00   0.18   0.13   0.33
parlam2[2,3]       0.44    0.00   0.20   0.10   0.29
parlam2[2,4]       0.54    0.00   0.18   0.20   0.41
w_0[1]             0.22    0.00   0.14   0.03   0.11
w_0[2]             0.27    0.00   0.14   0.06   0.16
w_0[3]             0.18    0.00   0.13   0.01   0.08
w_0[4]             0.33    0.00   0.16   0.06   0.21
w[1]               0.22    0.00   0.14   0.03   0.11
w[2]               0.27    0.00   0.14   0.06   0.16
w[3]               0.18    0.00   0.13   0.01   0.08
w[4]               0.33    0.00   0.16   0.06   0.21
w_full[1]          0.22    0.00   0.14   0.03   0.11
w_full[2]          0.27    0.00   0.14   0.06   0.16
w_full[3]          0.18    0.00   0.13   0.01   0.08
w_full[4]          0.33    0.00   0.16   0.06   0.21
w_full[5]          0.49    0.00   0.14   0.22   0.40
w_full[6]          0.51    0.00   0.14   0.24   0.41
prob_of_types[1]   0.10    0.00   0.09   0.00   0.03
prob_of_types[2]   0.15    0.00   0.12   0.01   0.06
prob_of_types[3]   0.08    0.00   0.09   0.00   0.02
prob_of_types[4]   0.12    0.00   0.10   0.00   0.04
prob_of_types[5]   0.12    0.00   0.10   0.00   0.04
prob_of_types[6]   0.19    0.00   0.14   0.01   0.08
prob_of_types[7]   0.09    0.00   0.09   0.00   0.03
prob_of_types[8]   0.14    0.00   0.11   0.01   0.05
lp__             -10.33    0.04   1.62 -14.43 -11.14
                   50%   75% 97.5% n_eff Rhat
gamma[1]          1.58  3.10 16.77  1879    1
gamma[2]          0.75  2.13 24.62  1237    1
gamma[3]          1.22  3.29 36.89  1331    1
gamma[4]          0.88  2.14 22.14  2813    1
lambdas[1]        0.39  0.54  0.80  2417    1
lambdas[2]        0.61  0.76  0.94  2417    1
lambdas[3]        0.23  0.36  0.62  1893    1
lambdas[4]        0.17  0.30  0.58  3827    1
lambdas[5]        0.29  0.44  0.72  3565    1
lambdas[6]        0.20  0.34  0.61  3555    1
sum_gammas[1]     2.58  4.10 17.77  1879    1
sum_gammas[2]     4.38  8.55 85.77  1220    1
parlam[1,1]       0.39  0.54  0.80  2417    1
parlam[1,2]       0.00  0.00  0.00   NaN  NaN
parlam[1,3]       0.39  0.54  0.80  2417    1
parlam[1,4]       0.00  0.00  0.00   NaN  NaN
parlam[2,1]       0.00  0.00  0.00   NaN  NaN
parlam[2,2]       0.61  0.76  0.94  2417    1
parlam[2,3]       0.00  0.00  0.00   NaN  NaN
parlam[2,4]       0.61  0.76  0.94  2417    1
parlam[3,1]       0.23  0.36  0.62  1893    1
parlam[3,2]       0.23  0.36  0.62  1893    1
parlam[3,3]       0.00  0.00  0.00   NaN  NaN
parlam[3,4]       0.00  0.00  0.00   NaN  NaN
parlam[4,1]       0.00  0.00  0.00   NaN  NaN
parlam[4,2]       0.17  0.30  0.58  3827    1
parlam[4,3]       0.17  0.30  0.58  3827    1
parlam[4,4]       0.00  0.00  0.00   NaN  NaN
parlam[5,1]       0.29  0.44  0.72  3565    1
parlam[5,2]       0.00  0.00  0.00   NaN  NaN
parlam[5,3]       0.00  0.00  0.00   NaN  NaN
parlam[5,4]       0.29  0.44  0.72  3565    1
parlam[6,1]       0.00  0.00  0.00   NaN  NaN
parlam[6,2]       0.00  0.00  0.00   NaN  NaN
parlam[6,3]       0.20  0.34  0.61  3555    1
parlam[6,4]       0.20  0.34  0.61  3555    1
parlam2[1,1]      0.39  0.54  0.80  2417    1
parlam2[1,2]      0.61  0.76  0.94  2417    1
parlam2[1,3]      0.39  0.54  0.80  2417    1
parlam2[1,4]      0.61  0.76  0.94  2417    1
parlam2[2,1]      0.58  0.71  0.90  2800    1
parlam2[2,2]      0.46  0.59  0.80  3231    1
parlam2[2,3]      0.42  0.58  0.82  2800    1
parlam2[2,4]      0.54  0.67  0.87  3231    1
w_0[1]            0.20  0.30  0.54  2805    1
w_0[2]            0.26  0.37  0.58  2807    1
w_0[3]            0.15  0.25  0.50  2485    1
w_0[4]            0.31  0.43  0.68  2868    1
w[1]              0.20  0.30  0.54  2805    1
w[2]              0.26  0.37  0.58  2807    1
w[3]              0.15  0.25  0.50  2485    1
w[4]              0.31  0.43  0.68  2868    1
w_full[1]         0.20  0.30  0.54  2805    1
w_full[2]         0.26  0.37  0.58  2807    1
w_full[3]         0.15  0.25  0.50  2485    1
w_full[4]         0.31  0.43  0.68  2868    1
w_full[5]         0.50  0.59  0.76  2433    1
w_full[6]         0.50  0.60  0.78  2433    1
prob_of_types[1]  0.08  0.14  0.34  2209    1
prob_of_types[2]  0.12  0.22  0.44  2116    1
prob_of_types[3]  0.06  0.12  0.32  3056    1
prob_of_types[4]  0.09  0.17  0.38  3350    1
prob_of_types[5]  0.09  0.17  0.38  2671    1
prob_of_types[6]  0.16  0.27  0.53  3259    1
prob_of_types[7]  0.07  0.13  0.33  2829    1
prob_of_types[8]  0.11  0.20  0.41  3744    1
lp__             -9.98 -9.14 -8.31  1351    1

Samples were drawn using NUTS(diag_e) at Thu Aug 31 04:20:06 2023.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
\end{verbatim}

Note the parameters include the gamma parameters plus transformed parameters, \(\lambda\), which are our parameters of interest and which \texttt{CausalQueries} then interprets as possible row probabilities for the \(P\) matrix.

\hypertarget{extensions}{%
\section{Extensions}\label{extensions}}

\hypertarget{arbitrary-parameters}{%
\subsection{Arbitrary parameters}\label{arbitrary-parameters}}

Although the package provides helpers to generate mappings from parameters to causal types via nodal types, it is possible to dispense with the nodal types altogether and provide a direct mapping from parameters to causal types.

For this you need to manually provide a \texttt{P} matrix and a corresponding \texttt{parameters\_df}. As an example here is a model with complete confounding and parameters that correspond to causal types directly.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X{-}\textgreater{}Y"}\NormalTok{)}

\NormalTok{model}\SpecialCharTok{$}\NormalTok{P }\OtherTok{\textless{}{-}} \FunctionTok{diag}\NormalTok{(}\DecValTok{8}\NormalTok{)}
\FunctionTok{colnames}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{P) }\OtherTok{\textless{}{-}} \FunctionTok{rownames}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{causal\_types)}

\NormalTok{model}\SpecialCharTok{$}\NormalTok{parameters\_df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{param\_names =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"x"}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{), }
  \AttributeTok{param\_set =} \DecValTok{1}\NormalTok{, }
  \AttributeTok{priors =} \DecValTok{1}\NormalTok{, }
  \AttributeTok{parameters =} \DecValTok{1}\SpecialCharTok{/}\DecValTok{8}\NormalTok{)}

\CommentTok{\# Update fully confounded model on strongly correlated data}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X{-}\textgreater{}Y"}\NormalTok{)}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{make\_data}\NormalTok{(}\FunctionTok{make\_model}\NormalTok{(}\StringTok{"X{-}\textgreater{}Y"}\NormalTok{), }\AttributeTok{n =} \DecValTok{100}\NormalTok{, }\AttributeTok{parameters =} \FunctionTok{c}\NormalTok{(.}\DecValTok{5}\NormalTok{, .}\DecValTok{5}\NormalTok{, .}\DecValTok{1}\NormalTok{,.}\DecValTok{1}\NormalTok{,.}\DecValTok{7}\NormalTok{,.}\DecValTok{1}\NormalTok{))}

\NormalTok{fully\_confounded }\OtherTok{\textless{}{-}} \FunctionTok{update\_model}\NormalTok{(model, data)}
\end{Highlighting}
\end{Shaded}

\hypertarget{non-binary-data}{%
\subsection{Non binary data}\label{non-binary-data}}

In principle the \texttt{stan} model could be extended to handle non binary data. Though a limitation of the current package there is no structural reason why nodes should be constrained to be dichotomous. The set of nodal and causal types however expands even more rapidly in the case of non binary data. .

\hypertarget{querying-models}{%
\chapter{Querying models}\label{querying-models}}

Models can be queried using the \texttt{query\_distribution} and \texttt{query\_model} functions. The difference between these functions is that \texttt{query\_distribution} examines a single query and returns a full distribution of draws from the distribution of the estimand (prior or posterior); \texttt{query\_model} takes a collection of queries and returns a dataframe with summary statistics on the queries.

The simplest queries ask about causal estimands given particular parameter values and case level data. Here is one surprising result of this form:

\hypertarget{case-level-queries}{%
\section{Case level queries}\label{case-level-queries}}

The \texttt{query\_model} function takes causal queries and conditions (\texttt{given}) and specifies the parameters to be used. The result is a dataframe which can be displayed as a table.

For a case level query we can make the query \emph{given} a particular parameter vector, as below:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make\_model}\NormalTok{(}\StringTok{"X{-}\textgreater{} M {-}\textgreater{} Y \textless{}{-} X"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  
  \FunctionTok{set\_restrictions}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{decreasing}\NormalTok{(}\StringTok{"X"}\NormalTok{, }\StringTok{"M"}\NormalTok{), }
                     \FunctionTok{decreasing}\NormalTok{(}\StringTok{"M"}\NormalTok{, }\StringTok{"Y"}\NormalTok{), }
                     \FunctionTok{decreasing}\NormalTok{(}\StringTok{"X"}\NormalTok{, }\StringTok{"Y"}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%}
  
  \FunctionTok{query\_model}\NormalTok{(}\AttributeTok{queries =} \StringTok{"Y[X=1]\textgreater{} Y[X=0]"}\NormalTok{,}
              \AttributeTok{given =} \FunctionTok{c}\NormalTok{(}\StringTok{"X==1 \& Y==1"}\NormalTok{, }
                        \StringTok{"X==1 \& Y==1 \& M==1"}\NormalTok{, }
                        \StringTok{"X==1 \& Y==1 \& M==0"}\NormalTok{),}
              \AttributeTok{using =} \FunctionTok{c}\NormalTok{(}\StringTok{"parameters"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  
  \FunctionTok{kable}\NormalTok{(}
    \AttributeTok{caption =} \StringTok{"In a monotonic model with flat priors, knowledge}
\StringTok{    that $M=1$ *reduces* confidence that $X=1$ caused $Y=1$"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-43}In a monotonic model with flat priors, knowledge
    that $M=1$ *reduces* confidence that $X=1$ caused $Y=1$}
\centering
\begin{tabular}[t]{l|l|l|l|r}
\hline
Query & Given & Using & Case.estimand & mean\\
\hline
Q 1 & X==1 \& Y==1 & parameters & FALSE & 0.6154\\
\hline
Q 1 & X==1 \& Y==1 \& M==1 & parameters & FALSE & 0.6000\\
\hline
Q 1 & X==1 \& Y==1 \& M==0 & parameters & FALSE & 0.6667\\
\hline
\end{tabular}
\end{table}

This example shows how inferences change given additional data on \(M\) in a monotonic \(X \rightarrow M \rightarrow Y \leftarrow X\) model. Surprisingly observing \(M=1\) \emph{reduces} beliefs that \(X\) caused \(Y\), the reason being that perhaps \(M\) and not \(X\) was responsible for \(Y=1\).

\hypertarget{posterior-queries}{%
\section{Posterior queries}\label{posterior-queries}}

Queries can also draw directly from the posterior distribution provided by \texttt{stan}. In this next example we illustrate the joint distribution of the posterior over causal effects, drawing directly from the posterior dataframe generated by \texttt{update\_model}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data  }\OtherTok{\textless{}{-}} \FunctionTok{fabricate}\NormalTok{(}\AttributeTok{N =} \DecValTok{100}\NormalTok{, }\AttributeTok{X =} \FunctionTok{complete\_ra}\NormalTok{(N), }\AttributeTok{Y =}\NormalTok{ X)}

\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} Y; X \textless{}{-}\textgreater{} Y"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{update\_model}\NormalTok{(data, }\AttributeTok{iter  =} \DecValTok{4000}\NormalTok{)}

\NormalTok{model}\SpecialCharTok{$}\NormalTok{posterior\_distribution }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(X}\FloatTok{.1} \SpecialCharTok{{-}}\NormalTok{ X}\FloatTok{.0}\NormalTok{, Y}\FloatTok{.01}\NormalTok{\_X}\FloatTok{.1} \SpecialCharTok{{-}}\NormalTok{ Y}\FloatTok{.10}\NormalTok{\_X}\FloatTok{.0}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{causalmodels_files/figure-latex/unnamed-chunk-45-1.pdf}

We see that beliefs about the size of the overall effect are related to beliefs that \(X\) is assigned differently when there is a positive effect.

\hypertarget{query-distribution}{%
\section{Query distribution}\label{query-distribution}}

\texttt{query\_distribution} works similarly except that the query is over an estimand. For instance:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} Y"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{query\_distribution}\NormalTok{(}\FunctionTok{increasing}\NormalTok{(}\StringTok{"X"}\NormalTok{, }\StringTok{"Y"}\NormalTok{), }\AttributeTok{using =} \StringTok{"priors"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
 \FunctionTok{hist}\NormalTok{(}\AttributeTok{main =} \StringTok{"Prior on Y increasing in X"}\NormalTok{)  }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Prior distribution added to model
\end{verbatim}

\includegraphics{causalmodels_files/figure-latex/unnamed-chunk-46-1.pdf}

\hypertarget{token-and-general-causation}{%
\section{Token and general causation}\label{token-and-general-causation}}

Note that in all these cases we use the same technology to make case level and population inferences. Indeed the case level query is just a conditional population query. As an illustration of this imagine we have a model of the form \(X \rightarrow M \rightarrow Y\) and are interested in whether \(X\) caused \(Y\) in a case in which \(M=1\). We answer the question by asking ``what would be the probability that \(X\) caused \(Y\) in a case in which \(X=M=Y=1\)?'' (line 3 below). This speculative answer is the same answer as we would get were we to ask the same question having updated our model with knowledge that in a particular case, indeed, \(X=M=Y=1\). See below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X{-}\textgreater{}M{-}\textgreater{}Y"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{set\_restrictions}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{decreasing}\NormalTok{(}\StringTok{"X"}\NormalTok{, }\StringTok{"M"}\NormalTok{), }\FunctionTok{decreasing}\NormalTok{(}\StringTok{"M"}\NormalTok{, }\StringTok{"Y"}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{update\_model}\NormalTok{(}\AttributeTok{data =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{X =} \DecValTok{1}\NormalTok{, }\AttributeTok{M =} \DecValTok{1}\NormalTok{, }\AttributeTok{Y =} \DecValTok{1}\NormalTok{), }\AttributeTok{iter =} \DecValTok{8000}\NormalTok{)}

\FunctionTok{query\_model}\NormalTok{(}
\NormalTok{            model, }
            \AttributeTok{query =} \StringTok{"Y[X=1]\textgreater{} Y[X=0]"}\NormalTok{,}
            \AttributeTok{given =} \FunctionTok{c}\NormalTok{(}\StringTok{"X==1 \& Y==1"}\NormalTok{, }\StringTok{"X==1 \& Y==1 \& M==1"}\NormalTok{),}
            \AttributeTok{using =} \FunctionTok{c}\NormalTok{(}\StringTok{"priors"}\NormalTok{, }\StringTok{"posteriors"}\NormalTok{),}
            \AttributeTok{expand\_grid =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-48}Posteriors equal priors for a query that conditions on data used to form the posterior }
\centering
\begin{tabular}[t]{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
Q 1 & X==1 \& Y==1 & priors & FALSE & 0.2091 & 0.2042 & 0.0026 & 0.7439\\
\hline
Q 1 & X==1 \& Y==1 & posteriors & FALSE & 0.2226 & 0.2102 & 0.0027 & 0.7566\\
\hline
Q 1 & X==1 \& Y==1 \& M==1 & priors & FALSE & 0.2485 & 0.2174 & 0.0046 & 0.7847\\
\hline
Q 1 & X==1 \& Y==1 \& M==1 & posteriors & FALSE & 0.2500 & 0.2197 & 0.0037 & 0.7823\\
\hline
\end{tabular}
\end{table}

We see the conditional inference is the same using the prior and the posterior distributions.

\hypertarget{complex-queries}{%
\section{Complex queries}\label{complex-queries}}

The Billy Suzy bottle breaking example illustrates complex queries. See Section \ref{Billy}.

\hypertarget{part-applications}{%
\part{Applications}\label{part-applications}}

\hypertarget{applications}{%
\chapter{Basic Models}\label{applications}}

\hypertarget{the-ladder-of-causation-in-an-x-rightarrow-y-model}{%
\section{\texorpdfstring{The ladder of causation in an \(X \rightarrow Y\) model}{The ladder of causation in an X \textbackslash rightarrow Y model}}\label{the-ladder-of-causation-in-an-x-rightarrow-y-model}}

We first introduce a simple \(X\) causes \(Y\) model with no confounding and use this to illustrate the ``ladder of causation'' \citep{pearl2018book}.

The model is written:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} Y"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We will assume a ``true'' distribution over parameters. Let's assume that the true effect of 0.5, but that this is not known to researchers. The .5 effect comes from the difference between the share of units with a positive effect (.6) and those with a negative effect (.1). (We say share but we may as well think in terms of the probability that a given unit is of one or other type.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} 
  \FunctionTok{set\_parameters}\NormalTok{(model, }\AttributeTok{node =} \StringTok{"Y"}\NormalTok{, }\AttributeTok{parameters =} \FunctionTok{c}\NormalTok{(.}\DecValTok{2}\NormalTok{, .}\DecValTok{1}\NormalTok{, .}\DecValTok{6}\NormalTok{, .}\DecValTok{1}\NormalTok{))}

\FunctionTok{kable}\NormalTok{(}\FunctionTok{t}\NormalTok{(}\FunctionTok{get\_parameters}\NormalTok{(model)))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{r|r|r|r|r|r}
\hline
X.0 & X.1 & Y.00 & Y.10 & Y.01 & Y.11\\
\hline
0.5 & 0.5 & 0.2 & 0.1 & 0.6 & 0.1\\
\hline
\end{tabular}

We can now simulate data using the model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{make\_data}\NormalTok{(model, }\AttributeTok{n =} \DecValTok{10000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

With a model and data in hand we update the model thus:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{updated }\OtherTok{\textless{}{-}} \FunctionTok{update\_model}\NormalTok{(model, data)}
\end{Highlighting}
\end{Shaded}

From the updated model we can draw posterior inferences over estimands of interest.

We will imagine three estimands, corresponding to Pearl's ``ladder of causation.''

\begin{itemize}
\item
  At the first level we are interested in the distribution of some node, perhaps given the value of another node. This question is answerable from observational data.
\item
  At the second level we are interested in treatment effects: how changing one node changes another. This question is answerable from experimental data.
\item
  At the third level we are interested in counterfactual statements: how would things have been different if some features of the world were different from what they are? Answering this question requires a causal model.
\end{itemize}

Here are the results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{query\_model}\NormalTok{(}
\NormalTok{ updated,}
 \AttributeTok{query =} \FunctionTok{list}\NormalTok{(}\StringTok{"Y | X=1"} \OtherTok{=} \StringTok{"Y==1"}\NormalTok{, }
              \AttributeTok{ATE =} \StringTok{"Y[X=1] {-} Y[X=0]"}\NormalTok{, }
              \AttributeTok{PC  =} \StringTok{"Y[X=1] \textgreater{} Y[X=0]"}\NormalTok{),}
 \AttributeTok{given =} \FunctionTok{c}\NormalTok{(}\StringTok{"X==1"}\NormalTok{, }\ConstantTok{TRUE}\NormalTok{, }\StringTok{"X==1 \& Y==1"}\NormalTok{),}
 \AttributeTok{using =} \StringTok{"posteriors"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|l|l|r|r|r|r}
\hline
Query (rung) & Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
1 Association & Y | X=1 & X==1 & posteriors & FALSE & 0.69 & 0.01 & 0.68 & 0.71\\
\hline
2 Intervention & ATE & - & posteriors & FALSE & 0.49 & 0.01 & 0.47 & 0.50\\
\hline
3 Imagining & PC & X==1 \& Y==1 & posteriors & FALSE & 0.85 & 0.09 & 0.71 & 0.99\\
\hline
\end{tabular}

We see from the posterior variance on PC that we have the greatest difficulty with the third rung. In fact the PC is not identified (the distribution does not tighten even with very large N). For more intuition we graph the posteriors:

\begin{verbatim}
## `stat_bin()` using `bins = 30`. Pick better value with
## `binwidth`.
\end{verbatim}

\begin{verbatim}
## Warning: Removed 4 rows containing missing values
## (`geom_bar()`).
\end{verbatim}

\begin{figure}
\centering
\includegraphics{causalmodels_files/figure-latex/PChist-1.pdf}
\caption{\label{fig:PChist}ATE is identified, PC is not identified but has informative bounds}
\end{figure}

We find that they do not converge but they do place positive mass in the right range. Within this range, the shape of the posterior depends on the priors only.

\hypertarget{x-causes-y-with-unmodelled-confounding}{%
\section{\texorpdfstring{\(X\) causes \(Y\), with unmodelled confounding}{X causes Y, with unmodelled confounding}}\label{x-causes-y-with-unmodelled-confounding}}

The first model assumed that \(X\) was as-if randomly assigned, but we do not need to make such strong assumptions.

An \(X\) causes \(Y\) model with confounding can be written:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} Y; X \textless{}{-}\textgreater{} Y"}\NormalTok{) }
\FunctionTok{plot}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\includegraphics{causalmodels_files/figure-latex/ap_with_conf-1.pdf}

If we look at the parameter matrix implied by this model we see that it has more parameters than nodal types, reflecting the joint assignment probabilities of \(\theta_X\) and \(\theta_Y\). Here we have parameters for \(\Pr(\theta_X=x)\) and \(\Pr(\theta_Y |\theta_X=x)\), which allow us to represent \(\Pr(\theta_X, \theta_Y)\) via \(\Pr(\theta_X=x)\Pr(\theta_Y |\theta_X=x)\).

\begin{table}

\caption{\label{tab:unnamed-chunk-58}Parameter matrix for X causes Y model with arbitrary confounding}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r|r}
\hline
  & X0.Y00 & X1.Y00 & X0.Y10 & X1.Y10 & X0.Y01 & X1.Y01 & X0.Y11 & X1.Y11\\
\hline
X.0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0\\
\hline
X.1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1\\
\hline
Y.00\_X.0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
\hline
Y.10\_X.0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\
\hline
Y.01\_X.0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\
\hline
Y.11\_X.0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\
\hline
Y.00\_X.1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\
\hline
Y.10\_X.1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\
\hline
Y.01\_X.1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\
\hline
Y.11\_X.1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\
\hline
\end{tabular}
\end{table}

With the possibility of any type of confounding, the best we can do is place ``Manski bounds'' on the average causal effect.

To see this, let's plot a histogram of our posterior on average causal effects, given lots of data. We will assume here that in truth there is no confounding, but that that is not known to researchers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data5000 }\OtherTok{\textless{}{-}} \FunctionTok{make\_data}\NormalTok{(}
\NormalTok{    model, }\AttributeTok{n =} \DecValTok{5000}\NormalTok{, }
    \AttributeTok{parameters =} \FunctionTok{c}\NormalTok{(.}\DecValTok{5}\NormalTok{, .}\DecValTok{5}\NormalTok{, .}\DecValTok{25}\NormalTok{, .}\DecValTok{0}\NormalTok{, .}\DecValTok{5}\NormalTok{, .}\DecValTok{25}\NormalTok{, .}\DecValTok{25}\NormalTok{, }\DecValTok{0}\NormalTok{, .}\DecValTok{5}\NormalTok{, .}\DecValTok{25}\NormalTok{))}

\NormalTok{data100 }\OtherTok{\textless{}{-}}\NormalTok{ data5000[}\FunctionTok{sample}\NormalTok{(}\DecValTok{5000}\NormalTok{, }\DecValTok{100}\NormalTok{), ]}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{causalmodels_files/figure-latex/unnamed-chunk-61-1.pdf}
\caption{\label{fig:unnamed-chunk-61}Modest gains from additional data when ATE is not identified}
\end{figure}

The key thing here is that the posterior on the ATE has shifted, as it should, but it is not tight, even with large data. In fact the distribution of the posterior covers one unit of the range between -1 and 1.

\hypertarget{x-causes-y-with-confounding-modeled}{%
\section{\texorpdfstring{\(X\) causes \(Y\), with confounding modeled}{X causes Y, with confounding modeled}}\label{x-causes-y-with-confounding-modeled}}

Say now we have a theory that the relationship between \(X\) and \(Y\) is confounded by possibly unobserved variable \(C\). Although \(C\) is unobserved we can still include it in the model and observe the confounding it generates by estimating the model on data generated by the model (but without benefiting from observing \(C\)).
We will assume that it is know that \(X\) does not have a negative effet on \(X\). In addition we will assume that both \(C\) and \(X\) have a positive effect on \(Y\)---though this is not known (and so this is built into the model parameters but not into the priors).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"C {-}\textgreater{} X {-}\textgreater{} Y \textless{}{-} C"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  
  \FunctionTok{set\_restrictions}\NormalTok{(}\StringTok{"(X[C=1] \textless{} X[C=0])"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}

  \FunctionTok{set\_parameters}\NormalTok{(}
           \AttributeTok{statement =} \FunctionTok{c}\NormalTok{(}\StringTok{"(Y[X=1] \textless{} Y[X=0]) | (Y[C=1] \textless{} Y[C=0])"}\NormalTok{), }
           \AttributeTok{parameters =} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The ATE estimand in this case is given by:

\begin{tabular}{l|l|l|l|r}
\hline
Query & Given & Using & Case.estimand & mean\\
\hline
ATE & - & parameters & FALSE & 0.3333\\
\hline
\end{tabular}

A regression based approach won't fare very well here without data on \(C\). It would yield a precise but incorrect estimate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{make\_data}\NormalTok{(model, }\AttributeTok{n =} \DecValTok{1000}\NormalTok{)}

\NormalTok{estimatr}\SpecialCharTok{::}\FunctionTok{lm\_robust}\NormalTok{(Y}\SpecialCharTok{\textasciitilde{}}\NormalTok{X, }\AttributeTok{data =}\NormalTok{ data) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{tidy}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r|r|r|r|r|r|l}
\hline
term & estimate & std.error & statistic & p.value & conf.low & conf.high & df & outcome\\
\hline
(Intercept) & 0.26 & 0.02 & 13.45 & 0 & 0.22 & 0.30 & 998 & Y\\
\hline
X & 0.47 & 0.03 & 16.80 & 0 & 0.41 & 0.52 & 998 & Y\\
\hline
\end{tabular}

In contrast, the Bayesian estimate takes account of the fact that we are missing data on \(C\).

Our posteriors over the effect of \(X\) on \(Y\) and the effect of the unobserved confounder (\(C\)) on \(Y\) have a joint distribution with negative covariance.

To illustrate we update on the same data (note that although relationship between \(C\) and \(Y\) is resrticted in the parameters it is not restricted in the priors). We then plot the joint posterior over our estimand and a measure of confounding (we will use the effect of \(C\) on \(Y\), since we have built in already that \(C\) matters for \(X\)).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{updated  }\OtherTok{\textless{}{-}} \FunctionTok{update\_model}\NormalTok{(model, }\FunctionTok{select}\NormalTok{(data, X, Y))}

\NormalTok{ate }\OtherTok{\textless{}{-}} 
  \FunctionTok{query\_distribution}\NormalTok{(updated, }\StringTok{"c(Y[X=1] {-} Y[X=0])"}\NormalTok{, }\AttributeTok{using =} \StringTok{"posteriors"}\NormalTok{)}

\NormalTok{confound }\OtherTok{\textless{}{-}} 
  \FunctionTok{query\_distribution}\NormalTok{(updated, }\StringTok{"c(Y[C=1] {-} Y[C=0])"}\NormalTok{, }\AttributeTok{using =} \StringTok{"posteriors"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{causalmodels_files/figure-latex/unnamed-chunk-66-1.pdf}

The strong negative correlation shows that when we update we contemplate possibilities in which there is a strong effect and negative confounding, or a weak or even negative effect and positive confounding. If we knew the extent of confounding we would have tighter posteriors on the estimand, but our ignorance regarding the nature of confounding keeps the posterior variance on the estimand large.

\hypertarget{simple-mediation-model}{%
\section{Simple mediation model}\label{simple-mediation-model}}

We define a simple mediation model and illustrate learning about whether \(X=1\) caused \(Y=1\) from observations of \(M\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} M {-}\textgreater{} Y; X \textless{}{-}\textgreater{} M"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
         \FunctionTok{set\_parameters}\NormalTok{(}\AttributeTok{node =} \StringTok{"M"}\NormalTok{, }\AttributeTok{given =} \StringTok{"X.0"}\NormalTok{, }\AttributeTok{parameters =} \FunctionTok{c}\NormalTok{(.}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, .}\DecValTok{8}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
         \FunctionTok{set\_parameters}\NormalTok{(}\AttributeTok{node =} \StringTok{"M"}\NormalTok{, }\AttributeTok{given =} \StringTok{"X.1"}\NormalTok{, }\AttributeTok{parameters =} \FunctionTok{c}\NormalTok{(.}\DecValTok{2}\NormalTok{, .}\DecValTok{8}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
         \FunctionTok{set\_parameters}\NormalTok{(}\AttributeTok{node =} \StringTok{"Y"}\NormalTok{, }\AttributeTok{parameters =} \FunctionTok{c}\NormalTok{(.}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, .}\DecValTok{8}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\includegraphics{causalmodels_files/figure-latex/unnamed-chunk-68-1.pdf}
Data and estimation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{make\_data}\NormalTok{(model, }\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }\AttributeTok{using =} \StringTok{"parameters"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{updated }\OtherTok{\textless{}{-}} \FunctionTok{update\_model}\NormalTok{(model, data)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{query\_model}\NormalTok{(}
\NormalTok{    updated, }
    \AttributeTok{queries =} \FunctionTok{list}\NormalTok{(}\AttributeTok{COE =} \StringTok{"c(Y[X=1] \textgreater{} Y[X=0])"}\NormalTok{), }
    \AttributeTok{given =} \FunctionTok{c}\NormalTok{(}\StringTok{"X==1 \& Y==1"}\NormalTok{, }\StringTok{"X==1 \& Y==1 \& M==0"}\NormalTok{, }\StringTok{"X==1 \& Y==1 \& M==1"}\NormalTok{),}
    \AttributeTok{using =} \StringTok{"posteriors"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
COE & X==1 \& Y==1 & posteriors & FALSE & 0.2441 & 0.1794 & 0.0140 & 0.6674\\
\hline
COE & X==1 \& Y==1 \& M==0 & posteriors & FALSE & 0.2484 & 0.2124 & 0.0059 & 0.7655\\
\hline
COE & X==1 \& Y==1 \& M==1 & posteriors & FALSE & 0.2386 & 0.2138 & 0.0029 & 0.7644\\
\hline
\end{tabular}

Note that observation of \(M=0\) results in a near 0 posterior that \(X\) caused \(Y\), while observation of \(M=1\) has only a modest positive effect. The mediator thus provides what qualitative scholars call a ``hoop'' test for the proposition that \(X\) caused \(Y\).

\hypertarget{simple-moderator-model}{%
\section{Simple moderator model}\label{simple-moderator-model}}

We define a simple model with a moderator and illustrate how updating about COE is possible using the value of a moderator as a clue.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} Y; Z {-}\textgreater{} Y"}\NormalTok{) }
\FunctionTok{plot}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\includegraphics{causalmodels_files/figure-latex/appsimpmod-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{make\_data}\NormalTok{(}
\NormalTok{    model, }\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }
    \AttributeTok{parameters =} \FunctionTok{c}\NormalTok{(.}\DecValTok{5}\NormalTok{, .}\DecValTok{5}\NormalTok{, .}\DecValTok{5}\NormalTok{, .}\DecValTok{5}\NormalTok{, }
\NormalTok{                   .}\DecValTok{01}\NormalTok{, .}\DecValTok{01}\NormalTok{, .}\DecValTok{01}\NormalTok{, .}\DecValTok{01}\NormalTok{, .}\DecValTok{01}\NormalTok{, .}\DecValTok{01}\NormalTok{, .}\DecValTok{01}\NormalTok{, .}\DecValTok{01}\NormalTok{,}
\NormalTok{                   .}\DecValTok{01}\NormalTok{, .}\DecValTok{85}\NormalTok{, .}\DecValTok{01}\NormalTok{, .}\DecValTok{01}\NormalTok{, .}\DecValTok{01}\NormalTok{, .}\DecValTok{01}\NormalTok{, .}\DecValTok{01}\NormalTok{, .}\DecValTok{01}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{posterior }\OtherTok{\textless{}{-}} \FunctionTok{update\_model}\NormalTok{(model, data)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{query\_model}\NormalTok{(}
\NormalTok{    updated, }
    \AttributeTok{queries =} \FunctionTok{list}\NormalTok{(}\AttributeTok{COE =} \StringTok{"Y[X=1] \textgreater{} Y[X=0]"}\NormalTok{), }
    \AttributeTok{given =} \FunctionTok{list}\NormalTok{(}\StringTok{"X==1 \& Y==1"}\NormalTok{, }\StringTok{"X==1 \& Y==1 \& Z==0"}\NormalTok{, }\StringTok{"X==1 \& Y==1 \& Z==1"}\NormalTok{),}
    \AttributeTok{using =} \StringTok{"posteriors"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
COE & X==1 \& Y==1 & posteriors & FALSE & 0.89 & 0.03 & 0.83 & 0.94\\
\hline
COE & X==1 \& Y==1 \& Z==0 & posteriors & FALSE & 0.42 & 0.16 & 0.14 & 0.74\\
\hline
COE & X==1 \& Y==1 \& Z==1 & posteriors & FALSE & 0.93 & 0.02 & 0.89 & 0.97\\
\hline
\end{tabular}

Knowledge of the moderator provides sharp updating in both directions, depending on what is found.

\hypertarget{explanation}{%
\chapter{Explanation}\label{explanation}}

\hypertarget{tightening-bounds-on-causes-of-effects-using-an-unobserved-covariate}{%
\section{Tightening bounds on causes of effects using an unobserved covariate}\label{tightening-bounds-on-causes-of-effects-using-an-unobserved-covariate}}

``Explanation'' can sometimes be thought of assessing whether an outcome was due to a cause: ``\(Y\) because \(X\).'' We saw examples showing the difficulty of identifying the ``probability of causation'' (whether \(X\) caused \(Y\) in a case) above (See for example Figure \ref{fig:PChist}).

Knowledge of moderators and mediators can help however. In a particularly striking result, \citet{dawid2011role} shows that knowledge derived from moderators can help \emph{even when the moderator is not observed for the case in question}.

We illustrate with a simple example in which data is drawn from a process in which \(X\) has a positive effect on \(Y\) when \(C=1\) and a negative effect otherwise. We will assume \(X\) is as-if randomized, though \(C\) is not:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} Y \textless{}{-} C; Y \textless{}{-}\textgreater{} C"}\NormalTok{)}

\NormalTok{data }\OtherTok{\textless{}{-}} 
\NormalTok{  model }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_restrictions}\NormalTok{(}\StringTok{"(Y[X=1, C=1] \textgreater{} Y[X=0, C=1]) \& }
\StringTok{                    (Y[X=1, C=0] \textless{} Y[X=0, C=0])"}\NormalTok{, }\AttributeTok{keep =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{make\_data}\NormalTok{(}\AttributeTok{n=} \DecValTok{200}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

These restrictions, coupled with flat priors, produce the following priors (i) on the effect of \(X\) on \(Y\) and (ii) that \(X\) caused \(Y\) in a case with \(X=Y=1\):

\begin{tabular}{l|l|l|l|r}
\hline
Query & Given & Using & Case.estimand & mean\\
\hline
ATE & All & parameters & FALSE & 0.0\\
\hline
PC & X==1 \& Y==1 & parameters & FALSE & 0.5\\
\hline
\end{tabular}

We now compare inferences on the PC (for a case where we have no data on \(C\)) using one model that has been updated using data on \(C\) and one that has not:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{update\_model}\NormalTok{(model, }\FunctionTok{select}\NormalTok{(data, X, Y)) }\SpecialCharTok{|\textgreater{}}
  
  \FunctionTok{query\_model}\NormalTok{(}\AttributeTok{query =} \StringTok{"Y[X=1]\textgreater{}Y[X=0]"}\NormalTok{, }\AttributeTok{given =} \StringTok{"X==1 \& Y==1"}\NormalTok{,}
              \AttributeTok{using =} \StringTok{"posteriors"}\NormalTok{) }

\FunctionTok{update\_model}\NormalTok{(model, data) }\SpecialCharTok{|\textgreater{}}

    \FunctionTok{query\_model}\NormalTok{(}\AttributeTok{query =} \StringTok{"Y[X=1]\textgreater{}Y[X=0]"}\NormalTok{, }\AttributeTok{given =} \StringTok{"X==1 \& Y==1"}\NormalTok{, }
              \AttributeTok{using =} \StringTok{"posteriors"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-81}Probability $X$ caused $Y$ from model updated without data on $C$}
\centering
\begin{tabular}[t]{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
X caused Y & X==1 \& Y==1 & posteriors & FALSE & 0.5153 & 0.1108 & 0.2964 & 0.7274\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:unnamed-chunk-82}Probability $X$ caused $Y$ from model updated using data on $C$}
\centering
\begin{tabular}[t]{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
X caused Y & X==1 \& Y==1 & posteriors & FALSE & 0.8706 & 0.0415 & 0.7803 & 0.9406\\
\hline
\end{tabular}
\end{table}

We see tight gains even though \(C\) is not observed. The remarkable result arises because although \(C\) is not observed in the case at hand, the model that has been updated with knowledge of \(C\) lets us figure out that the average effect of 0 is due to strong heterogeneity of effects. Indeed \(X=Y=1\) only arises when \(C=1\), in which case \(X\) causes \(Y\). Thus observing \(X=Y=1\) lets us infer that \(C=1\) and so in this case \(X\) causes \(Y\).

\hypertarget{Billy}{%
\section{Actual Causation: Billy and Suzy's moderator and mediation model}\label{Billy}}

A classic problem in the philosophy of causation examines a story in which Billy and Suzy throw stones at a bottle \citep{hall2004two}. Both are deadly shots but Suzy's stone hits the bottle first. Had it not, Billy's surely would have. Can we say that Suzy's throw caused the bottle to break if it would have broken even if she hadn't thrown?

We model a simple version of the Billy and Suzy stone throwing game as a causal model with moderation and mediation in three nodes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"Suzy {-}\textgreater{} Billy {-}\textgreater{} Smash \textless{}{-} Suzy"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
         \FunctionTok{set\_restrictions}\NormalTok{(}\FunctionTok{c}\NormalTok{(}
           
           \CommentTok{\# If Suzy throws the bottle breaks}
           \StringTok{"(Smash[Suzy=1]==0)"}\NormalTok{,}

           \CommentTok{\# The bottle won\textquotesingle{}t break by itself}
           \StringTok{"(Smash[Billy=0, Suzy = 0]==1)"}\NormalTok{,}
           
           \CommentTok{\# Suzy\textquotesingle{}s throw doesn\textquotesingle{}t *encourage* Billy to throw}
           \StringTok{"Billy[Suzy=1]\textgreater{}Billy[Suzy=0]"}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\includegraphics{causalmodels_files/figure-latex/appbillysuzy1-1.pdf}

Here ``Suzy'' means Suzy throws, ``Billy'': means Billy throws---which he might not do if Suzy throws---and ``Smash'' means the bottle gets smashed.

The version here is a somewhat less deterministic version of the classic account. Suzy is still an ace shot but now she may or may not throw and Billy may or may not respond positively to Suzy and if he does respond he may or may not be successful. With all these possibilities we have twelve unit causal types instead of 1.

We have two estimands of interest: counterfactual causation and actual causation. Conditional on Suzy throwing and the bottle breaking, would the bottle not have broken had Suzy not thrown her stone? That's counterfactual causation. The actual causation asks the same question but \emph{conditioning} on the fact that Billy did or did not throw \emph{his} stone---which we know could itself be due to Suzy throwing her stone. If so then we might think of an ``active path'' from Suzy's throw to the smashing, even though had she not thrown the bottle might have smashed anyhow.

Our results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{actual\_cause }\OtherTok{\textless{}{-}} \FunctionTok{query\_model}\NormalTok{(model, }\AttributeTok{using =} \StringTok{"priors"}\NormalTok{,}
  \AttributeTok{queries =} \FunctionTok{c}\NormalTok{(}
    \AttributeTok{Counterfactual =} \StringTok{"Smash[Suzy = 1] \textgreater{} Smash[Suzy = 0]"}\NormalTok{,}
    \AttributeTok{Actual =} \StringTok{"Smash[Suzy = 1, Billy = Billy[Suzy = 1] ] \textgreater{} }
\StringTok{              Smash[Suzy = 0, Billy = Billy[Suzy = 1]]"}\NormalTok{),}
  \AttributeTok{given =} \FunctionTok{c}\NormalTok{(}\StringTok{"Suzy==1 \& Smash==1"}\NormalTok{, }\StringTok{"Suzy==1 \& Smash==1 \& Billy==0"}\NormalTok{, }\StringTok{"Suzy==1 \& Smash==1 \& Billy==1"}\NormalTok{),}
  \AttributeTok{expand\_grid =} \ConstantTok{TRUE}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
Counterfactual & Suzy==1 \& Smash==1 & priors & FALSE & 0.6662 & 0.2326 & 0.1670 & 0.9887\\
\hline
Counterfactual & Suzy==1 \& Smash==1 \& Billy==0 & priors & FALSE & 0.7521 & 0.2149 & 0.2432 & 0.9965\\
\hline
Counterfactual & Suzy==1 \& Smash==1 \& Billy==1 & priors & FALSE & 0.4972 & 0.2843 & 0.0264 & 0.9798\\
\hline
Actual & Suzy==1 \& Smash==1 & priors & FALSE & 0.8301 & 0.1677 & 0.3866 & 0.9982\\
\hline
Actual & Suzy==1 \& Smash==1 \& Billy==0 & priors & FALSE & 1.0000 & 0.0000 & 1.0000 & 1.0000\\
\hline
Actual & Suzy==1 \& Smash==1 \& Billy==1 & priors & FALSE & 0.4972 & 0.2843 & 0.0264 & 0.9798\\
\hline
\end{tabular}

Our inferences, \emph{without even observing} Billy's throw distinguish between Suzy being a counterfactual cause and an actual cause. We think it likely that Suzy's throw was an actual cause of the outcome though we are less sure that it was a counterfactual causes. Observing Billy's throw strengthens our inferences. If Billy didn't throw then we are sure Suzy's throw was the actual cause, though we are still in doubt about whether her throw was a counterfactual cause (since Billy might have thrown if she hadn't).

Note that if we observed Suzy \emph{not} throwing then we would learn \emph{more} about whether she would be a counterfactual cause since we would have learned more about whether Billy reacts to her and also about whether Billy is a good shot.

\begin{table}

\caption{\label{tab:appbillysuzy3}Inferences when Suzy does *not* throw}
\centering
\begin{tabular}[t]{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
Counterfactual & Suzy==0 \& Billy==0 & priors & FALSE & 1.0000 & 0.0000 & 1.0000 & 1.0000\\
\hline
Counterfactual & Suzy==0 \& Billy==1 & priors & FALSE & 0.5065 & 0.2924 & 0.0219 & 0.9750\\
\hline
Counterfactual & Suzy==0 \& Billy==1 \& Smash==1 & priors & FALSE & 0.0000 & 0.0000 & 0.0000 & 0.0000\\
\hline
Actual & Suzy==0 \& Billy==0 & priors & FALSE & 1.0000 & 0.0000 & 1.0000 & 1.0000\\
\hline
Actual & Suzy==0 \& Billy==1 & priors & FALSE & 0.7512 & 0.2241 & 0.2083 & 0.9962\\
\hline
Actual & Suzy==0 \& Billy==1 \& Smash==1 & priors & FALSE & 0.4984 & 0.2888 & 0.0226 & 0.9780\\
\hline
\end{tabular}
\end{table}

\hypertarget{diagnosis-inferring-a-cause-from-symptoms}{%
\section{Diagnosis: Inferring a cause from symptoms}\label{diagnosis-inferring-a-cause-from-symptoms}}

Sometimes we want to know whether a particular condition was present that could have caused an observed outcome. This is the stuff of medical diagnosis: on observing symptoms, is the sickness due to \(A\) or to \(B\)?

We imagine cases in which we do not get to observe the putative cause directly and we want to infer both whether the putative cause was present and whether it caused the outcome. This requires stating a query on both an effect and the level of an unobserved node.

An illustration:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"A {-}\textgreater{} S {-}\textgreater{} Y \textless{}{-} B"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_restrictions}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"(S[A=1]\textless{} S[A=0])"}\NormalTok{, }
                   \StringTok{"(Y[S=1]\textless{}Y[S=0])"}\NormalTok{,}
                   \StringTok{"(Y[S = 0, B = 0]== 1)"}\NormalTok{))}

\FunctionTok{plot}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\includegraphics{causalmodels_files/figure-latex/appdiagnosis-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{query\_model}\NormalTok{(model, }
       \AttributeTok{queries =} \FunctionTok{list}\NormalTok{(}\AttributeTok{A=}\StringTok{"(Y[A=1] \textgreater{} Y[A =0]) \& A==1"}\NormalTok{, }\AttributeTok{B=}\StringTok{"(Y[B=1] \textgreater{} Y[B =0]) \& B==1"}\NormalTok{),}
       \AttributeTok{given =} \FunctionTok{list}\NormalTok{(}\StringTok{"Y==1"}\NormalTok{,  }\StringTok{"Y==1 \& S==1"}\NormalTok{), }\AttributeTok{using =} \StringTok{"priors"}\NormalTok{, }
       \AttributeTok{expand\_grid =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
A & Y==1 & priors & FALSE & 0.1949 & 0.1978 & 0.0023 & 0.7133\\
\hline
A & Y==1 \& S==1 & priors & FALSE & 0.2512 & 0.2271 & 0.0033 & 0.7886\\
\hline
B & Y==1 & priors & FALSE & 0.4435 & 0.2610 & 0.0228 & 0.9150\\
\hline
B & Y==1 \& S==1 & priors & FALSE & 0.2889 & 0.2207 & 0.0086 & 0.7799\\
\hline
\end{tabular}

In this example there are two possible causes of interest, \(A\) and \(B\). With flat priors the \(B\) path starts as clearly more probable. Observation of symptom \(S\), which is a consequence of \(A\), however raises the chances that the outcome is due to \(A\) and lowers the chances that it is due to \(B\).

\hypertarget{process-tracing}{%
\chapter{Process tracing}\label{process-tracing}}

\hypertarget{what-to-infer-from-what}{%
\section{What to infer from what}\label{what-to-infer-from-what}}

The simplest application of the \texttt{CausalQueries} package is to figure out what inferences to make about a case upon observing within-case data, given a model. One might observe many pieces of evidence and have to figure out how to update from these jointly.

In \emph{Integrated Inferences} we explore an inequality-democratization model where for a case with low inequality and democratization (say) one is interested in whether the democratization was due to the low inequality. In the simple model, inequality can give rise to popular mobilization which in turn forces democratization; or alternatively, inequality could prevent democratization by generating a threat from elites. In addition other forces, such as international pressure, could give rise to democratization. The question is: how do we update on our beliefs that low inequality caused democratization when we observe mobilization or international pressure?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"I {-}\textgreater{} M {-}\textgreater{} D \textless{}{-} I; P {-}\textgreater{} D"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{set\_restrictions}\NormalTok{(}\FunctionTok{c}\NormalTok{( }
    \StringTok{"(M[I=1] \textless{} M[I=0])"}\NormalTok{, }
    \StringTok{"(D[I=1] \textgreater{} D[I=0]) | (D[M=1] \textless{} D[M=0]) | (D[P=1] \textless{} D[P=0])"}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

We can read inferences directly from \texttt{query\_model}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{query\_model}\NormalTok{(model, }
            \AttributeTok{query =} \FunctionTok{list}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{I = 0 caused D = 1}\StringTok{\textasciigrave{}} \OtherTok{=} \StringTok{"D[I=1] != D[I=0]"}\NormalTok{), }
            \AttributeTok{using =} \StringTok{"parameters"}\NormalTok{, }
            \AttributeTok{given =} \FunctionTok{c}\NormalTok{(}\StringTok{"I==0 \& D==1"}\NormalTok{, }
                       \StringTok{"I==0 \& D==1 \& M==0"}\NormalTok{, }
                       \StringTok{"I==0 \& D==1 \& M==1"}\NormalTok{, }
                       \StringTok{"I==0 \& D==1 \& P==0"}\NormalTok{, }
                       \StringTok{"I==0 \& D==1 \& P==1"}\NormalTok{, }
                       \StringTok{"I==0 \& D==1 \& M == 0 \& P==0"}\NormalTok{,}
                       \StringTok{"I==0 \& D==1 \& M == 1 \& P==0"}\NormalTok{,}
                       \StringTok{"I==0 \& D==1 \& M == 0 \& P==1"}\NormalTok{,}
                       \StringTok{"I==0 \& D==1 \& M == 1 \& P==1"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} \FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|l|r}
\hline
Query & Given & Using & Case.estimand & mean\\
\hline
I = 0 caused D = 1 & I==0 \& D==1 & parameters & FALSE & 0.4384\\
\hline
I = 0 caused D = 1 & I==0 \& D==1 \& M==0 & parameters & FALSE & 0.4750\\
\hline
I = 0 caused D = 1 & I==0 \& D==1 \& M==1 & parameters & FALSE & 0.3939\\
\hline
I = 0 caused D = 1 & I==0 \& D==1 \& P==0 & parameters & FALSE & 0.6154\\
\hline
I = 0 caused D = 1 & I==0 \& D==1 \& P==1 & parameters & FALSE & 0.3404\\
\hline
I = 0 caused D = 1 & I==0 \& D==1 \& M == 0 \& P==0 & parameters & FALSE & 0.6667\\
\hline
I = 0 caused D = 1 & I==0 \& D==1 \& M == 1 \& P==0 & parameters & FALSE & 0.5714\\
\hline
I = 0 caused D = 1 & I==0 \& D==1 \& M == 0 \& P==1 & parameters & FALSE & 0.3929\\
\hline
I = 0 caused D = 1 & I==0 \& D==1 \& M == 1 \& P==1 & parameters & FALSE & 0.2632\\
\hline
\end{tabular}

We see in this example that learning about a rival cause---the moderator \(P\) (international pressure)---induces larger changes in beliefs than learning about the mediator, \(M\) (mobilization). The two clues substitute for each other marginally.

The importance of different clues depends however on what one wants to explain. In the next analysis, we see that if we want to know if inequality explained democratization, learning that \(M=0\) has a large impact on beliefs.

\begin{tabular}{l|l|l|l|r}
\hline
Query & Given & Using & Case.estimand & mean\\
\hline
I = 1 caused D = 1 & I==1 \& D==1 & parameters & FALSE & 0.1277\\
\hline
I = 1 caused D = 1 & I==1 \& D==1 \& M==0 & parameters & FALSE & 0.0000\\
\hline
I = 1 caused D = 1 & I==1 \& D==1 \& M==1 & parameters & FALSE & 0.1500\\
\hline
I = 1 caused D = 1 & I==1 \& D==1 \& P==0 & parameters & FALSE & 0.2308\\
\hline
I = 1 caused D = 1 & I==1 \& D==1 \& P==1 & parameters & FALSE & 0.0882\\
\hline
\end{tabular}

Note that inferences are taken here based on the model made by \texttt{make\_model}, without any updating of the model using data. In this sense the approach simply makes the model used for process tracing explicit, but it does not justify. It is possible however to first update a model using data from many cases and then use the updated model to draw inferences about a single case.

\hypertarget{probative-value-and-d-separation}{%
\section{\texorpdfstring{Probative value and \(d\)-separation}{Probative value and d-separation}}\label{probative-value-and-d-separation}}

Observation of a node (a ``clue'') is potentially informative for a query when it is \emph{not} \(d\)-separated\footnote{\(d\)-separation is a key idea in the study of directed acyclic graphs; for an introduction see \href{http://www.dagitty.net/learn/dsep/index\%3C-.html}{\(d\)-separation without tears}.} from query-relevant nodes (See \emph{Integrated Inferences}, Ch 6).

An implication of this is that the observation of some nodes may render other nodes more or less informative. From the graph alone you can sometimes tell when additional data will be uninformative for a query.

To wit:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} Y {-}\textgreater{} S \textless{}{-} W"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{set\_restrictions}\NormalTok{(}\FunctionTok{complements}\NormalTok{(}\StringTok{"Y"}\NormalTok{, }\StringTok{"W"}\NormalTok{, }\StringTok{"S"}\NormalTok{), }\AttributeTok{keep =} \ConstantTok{TRUE}\NormalTok{)}

\FunctionTok{plot}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\includegraphics{causalmodels_files/figure-latex/appdsepar-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{query\_model}\NormalTok{(model,}
            \AttributeTok{query =} \StringTok{"Y[X=1] \textgreater{} Y[X=0]"}\NormalTok{,}
            \AttributeTok{using =} \StringTok{"parameters"}\NormalTok{,}
            \AttributeTok{given =} \FunctionTok{c}\NormalTok{(}\StringTok{"X==1"}\NormalTok{,}
                      \StringTok{"X==1 \& W==1"}\NormalTok{,}
                      \StringTok{"X==1 \& S==1"}\NormalTok{,}
                      \StringTok{"X==1 \& S==1 \& W==1"}\NormalTok{, }
                      \StringTok{"X==1 \& Y==1"}\NormalTok{,}
                      \StringTok{"X==1 \& W==1 \& S==1 \& Y==1"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} \FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-88}Whether a clue is informative or not depends on what else has been observed: in particular whether the clue is $d$-separated from the query.}
\centering
\begin{tabular}[t]{r|l|l|l|r}
\hline
Query & Given & Using & Case.estimand & mean\\
\hline
1 & X==1 & parameters & FALSE & 0.25\\
\hline
2 & X==1 \& W==1 & parameters & FALSE & 0.25\\
\hline
3 & X==1 \& S==1 & parameters & FALSE & 0.25\\
\hline
4 & X==1 \& S==1 \& W==1 & parameters & FALSE & 0.40\\
\hline
5 & X==1 \& Y==1 & parameters & FALSE & 0.50\\
\hline
6 & X==1 \& W==1 \& S==1 \& Y==1 & parameters & FALSE & 0.50\\
\hline
\end{tabular}
\end{table}

In this example \(W\) is not informative for the \(X\) causes \(Y\) query (a query about \(\theta^Y\), a parent of \(Y\)), when \(Y\) and \(S\) are unobserved (Row 1 = Row 3). It becomes informative, however, when \(S\), a symptom of \(Y\), is observed (Row 3 \(\neq\) Row 4). But when \(Y\) is observed neither \(S\) nor \(W\) are informative (Row 5 = Row 6).

The reason is that \(W\) is \(d\)-separated from \(\theta^Y\) when \(Y\) and \(S\) are unobserved. But \(S\) is a ``collider'' for \(Y\) and \(W\) and so \(W\) \emph{becomes} informative about \(Y\) once \(S\) is observed, and hence of \(\theta^Y\) (so long as \(Y\) is unobserved). When \(Y\) is observed however now \(S\) and \(W\) become \(d\)-separated from \(\theta^Y\) and neither is informative.

\hypertarget{foundations-for-van-everas-tests}{%
\section{Foundations for Van Evera's tests}\label{foundations-for-van-everas-tests}}

Students of process tracing often refer to a set of classical ``qualitative tests'' that are used to link within-case evidence to inferences around specific (often case-level) hypotheses. The four classical tests as described by \citet{collier2011understanding} and drawing on \citet{Van-Evera:1997} are ``smoking gun'' tests, ``hoop'' tests, ``doubly decisive'' tests, and ``straw-in-the-wind'' tests. A hoop test is one which, if failed, bodes especially badly for a claim; a smoking gun test is one that bodes well for a hypothesis if passed; a doubly decisive test is strongly conclusive no matter what is found, and a straw-in-the-wind test is suggestive, though not conclusive, either way.

In some treatments (such as \citet{humphreys2015mixing}) formalization involves specifying a prior that a hypothesis is true and an independent set of beliefs about the probability of seeing some data if the hypothesis is true and if it is false. Then updating proceeds using Bayes' rule.

This simple approach suffers from two related weaknesses however: first, there is no good reason to expect these probabilities to be independent; second, there is nothing in the set-up to indicate how beliefs around the probative value of clues can be established or justified.

Both of these problems are easily resolved if the problem is articulated using fully specified causal models.

Many different causal models might justify Van Evera's tests. We illustrate using one in which the requisite background knowledge to justify the tests can be derived from a factorial experiment and in which one treatment serves as a clue for the effect of another.

For the illustration we first make use of a function that generates data from a model with a constrained set of types for \(Y\) and a given prior distribution over clue \(K\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{van\_evera\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(y\_types, k\_types)}
  
  \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} Y \textless{}{-} K"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  
  \FunctionTok{set\_restrictions}\NormalTok{(}\AttributeTok{labels =} \FunctionTok{list}\NormalTok{(}\AttributeTok{Y =}\NormalTok{ y\_types), }\AttributeTok{keep =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  
  \FunctionTok{set\_parameters}\NormalTok{(}\AttributeTok{param\_type =} \StringTok{"define"}\NormalTok{, }\AttributeTok{node =} \StringTok{"K"}\NormalTok{, }\AttributeTok{parameters =} \FunctionTok{c}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ k\_types, k\_types)) }\SpecialCharTok{|\textgreater{}}
  
  \FunctionTok{make\_data}\NormalTok{(}\AttributeTok{n =} \DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We then use a function that draws inferences, given different values of a clue \(K\), from a model that has been updated using available data. Note that the model that is updated has no constraints on \(Y\), has flat beliefs over the distribution of \(K\), and imposes no assumption that \(K\) is informative for how \(Y\) reacts to \(X\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{van\_evera\_inference }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data)}
  
  \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} Y \textless{}{-} K"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  
  \FunctionTok{update\_model}\NormalTok{(}\AttributeTok{data =}\NormalTok{ data) }\SpecialCharTok{|\textgreater{}}  
  
  \FunctionTok{query\_model}\NormalTok{(}\AttributeTok{query =} \StringTok{"Y[X=1] \textgreater{} Y[X=0]"}\NormalTok{, }
              \AttributeTok{given =} \FunctionTok{c}\NormalTok{(}\ConstantTok{TRUE}\NormalTok{, }\StringTok{"K==0"}\NormalTok{, }\StringTok{"K==1"}\NormalTok{),}
              \AttributeTok{using =} \StringTok{"posteriors"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can now generate posterior beliefs, given \(K\), for different types of tests where the tests are now justified by different types of data, coupled with a common prior causal model.

Results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{doubly\_decisive }\OtherTok{\textless{}{-}} \FunctionTok{van\_evera\_data}\NormalTok{(}\StringTok{"0001"}\NormalTok{, .}\DecValTok{5}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{van\_evera\_inference}\NormalTok{()}

\NormalTok{hoop            }\OtherTok{\textless{}{-}} \FunctionTok{van\_evera\_data}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"0001"}\NormalTok{, }\StringTok{"0101"}\NormalTok{), .}\DecValTok{9}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{van\_evera\_inference}\NormalTok{()}

\NormalTok{smoking\_gun     }\OtherTok{\textless{}{-}} \FunctionTok{van\_evera\_data}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"0001"}\NormalTok{, }\StringTok{"0011"}\NormalTok{), .}\DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{van\_evera\_inference}\NormalTok{()}

\NormalTok{straw\_in\_wind   }\OtherTok{\textless{}{-}} \FunctionTok{van\_evera\_data}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"0001"}\NormalTok{, }\StringTok{"0101"}\NormalTok{, }\StringTok{"0011"}\NormalTok{), .}\DecValTok{5}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{van\_evera\_inference}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-92}Doubly decisive test}
\centering
\begin{tabular}[t]{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
Q 1 & - & posteriors & FALSE & 0.5091 & 0.0155 & 0.4793 & 0.5390\\
\hline
Q 1 & K==0 & posteriors & FALSE & 0.0094 & 0.0052 & 0.0023 & 0.0222\\
\hline
Q 1 & K==1 & posteriors & FALSE & 0.9763 & 0.0074 & 0.9591 & 0.9886\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:unnamed-chunk-92}Hoop test}
\centering
\begin{tabular}[t]{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
Q 1 & - & posteriors & FALSE & 0.4605 & 0.0224 & 0.4174 & 0.5033\\
\hline
Q 1 & K==0 & posteriors & FALSE & 0.0367 & 0.0231 & 0.0069 & 0.0933\\
\hline
Q 1 & K==1 & posteriors & FALSE & 0.5122 & 0.0243 & 0.4646 & 0.5594\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:unnamed-chunk-92}Smoking gun test}
\centering
\begin{tabular}[t]{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
Q 1 & - & posteriors & FALSE & 0.5240 & 0.0226 & 0.4799 & 0.5692\\
\hline
Q 1 & K==0 & posteriors & FALSE & 0.4801 & 0.0243 & 0.4323 & 0.5283\\
\hline
Q 1 & K==1 & posteriors & FALSE & 0.8991 & 0.0371 & 0.8132 & 0.9566\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:unnamed-chunk-92}Straw in the wind test}
\centering
\begin{tabular}[t]{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
Q 1 & - & posteriors & FALSE & 0.4919 & 0.0216 & 0.4492 & 0.5340\\
\hline
Q 1 & K==0 & posteriors & FALSE & 0.3075 & 0.0285 & 0.2544 & 0.3649\\
\hline
Q 1 & K==1 & posteriors & FALSE & 0.6828 & 0.0301 & 0.6224 & 0.7404\\
\hline
\end{tabular}
\end{table}

We see that these tests all behave as expected. Importantly, however, the approach to thinking about the tests is quite different to that described in \citet{collier2011understanding} or \citet{humphreys2015mixing}. Rather than having a belief about the probative value of a clue, and a prior over a hypothesis, inferences are drawn directly from a causal model that relates a clue to possible causal effects. Critically, with this approach, the inferences made from observing clues can be justified by reference to a more fundamental, agnostic model, that has been updated in light of data. The updated model yields both a prior over the proposition, belief about probative values, and guidance for what conclusions to draw given knowledge of \(K\).

\hypertarget{clue-selection-clues-at-the-center-of-chains-can-be-more-informative}{%
\section{Clue selection: clues at the center of chains can be more informative}\label{clue-selection-clues-at-the-center-of-chains-can-be-more-informative}}

Model querying can also be used to assess which types of clues are more informative among a set of informative clues. Consider a chain linking \(X\) to \(Y\) via \(M_1\), \(M_2\), \(M_3\). To keep things simple let's assume that the chain is monotonic: no node in the chain has a negative effect on the next node in the chain.

Which clue is most informative for the proposition that \(X\) caused \(Y\) in a case with \(X=Y=1\)?

In all case we will conclude that \(X\) did not cause \(Y\) if we see a 0 along the chain (since a 1 can not cause a 0). But what do we conclude if we see a 1?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} M1 {-}\textgreater{} M2 {-}\textgreater{} M3 {-}\textgreater{} Y"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_restrictions}\NormalTok{(}\AttributeTok{labels =} \FunctionTok{list}\NormalTok{(}\AttributeTok{M1 =} \StringTok{"10"}\NormalTok{, }\AttributeTok{M2 =} \StringTok{"10"}\NormalTok{, }\AttributeTok{M3 =} \StringTok{"10"}\NormalTok{, }\AttributeTok{Y =} \StringTok{"10"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

In imposing monotonicity and using default parameter values we are assuming that the effect of each node on the next node is 1/3. What does this imply for our query? We get the answer using \texttt{query\_model}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{query\_model}\NormalTok{(model, }
            \AttributeTok{query =} \StringTok{"Y[X=1] \textgreater{} Y[X=0]"}\NormalTok{, }
            \AttributeTok{given =} \FunctionTok{c}\NormalTok{(}\StringTok{"X==1 \& Y==1"}\NormalTok{, }\StringTok{"X==1 \& Y==1 \& M1==1"}\NormalTok{, }\StringTok{"X==1 \& Y==1 \& M2==1"}\NormalTok{, }
                      \StringTok{"X==1 \& Y==1 \& M3==1"}\NormalTok{, }\StringTok{"X==1 \& Y==1 \& M1==1 \& M2==1 \& M3==1"}\NormalTok{),}
            \AttributeTok{using=} \StringTok{"parameters"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|l|r}
\hline
Query & Given & Using & Case.estimand & mean\\
\hline
Q 1 & X==1 \& Y==1 & parameters & FALSE & 0.0244\\
\hline
Q 1 & X==1 \& Y==1 \& M1==1 & parameters & FALSE & 0.0357\\
\hline
Q 1 & X==1 \& Y==1 \& M2==1 & parameters & FALSE & 0.0400\\
\hline
Q 1 & X==1 \& Y==1 \& M3==1 & parameters & FALSE & 0.0357\\
\hline
Q 1 & X==1 \& Y==1 \& M1==1 \& M2==1 \& M3==1 & parameters & FALSE & 0.0625\\
\hline
\end{tabular}

A couple of features are worth noting. First without any data our beliefs that \(X\) caused \(Y\) are quite low. This is due to the fact that even though the ATE at each step is reasonably large, the ATE over the whole chain is small, only \((1/3)^4)\) (incidentally, a beautiful number: 0.01234568).

Second we learn from which nodes we learn the most. We update most strongly from positive evidence on the middle mediator. One can also show that not only is there greater updating higher if a positive outcome is seen on the middle mediator, but the \emph{expected} reduction in posterior variance is also greater (expected reduction in posterior variance takes account of the probability of observing different outcomes, which can also be calculated from the model given available data.)\footnote{These quantities can be calculated by the \texttt{CQtools} package, still in alpha, via: \texttt{CQtools::expected\_learning(model,\ "Y{[}X=1{]}\ \textgreater{}\ Y{[}X=0{]}",\ given\ =\ "X==1\ \&\ Y==1",\ strategy\ =\ "M2")}}

Last, while we update most strongly when we observe positive evidence on all steps, even that does not produce a large posterior probability that \(X=1\) caused \(Y=1\). Positive evidence on a causal chain is often not very informative. Explanations for this are in \citet{dawid2019bounding}.

\hypertarget{identification}{%
\chapter{Identification}\label{identification}}

\hypertarget{illustration-of-the-backdoor-criterion}{%
\section{Illustration of the backdoor criterion}\label{illustration-of-the-backdoor-criterion}}

Perhaps the most common approach to identifying causal effects in observational research is to condition on possible confounders. The ``backdoor'' criterion for identifying an effect of \(X\) on \(Y\) involves finding a set of nodes to condition on that collectively block all ``backdoor paths'' between \(X\) and \(Y\). The intuition is that if these paths are blocked, then any systematic correlation between \(X\) and \(Y\) reflects the effect of \(X\) on \(Y\).

To illustrate the backdoor criterion we want to show that estimates of the effect of \(X\) on \(Y\) are identified if we have data on a node that blocks a backdoor path---\(C\)---but not otherwise. With \texttt{CausalQueries} models however, rather than conditioning on \(C\) we simply include data on \(C\) in our model and update as usual.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"C {-}\textgreater{} X {-}\textgreater{} Y \textless{}{-} C"}\NormalTok{)  }\SpecialCharTok{|\textgreater{}}
         \FunctionTok{set\_restrictions}\NormalTok{(}\StringTok{"(Y[C=1]\textless{}Y[C=0])"}\NormalTok{)}


\CommentTok{\# Four types of data: Large, small, door open, door closed}
\NormalTok{N }\OtherTok{\textless{}{-}} \DecValTok{10000}
\NormalTok{df\_closed\_door\_large }\OtherTok{\textless{}{-}} \FunctionTok{make\_data}\NormalTok{(model, }\AttributeTok{n =}\NormalTok{ N)}
\NormalTok{df\_open\_door\_large   }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{(df\_closed\_door\_large, }\AttributeTok{C =} \ConstantTok{NA}\NormalTok{)}
\NormalTok{df\_closed\_door\_small }\OtherTok{\textless{}{-}}\NormalTok{ df\_closed\_door\_large[}\FunctionTok{sample}\NormalTok{(N, }\DecValTok{200}\NormalTok{), ]}
\NormalTok{df\_open\_door\_small   }\OtherTok{\textless{}{-}}\NormalTok{ df\_open\_door\_large[}\FunctionTok{sample}\NormalTok{(N, }\DecValTok{200}\NormalTok{), ]}
\end{Highlighting}
\end{Shaded}

\includegraphics{causalmodels_files/figure-latex/unnamed-chunk-94-1.pdf}

We see that with small \(n\) (200 units), closing the backdoor (by including data on \(C\)) produces a tighter distribution on the ATE. With large \(N\) (10,000 units) the distribution around the estimand collapses when the backdoor is closed but not when it is open.

\hypertarget{identification-instruments}{%
\section{Identification: Instruments}\label{identification-instruments}}

We illustrate how you can learn about whether \(X=1\) caused \(Y=1\) by taking advantage of an ``instrument,'' \(Z\).

We start with a model that builds in the instrumental variables exclusion restriction (no unobserved confounding between \(Z\) and \(Y\), no paths between \(Z\) and \(Y\) except through \(X\)) but does not include a monotonicity restriction (no negative effect of \(Z\) on \(X\)).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"Z {-}\textgreater{} X {-}\textgreater{} Y; X \textless{}{-}\textgreater{} Y"}\NormalTok{)  }

\FunctionTok{plot}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\includegraphics{causalmodels_files/figure-latex/appinstruments-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result }\OtherTok{\textless{}{-}} \FunctionTok{query\_model}\NormalTok{(}
\NormalTok{    updated, }
    \AttributeTok{queries =} \FunctionTok{list}\NormalTok{(}\AttributeTok{ATE =} \StringTok{"c(Y[X=1] {-} Y[X=0])"}\NormalTok{), }
    \AttributeTok{given =} \FunctionTok{list}\NormalTok{(}\ConstantTok{TRUE}\NormalTok{, }\StringTok{"X[Z=1] \textgreater{} X[Z=0]"}\NormalTok{,  }\StringTok{"X==0"}\NormalTok{,  }\StringTok{"X==1"}\NormalTok{),}
    \AttributeTok{using =} \StringTok{"posteriors"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
ATE & - & posteriors & FALSE & 0.4784 & 0.1073 & 0.2604 & 0.6316\\
\hline
ATE & X[Z=1] > X[Z=0] & posteriors & FALSE & 0.5813 & 0.0504 & 0.4765 & 0.6736\\
\hline
ATE & X==0 & posteriors & FALSE & 0.4851 & 0.1309 & 0.2155 & 0.6646\\
\hline
ATE & X==1 & posteriors & FALSE & 0.4716 & 0.1344 & 0.1980 & 0.6545\\
\hline
\end{tabular}

We calculate the average causal effect (a) for all (b) for the compliers and (c) conditional on values of \(M\).

We see here that the effects are strongest for the ``compliers''---units for whom \(X\) responds positively to \(Z\); in addition they are stronger for the treated than for the untreated. Moreover we see that the posterior variance on the complier average effect is low. If our model also imposed a monotonicity assumption then it would be lower still.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"Z {-}\textgreater{} X {-}\textgreater{} Y; X \textless{}{-}\textgreater{} Y"}\NormalTok{)  }\SpecialCharTok{|\textgreater{}}
         \FunctionTok{set\_restrictions}\NormalTok{(}\FunctionTok{decreasing}\NormalTok{(}\StringTok{"Z"}\NormalTok{, }\StringTok{"X"}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
ATE & - & posteriors & FALSE & 0.6055 & 0.0113 & 0.5838 & 0.628\\
\hline
ATE & X[Z=1] > X[Z=0] & posteriors & FALSE & 0.6055 & 0.0113 & 0.5838 & 0.628\\
\hline
ATE & X==0 & posteriors & FALSE & 0.6055 & 0.0113 & 0.5838 & 0.628\\
\hline
ATE & X==1 & posteriors & FALSE & 0.6055 & 0.0113 & 0.5838 & 0.628\\
\hline
\end{tabular}

\hypertarget{identification-through-the-frontdoor}{%
\section{Identification through the frontdoor}\label{identification-through-the-frontdoor}}

A less well known approach to identification uses information on the causal path from \(X\) to \(Y\). Consider the following model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frontdoor }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} M {-}\textgreater{} Y; X \textless{}{-}\textgreater{} Y"}\NormalTok{) }

\FunctionTok{plot}\NormalTok{(frontdoor)}
\end{Highlighting}
\end{Shaded}

\includegraphics{causalmodels_files/figure-latex/appfrontdoor-1.pdf}

Although in both the instrumental variables (IV) setup and the frontdoor setup we are trying to deal with confounding between \(X\) and \(Y\), the two differ in that in the IV set up we make use of a variable that is prior to \(X\) whereas in the frontdoor model we make use of a variable between \(X\) and \(Y\). In both cases we need other exclusion restrictions: here we see that there is no unobserved confounding between \(X\) and \(M\) or between \(M\) and \(Y\). Importantly too there is no direct path from \(X\) to \(Y\), only the path that runs through \(M\).

Below we plot posterior distributions given observations on 2000 units, with and without data on \(M\):

\includegraphics{causalmodels_files/figure-latex/appfrontdoor2-1.pdf}

The spike on the right confirms that we have identification.

\hypertarget{simple-sample-selection-bias}{%
\section{Simple sample selection bias}\label{simple-sample-selection-bias}}

Say we are interested in assessing the share of Republicans in a population but Republicans are (possible) systematically likely to be absent from our sample. What inferences can we make given our sample?

We will assume that we know when we have missing data, though of course we do not know the value of the missing data.

To tackle the problem we will include sample selection into our model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"R {-}\textgreater{} S"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_parameters}\NormalTok{(}\AttributeTok{node =} \StringTok{"R"}\NormalTok{, }\AttributeTok{parameters =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{/}\DecValTok{3}\NormalTok{,}\DecValTok{1}\SpecialCharTok{/}\DecValTok{3}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_parameters}\NormalTok{(}\AttributeTok{node =} \StringTok{"S"}\NormalTok{, }\AttributeTok{parameters =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\DecValTok{3}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\SpecialCharTok{/}\DecValTok{3}\NormalTok{, }\DecValTok{1}\SpecialCharTok{/}\DecValTok{3}\NormalTok{))}

\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{make\_data}\NormalTok{(model, }\AttributeTok{n =} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
        \FunctionTok{mutate}\NormalTok{(}\AttributeTok{R =} \FunctionTok{ifelse}\NormalTok{(S}\SpecialCharTok{==}\DecValTok{0}\NormalTok{, }\ConstantTok{NA}\NormalTok{, R ))}
\end{Highlighting}
\end{Shaded}

From this data and model, the priors and posteriors for population and sample quantities are:

\begin{tabular}{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
Q 1 & - & parameters & FALSE & 0.3333 &  & 0.3333 & 0.3333\\
\hline
Q 1 & - & priors & FALSE & 0.4975 & 0.2908 & 0.0236 & 0.9741\\
\hline
Q 1 & - & posteriors & FALSE & 0.5043 & 0.1391 & 0.2553 & 0.7352\\
\hline
Q 1 & S==1 & parameters & FALSE & 0.5000 &  & 0.5000 & 0.5000\\
\hline
Q 1 & S==1 & priors & FALSE & 0.4967 & 0.3071 & 0.0186 & 0.9820\\
\hline
Q 1 & S==1 & posteriors & FALSE & 0.4941 & 0.0234 & 0.4478 & 0.5398\\
\hline
\end{tabular}

For the population average effect we tightened our posteriors relative to the priors, though credibility intervals remain wide, even with large data, reflecting our uncertainty about the nature of selection. Our posteriors on the sample mean are accurate and tight.

Importantly we would not do so well if our data did not indicate that we had missingness.

\begin{tabular}{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
Q 1 & - & parameters & FALSE & 0.3333 &  & 0.3333 & 0.3333\\
\hline
Q 1 & - & posteriors & FALSE & 0.5096 & 0.0074 & 0.4954 & 0.5246\\
\hline
Q 1 & S==1 & parameters & FALSE & 0.5000 &  & 0.5000 & 0.5000\\
\hline
Q 1 & S==1 & posteriors & FALSE & 0.5096 & 0.0074 & 0.4955 & 0.5245\\
\hline
\end{tabular}

We naively conclude that all cases are sampled and that population effects are the same as sample effects. The problem here arises because the causal model does not encompass the data gathering process.

\hypertarget{addressing-both-sample-selection-bias-and-confounding}{%
\section{Addressing both sample selection bias and confounding}\label{addressing-both-sample-selection-bias-and-confounding}}

Consider the following model from \citet{bareinboim2016causal} (their Figure 4C). The key feature is that data is only seen for units with \(S=1\) (\(S\) for sampling).

In this model the relationship between \(X\) and \(Y\) is confounded. Two strategies work to address confounding: controlling for either \(Z\) or for \(W1\) \emph{and} \(W2\) works. But only the first strategy addresses the sample selection problem properly. The reason is that \(Z\) is independent of \(S\) and so variation in \(Z\) is not affected by selection on \(S\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{selection }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X \textless{}{-} W1 {-}\textgreater{} W2 {-}\textgreater{} X {-}\textgreater{} Y \textless{}{-} Z {-}\textgreater{} W2; W1 {-}\textgreater{} S"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{causalmodels_files/figure-latex/unnamed-chunk-100-1.pdf}

To keep the parameter and type space small we also impose a set of restrictions: \(S\) is non decreasing in \(W_1\), \(X\) is not decreasing in either \(W1\) or \(W2\), \(Y\) is not decreasing \(Z\) or \(X\) and \(X\) affects \(Y\) only if \(Z=1\). \(W_2=1\) if and only if both \(W_1=1\) and \(Z=1\). These all reduce the problem to one with 18 nodal types and 288 causal types.

Worth noting that in this model although selection is related to patterns of confounding, it is not related to causal effects: the effect of \(X\) on \(Y\) is not different from units that are or are not selected.

Given these priors we will assume a true (unknown) data generating process with no effect of \(X\) on \(Y\), in which \(W_1\) arises with a \(1/3\) probability but has a strong positive effect on selection into the sample when it does arise.

The estimand values given the true parameters and priors for this model are as shown below.

\begin{table}

\caption{\label{tab:appsimpleselcconf5}Estimand values}
\centering
\begin{tabular}[t]{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
Q 1 & - & parameters & FALSE & 0.000 &  & 0.0000 & 0.0000\\
\hline
Q 1 & - & priors & FALSE & 0.336 & 0.1541 & 0.0839 & 0.6725\\
\hline
\end{tabular}
\end{table}

This confirms a zero true effect, though priors are dispersed, centered on a positive effect.

We can see the inference challenge from observational data using regression analysis with and without conditioning on \(Z\) and \(W_1, W_2\).

Dependent variable:

Y

(1)

(2)

(3)

X

0.081***

0.080***

-0.006

(0.008)

(0.016)

(0.008)

X:W1\_norm

0.161***

(0.031)

X:W2\_norm

0.098***

(0.033)

X:Z\_norm

-0.047***

(0.016)

Observations

14,947

14,947

14,947

R2

0.007

0.021

0.117

Adjusted R2

0.007

0.020

0.117

Residual Std. Error

0.498 (df = 14945)

0.495 (df = 14941)

0.470 (df = 14943)

F Statistic

99.020*** (df = 1; 14945)

62.770*** (df = 5; 14941)

661.200*** (df = 3; 14943)

Note:

\emph{p\textless0.1; \textbf{p\textless0.05; }}p\textless0.01

Naive analysis is far off; but even after conditioning on \(W_1, W_2\) we still wrongly infer a positive effect.

Bayesian inferences given different data strategies are shown below:

\begin{tabular}{l|r|r}
\hline
data & mean & sd\\
\hline
X,Y & 0.0714 & 0.0144\\
\hline
X,Y, W1, W2 & 0.0127 & 0.0061\\
\hline
X, Y, Z & 0.0133 & 0.0054\\
\hline
\end{tabular}

We see the best performance is achieved for the model with data on \(Z\)---in this case the mean posterior estimate is closest to the truth--0--and the standard deviation is lowest also. However the gains in choosing \(Z\) over \(W1, W2\) are not as striking as in the regression estimates since knowledge of the model structure protects us from error.

\hypertarget{learning-from-a-collider}{%
\section{Learning from a collider!}\label{learning-from-a-collider}}

Conditioning on a collider can be a bad idea as it can introduce a correlation between variables that might not have existed otherwise \citep{elwert2014endogenous}. But that doesn't mean colliders should be ignored in analysis altogether. For a Bayesian, knowledge of the value of a collider can still be informative.

Pearl describes a model similar to the following as a case for which controlling for covariate \(W\) induces bias in the estimation of the effect of \(X\) on \(Y\), which could otherwise be estimated without bias using simple differences in means.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} Y \textless{}{-} U1 {-}\textgreater{} W \textless{}{-} U2 {-}\textgreater{} X"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  
  \FunctionTok{set\_restrictions}\NormalTok{(}\AttributeTok{labels =} \FunctionTok{list}\NormalTok{(}\AttributeTok{Y =} \FunctionTok{c}\NormalTok{(}\StringTok{"0001"}\NormalTok{, }\StringTok{"1111"}\NormalTok{), }\AttributeTok{W =} \StringTok{"0001"}\NormalTok{), }\AttributeTok{keep =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_restrictions}\NormalTok{(}\StringTok{"(X[U2=1]\textless{}X[U2=0])"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_parameters}\NormalTok{(}\AttributeTok{node =} \StringTok{"U1"}\NormalTok{,  }\AttributeTok{parameters =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\DecValTok{4}\NormalTok{, }\DecValTok{3}\SpecialCharTok{/}\DecValTok{4}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_parameters}\NormalTok{(}\AttributeTok{node =} \StringTok{"Y"}\NormalTok{, }\AttributeTok{parameters =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{/}\DecValTok{3}\NormalTok{, }\DecValTok{1}\SpecialCharTok{/}\DecValTok{3}\NormalTok{))}

\FunctionTok{plot}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\includegraphics{causalmodels_files/figure-latex/applearncoll-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{make\_data}\NormalTok{(model, }
                  \AttributeTok{n =} \DecValTok{25000}\NormalTok{, }
                  \AttributeTok{vars =} \FunctionTok{c}\NormalTok{(}\StringTok{"W"}\NormalTok{, }\StringTok{"X"}\NormalTok{, }\StringTok{"Y"}\NormalTok{), }
                  \AttributeTok{using =} \StringTok{"parameters"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The effect of \(X\) on \(Y\) is .5 but average effects as well as the probability of causation, are different for units with \(W=0\) and \(W=1\) (this, even though \(W\) does not affect \(Y\)):

\begin{tabular}{l|l|l|l|r}
\hline
Query & Given & Using & Case.estimand & mean\\
\hline
Y(1)-Y(0) & - & parameters & FALSE & 0.5000\\
\hline
Y(1)-Y(0) & W==0 & parameters & FALSE & 0.4000\\
\hline
Y(1)-Y(0) & W==1 & parameters & FALSE & 0.6667\\
\hline
Y(1)-Y(0) & X==1 \& Y==1 & parameters & FALSE & 0.6000\\
\hline
Y(1)-Y(0) & X==1 \& Y==1 \& W==0 & parameters & FALSE & 0.5000\\
\hline
Y(1)-Y(0) & X==1 \& Y==1 \& W==1 & parameters & FALSE & 0.6667\\
\hline
\end{tabular}

These are the quantities we seek to recover. The ATE can be gotten fairly precisely in a simple regression. But controlling for \(W\) introduces bias in the estimation of this effect (whether done using a simple control or an interactive model):

Dependent variable:

Y

(1)

(2)

(3)

X

0.498***

0.453***

0.458***

(0.005)

(0.005)

(0.005)

W

0.187***

(0.006)

W\_norm

-0.008

(0.008)

X:W\_norm

0.346***

(0.011)

Constant

0.334***

0.285***

0.333***

(0.004)

(0.004)

(0.004)

Observations

25,000

25,000

25,000

R2

0.255

0.286

0.313

Adjusted R2

0.255

0.286

0.313

Residual Std. Error

0.426 (df = 24998)

0.417 (df = 24997)

0.409 (df = 24996)

F Statistic

8,539.000*** (df = 1; 24998)

5,013.000*** (df = 2; 24997)

3,795.000*** (df = 3; 24996)

Note:

\emph{p\textless0.1; \textbf{p\textless0.05; }}p\textless0.01

How does the Bayesian model do, with and without data on \(W\)?

Without \(W\) we have:

\begin{table}

\caption{\label{tab:applearncoll6}Collider excluded from model}
\centering
\begin{tabular}[t]{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
Y(1)-Y(0) & - & posteriors & FALSE & 0.4904 & 0.0054 & 0.4797 & 0.5010\\
\hline
Y(1)-Y(0) & W==0 & posteriors & FALSE & 0.3608 & 0.1086 & 0.0795 & 0.4878\\
\hline
Y(1)-Y(0) & W==1 & posteriors & FALSE & 0.6620 & 0.0042 & 0.6535 & 0.6704\\
\hline
Y(1)-Y(0) & X==1 \& Y==1 & posteriors & FALSE & 0.5920 & 0.0054 & 0.5813 & 0.6024\\
\hline
Y(1)-Y(0) & X==1 \& Y==1 \& W==0 & posteriors & FALSE & 0.4181 & 0.1603 & 0.0364 & 0.5907\\
\hline
Y(1)-Y(0) & X==1 \& Y==1 \& W==1 & posteriors & FALSE & 0.6620 & 0.0042 & 0.6535 & 0.6704\\
\hline
\end{tabular}
\end{table}

Thus we estimate the treatment effect well. What's more we can estimate the probability of causation when \(W=1\) accurately, even though we have not observed \(W\). The reason is that if \(W=1\) then, given the model restrictions, we know that both \(U_1=1\) and \(U_2=1\) which is enough. We are not sure however what to infer when \(W=0\) since this could be due to either \(U_1=0\) or \(U_2=0\).

When we incorporate data on \(W\) our posteriors are:

\begin{table}

\caption{\label{tab:applearncoll8}Collider included in model}
\centering
\begin{tabular}[t]{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
Y(1)-Y(0) & - & posteriors & FALSE & 0.4905 & 0.0054 & 0.4795 & 0.5012\\
\hline
Y(1)-Y(0) & W==0 & posteriors & FALSE & 0.3878 & 0.0067 & 0.3741 & 0.4007\\
\hline
Y(1)-Y(0) & W==1 & posteriors & FALSE & 0.6621 & 0.0042 & 0.6540 & 0.6706\\
\hline
Y(1)-Y(0) & X==1 \& Y==1 & posteriors & FALSE & 0.5921 & 0.0053 & 0.5817 & 0.6029\\
\hline
Y(1)-Y(0) & X==1 \& Y==1 \& W==0 & posteriors & FALSE & 0.4874 & 0.0079 & 0.4712 & 0.5027\\
\hline
Y(1)-Y(0) & X==1 \& Y==1 \& W==1 & posteriors & FALSE & 0.6621 & 0.0042 & 0.6540 & 0.6706\\
\hline
\end{tabular}
\end{table}

We see including the collider does not induce error in estimation of the ATE, even though it does in a regression framework. Where we do well before we continue to do well. However the new information lets us improve our model and, in particular, we see that we now get a good and tight estimate for the probability that \(X=1\) caused \(Y=1\) in a case where \(W=0\).

In short, though conditioning on a collider induces error in a regression framework; including the collider as data for updating our causal model doesn't hurt us and can help us.

\hypertarget{mixing-methods}{%
\chapter{Mixing methods}\label{mixing-methods}}

In \citet{humphreys2015mixing} we describe an approach to mixed methods in which within-case inference from a small set of cases are combined with cross sectional data from many cases to form integrated inferences. Reconceived of as a process of updating causal models, the distinction between within-case and cross case data becomes difficult to maintain---both are, after all, just nodes on a model. However the basic procedure can still be implemented, with, in this case, a rooted justification for \emph{why} within case information is informative for estimands of interest.

\hypertarget{using-within-case-data-to-help-with-identification}{%
\section{Using within case data to help with identification}\label{using-within-case-data-to-help-with-identification}}

Here is a model in which a little within-case data adds a lot of leverage to assessing estimands of interest that cannot be estimated confidently with \(X\), \(Y\) data alone. In this model a front door type criterion is half satisfied. We assume \(X\rightarrow M \rightarrow Y\) but we allow confounding to take the form of \(X\) less likely in cases where \(Y=1\) regardless of \(X\).

With simple \(X,Y\) data, even on many cases (20,000 here), we cannot get precise estimates and we greatly underestimate the effect of \(X\) on \(Y\); with full data on \(M\) in many cases we do very well.

But we \emph{also} do well even with quite partial data on \(M\), e.g.~if we have data for 100 out of 20,000 cases.

Thus a little within-case data helps us make sense of the \(X,Y\) data we have.

The model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} 
  \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} M {-}\textgreater{} Y; M \textless{}{-}\textgreater{} X; X \textless{}{-}\textgreater{} Y"}\NormalTok{) }

\FunctionTok{plot}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\includegraphics{causalmodels_files/figure-latex/appmm2-1.pdf}

Parameters: We imagine a true model with a treatment effect of .25 but positive confounding (\(X=1\) more likely in cases in which \(Y=1\), regardless of \(X\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} 
  \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} M {-}\textgreater{} Y;  X \textless{}{-}\textgreater{} Y"}\NormalTok{) }

\NormalTok{model }\OtherTok{\textless{}{-}}\NormalTok{ model }\SpecialCharTok{|\textgreater{}}  
  \FunctionTok{set\_parameters}\NormalTok{(}
       \AttributeTok{node =} \FunctionTok{c}\NormalTok{(}\StringTok{"X"}\NormalTok{, }\StringTok{"M"}\NormalTok{, }\StringTok{"Y"}\NormalTok{),}
       \AttributeTok{parameters =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.75}\NormalTok{, }\FloatTok{0.25}\NormalTok{,}
\NormalTok{         .}\DecValTok{25}\NormalTok{, }\DecValTok{0}\NormalTok{, .}\DecValTok{5}\NormalTok{, .}\DecValTok{25}\NormalTok{,}
\NormalTok{         .}\DecValTok{25}\NormalTok{, }\DecValTok{0}\NormalTok{, .}\DecValTok{5}\NormalTok{, .}\DecValTok{25}\NormalTok{,}
\NormalTok{         .}\DecValTok{25}\NormalTok{, }\DecValTok{0}\NormalTok{, .}\DecValTok{5}\NormalTok{, .}\DecValTok{25}\NormalTok{))}

\FunctionTok{query\_model}\NormalTok{(model, }\FunctionTok{list}\NormalTok{(}\AttributeTok{ATE =} \StringTok{"Y[X=1] {-} Y[X=0]"}\NormalTok{), }\AttributeTok{using =} \StringTok{"parameters"}\NormalTok{)  }\SpecialCharTok{|\textgreater{}} \FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|l|r}
\hline
Query & Given & Using & Case.estimand & mean\\
\hline
ATE & - & parameters & FALSE & 0.25\\
\hline
\end{tabular}

We use the model to simulate different types of data we might have access to thus:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{20000}

\NormalTok{full\_data }\OtherTok{\textless{}{-}}  \FunctionTok{make\_data}\NormalTok{(model, n, }\AttributeTok{using =} \StringTok{"parameters"}\NormalTok{)}

\NormalTok{XY\_data }\OtherTok{\textless{}{-}}\NormalTok{ full\_data }\SpecialCharTok{|\textgreater{}} \FunctionTok{select}\NormalTok{(X, Y)}

\NormalTok{some\_data  }\OtherTok{\textless{}{-}}\NormalTok{ full\_data }\SpecialCharTok{|\textgreater{}}  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{M =} \FunctionTok{ifelse}\NormalTok{((}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n) }\SpecialCharTok{\%in\%} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, }\DecValTok{100}\NormalTok{), M, }\ConstantTok{NA}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Naive analysis:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(Y}\SpecialCharTok{\textasciitilde{}}\NormalTok{X, }\AttributeTok{data =}\NormalTok{ full\_data))}\SpecialCharTok{$}\NormalTok{coef }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 0.38 & 0.00 & 95.25 & 0\\
\hline
X & 0.25 & 0.01 & 31.32 & 0\\
\hline
\end{tabular}

Note that the estimated ATE is too high -- over twice what it should be.

In contrast as seen in the next graph the \texttt{CausalQueries} estimate using \(X,Y\) data only is low -- close to our prior at 0 and estimated with wide posterior variance.

With full data on \(X, M\) and \(Y\) we get a tight estimate on the ATE, though our estimate of the nature of confounding is not identified---though it has much tighter bounds than before. With partial data on \(M\) (100 cases out of 20000) we do nearly as well. A small amount of data is enough to narrow bounds on confounding and improve our estimates of the ATE considerably.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# X, Y data only}
\NormalTok{updated\_XY      }\OtherTok{\textless{}{-}} \FunctionTok{update\_model}\NormalTok{(model, XY\_data)}

\CommentTok{\# Full Data}
\NormalTok{updated\_full    }\OtherTok{\textless{}{-}} \FunctionTok{update\_model}\NormalTok{(model, full\_data)}

\CommentTok{\# Partial Data}
\NormalTok{updated\_partial }\OtherTok{\textless{}{-}} \FunctionTok{update\_model}\NormalTok{(model, some\_data)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{causalmodels_files/figure-latex/ch17mmfig-1.pdf}
\caption{\label{fig:ch17mmfig}Red lines denote estimand values, black lines show posterior means. Full data allows narrowing of posterior variance on confounding and tight esimates of treatment effects. But even limited data on \(M\) gets us quite far (bottom right panel).}
\end{figure}

\hypertarget{distinguishing-paths}{%
\section{Distinguishing paths}\label{distinguishing-paths}}

Here is another example of mixing methods where a little within-case data goes a long way. We imagine that available data makes us very confident that \(X\) causes \(Y\) but we want to know about channels. In such cases, with high confidence about overall effects, and \emph{absent unobserved confounding}, a little data on mediators might be highly informative.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} M1 {-}\textgreater{} Y \textless{}{-} M2 \textless{}{-} X"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_restrictions}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{decreasing}\NormalTok{(}\StringTok{"X"}\NormalTok{, }\StringTok{"M1"}\NormalTok{), }
                     \FunctionTok{decreasing}\NormalTok{(}\StringTok{"M1"}\NormalTok{, }\StringTok{"Y"}\NormalTok{),}
                     \FunctionTok{decreasing}\NormalTok{(}\StringTok{"X"}\NormalTok{, }\StringTok{"M2"}\NormalTok{), }
                     \FunctionTok{decreasing}\NormalTok{(}\StringTok{"M2"}\NormalTok{, }\StringTok{"Y"}\NormalTok{))) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_parameters}\NormalTok{(}\AttributeTok{node =} \StringTok{"M1"}\NormalTok{, }\AttributeTok{parameters =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{), }\AttributeTok{normalize=}\ConstantTok{FALSE}\NormalTok{)  }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_parameters}\NormalTok{(}\AttributeTok{node =} \StringTok{"M2"}\NormalTok{, }\AttributeTok{parameters =} \FunctionTok{c}\NormalTok{(.}\DecValTok{5}\NormalTok{, }\DecValTok{0}\NormalTok{,.}\DecValTok{5}\NormalTok{), }\AttributeTok{normalize=}\ConstantTok{FALSE}\NormalTok{)  }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_parameters}\NormalTok{(}\AttributeTok{statement =} \StringTok{"(Y[M1=1] == Y[M1=0])"}\NormalTok{, }\AttributeTok{parameters =} \DecValTok{0}\NormalTok{)  }

\FunctionTok{plot}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\includegraphics{causalmodels_files/figure-latex/apppaths-1.pdf}

We imagine that in truth \(X\) always causes \(Y\) and it does so via \(M1\), though this is not known \emph{ex ante}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Q1 }\OtherTok{\textless{}{-}} \FunctionTok{query\_model}\NormalTok{(model, }
  \AttributeTok{queries =} \FunctionTok{list}\NormalTok{(}\AttributeTok{ate =} \FunctionTok{te}\NormalTok{(}\StringTok{"X"}\NormalTok{, }\StringTok{"Y"}\NormalTok{), }
                 \AttributeTok{via\_M1 =} \StringTok{"(M1[X=1]\textgreater{}M1[X=0]) \& (Y[M1=1]\textgreater{}Y[M1=0])"}\NormalTok{, }
                 \AttributeTok{via\_M2 =} \StringTok{"(M2[X=1]\textgreater{}M2[X=0]) \& (Y[M2=1]\textgreater{}Y[M2=0])"}\NormalTok{), }
  \AttributeTok{using =} \FunctionTok{c}\NormalTok{(}\StringTok{"parameters"}\NormalTok{, }\StringTok{"priors"}\NormalTok{),}
  \AttributeTok{expand\_grid =} \ConstantTok{TRUE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:apppaths3}Priors and parameters}
\centering
\begin{tabular}[t]{l|l|l|r|r|r|r}
\hline
Query & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
ate & parameters & FALSE & 1.0000 &  & 1.0000 & 1.0000\\
\hline
via\_M1 & parameters & FALSE & 1.0000 &  & 1.0000 & 1.0000\\
\hline
via\_M2 & parameters & FALSE & 0.0000 &  & 0.0000 & 0.0000\\
\hline
ate & priors & FALSE & 0.2230 & 0.1396 & 0.0300 & 0.5470\\
\hline
via\_M1 & priors & FALSE & 0.1131 & 0.1024 & 0.0034 & 0.3807\\
\hline
via\_M2 & priors & FALSE & 0.1098 & 0.1005 & 0.0031 & 0.3696\\
\hline
\end{tabular}
\end{table}

Now suppose we have access to large \(X\), \(Y\), data.

\begin{table}

\caption{\label{tab:apppath4}Inferences with 1000 observations; data on X, Y, only}
\centering
\begin{tabular}[t]{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
ate & - & posteriors & FALSE & 0.9880 & 0.0049 & 0.9767 & 0.9955\\
\hline
via\_M1 & - & posteriors & FALSE & 0.4905 & 0.2214 & 0.0518 & 0.9242\\
\hline
via\_M2 & - & posteriors & FALSE & 0.4980 & 0.2213 & 0.0654 & 0.9362\\
\hline
\end{tabular}
\end{table}

We infer that that \(X\) certainly causes \(Y\). But we are unsure about channels.

However, this changes dramatically with data on \(M_1\) and \(M_2\). Here we assume data on only 20 cases.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{query\_model}\NormalTok{(updated\_mixed, }
  \AttributeTok{queries =} \FunctionTok{list}\NormalTok{(}\AttributeTok{ate =} \FunctionTok{te}\NormalTok{(}\StringTok{"X"}\NormalTok{, }\StringTok{"Y"}\NormalTok{), }
                 \AttributeTok{via\_M1 =} \StringTok{"(M1[X=1]\textgreater{}M1[X=0]) \& (Y[M1=1]\textgreater{}Y[M1=0])"}\NormalTok{, }
                 \AttributeTok{via\_M2 =} \StringTok{"(M2[X=1]\textgreater{}M2[X=0]) \& (Y[M2=1]\textgreater{}Y[M2=0])"}\NormalTok{), }
  \AttributeTok{using =} \FunctionTok{c}\NormalTok{(}\StringTok{"posteriors"}\NormalTok{),}
  \AttributeTok{expand\_grid =} \ConstantTok{TRUE}\NormalTok{)  }\SpecialCharTok{|\textgreater{}} \FunctionTok{kable}\NormalTok{(}\AttributeTok{caption =} \StringTok{"Inferences with 1000 observations for X, Y,  20 observations for M1, M2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:apppaths6}Inferences with 1000 observations for X, Y,  20 observations for M1, M2}
\centering
\begin{tabular}[t]{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
ate & - & posteriors & FALSE & 0.9862 & 0.0050 & 0.9750 & 0.9941\\
\hline
via\_M1 & - & posteriors & FALSE & 0.9850 & 0.0057 & 0.9717 & 0.9937\\
\hline
via\_M2 & - & posteriors & FALSE & 0.0012 & 0.0019 & 0.0000 & 0.0060\\
\hline
\end{tabular}
\end{table}

The data now convinces us that \(X\) must work only through one channel. Thus, again, small within case data can dramatically alter conclusions from large \(N\) data when that data has little discriminatory power for the estimands of interest.

\hypertarget{nothing-from-nothing}{%
\section{Nothing from nothing}\label{nothing-from-nothing}}

Many of the models we have looked at---especially for process tracing---have a lot of structure, viz:

\begin{itemize}
\tightlist
\item
  conditional independence assumptions
\item
  no confounding assumptions, and
\item
  monotonicity assumptions, or other restrictions
\end{itemize}

What happens if you have none of these? Can access to observational data render clues meaningful for inferences on causal effects?

We show the scope for learning from a mediator for a ``good case''---a world in which in fact (though unknown \emph{ex ante} to the researcher):

\begin{itemize}
\tightlist
\item
  \(X\) causes \(Y\) through \(M\)
\item
  \(X\) is a necessary condition for \(M\) and \(M\) is a sufficient condition for \(Y\) -- and so \(Y\) is monotonic in \(X\) and
\item
  there is no confounding
\end{itemize}

Here is the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} M {-}\textgreater{} Y"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  
        \FunctionTok{make\_data}\NormalTok{(}\AttributeTok{n =} \DecValTok{2000}\NormalTok{, }
                  \AttributeTok{parameters =} \FunctionTok{c}\NormalTok{(.}\DecValTok{5}\NormalTok{, .}\DecValTok{5}\NormalTok{, .}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, .}\DecValTok{8}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, .}\DecValTok{8}\NormalTok{, .}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-107}Data contains strong correlations.}
\centering
\begin{tabular}[t]{l|r|r|r}
\hline
  & X & M & Y\\
\hline
X & 1.00 & 0.81 & 0.61\\
\hline
M & 0.81 & 1.00 & 0.77\\
\hline
Y & 0.61 & 0.77 & 1.00\\
\hline
\end{tabular}
\end{table}

We imagine inferences are made starting from two types of model. In both we allow all possible links and we impose no restrictions on nodal types. Even though there are only three nodes, this model has 128 causal types (\(2\times 4 \times 16\)). In addition:

\begin{itemize}
\item
  In \texttt{model\_1} we allow confounding between all pairs of nodes. This results in 127 free parameters.
\item
  In \texttt{model\_2} we assume that \(X\) is known to be randomized. There are now only 64 free parameters.
\end{itemize}

Here are the models:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_1 }\OtherTok{\textless{}{-}} 
  \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} M {-}\textgreater{} Y \textless{}{-} X; X \textless{}{-}\textgreater{} M; M \textless{}{-}\textgreater{}Y; X \textless{}{-}\textgreater{} Y"}\NormalTok{) }

\NormalTok{model\_2 }\OtherTok{\textless{}{-}} 
  \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} M {-}\textgreater{} Y \textless{}{-} X; M \textless{}{-}\textgreater{}Y"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{causalmodels_files/figure-latex/unnamed-chunk-108-1.pdf} \includegraphics{causalmodels_files/figure-latex/unnamed-chunk-108-2.pdf}

After updating we query the models to see how inferences depend on \(M\) like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_1 }\OtherTok{\textless{}{-}} \FunctionTok{update\_model}\NormalTok{(model\_1, data, }\AttributeTok{iter =} \DecValTok{6000}\NormalTok{)}

\FunctionTok{query\_model}\NormalTok{(model\_1, }
            \AttributeTok{queries =} \StringTok{"Y[X=1] \textgreater{} Y[X=0]"}\NormalTok{,}
            \AttributeTok{given =} \FunctionTok{c}\NormalTok{(}\StringTok{"X==1 \& Y==1"}\NormalTok{, }\StringTok{"X==1 \& M==1 \& Y==1"}\NormalTok{),}
            \AttributeTok{using =} \FunctionTok{c}\NormalTok{(}\StringTok{"priors"}\NormalTok{, }\StringTok{"posteriors"}\NormalTok{), }
            \AttributeTok{expand\_grid =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-110}Can observation of large N data render mediator $M$ informative for case level inference? Observational data.}
\centering
\begin{tabular}[t]{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
Q 1 & X==1 \& Y==1 & posteriors & FALSE & 0.4994 & 0.1581 & 0.1980 & 0.7968\\
\hline
Q 1 & X==1 \& M==1 \& Y==1 & posteriors & FALSE & 0.4993 & 0.1667 & 0.1815 & 0.8135\\
\hline
Q 1 & X==1 \& M==0 \& Y==1 & posteriors & FALSE & 0.5005 & 0.1380 & 0.2405 & 0.7639\\
\hline
Q 1 & X==1 \& Y==1 & priors & FALSE & 0.5016 & 0.1042 & 0.2962 & 0.7056\\
\hline
Q 1 & X==1 \& M==1 \& Y==1 & priors & FALSE & 0.5018 & 0.1361 & 0.2374 & 0.7672\\
\hline
Q 1 & X==1 \& M==0 \& Y==1 & priors & FALSE & 0.5017 & 0.1346 & 0.2407 & 0.7533\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:unnamed-chunk-110}Can observation of large N data render mediator $M$ informative for case level inference? X randomized.}
\centering
\begin{tabular}[t]{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
Q 1 & X==1 \& Y==1 & posteriors & FALSE & 0.8519 & 0.0297 & 0.7959 & 0.9111\\
\hline
Q 1 & X==1 \& M==1 \& Y==1 & posteriors & FALSE & 0.8692 & 0.0320 & 0.8080 & 0.9321\\
\hline
Q 1 & X==1 \& M==0 \& Y==1 & posteriors & FALSE & 0.5294 & 0.1618 & 0.2134 & 0.8243\\
\hline
Q 1 & X==1 \& Y==1 & priors & FALSE & 0.5019 & 0.1061 & 0.2953 & 0.7048\\
\hline
Q 1 & X==1 \& M==1 \& Y==1 & priors & FALSE & 0.4996 & 0.1350 & 0.2443 & 0.7629\\
\hline
Q 1 & X==1 \& M==0 \& Y==1 & priors & FALSE & 0.5032 & 0.1366 & 0.2432 & 0.7622\\
\hline
\end{tabular}
\end{table}

We find that even with an auspicious monotonic data generating process in which \(M\) is a total mediator, \(M\) gives no traction on causal inference in Model 1 but it gives considerable leverage in Model 2: \(M\) is informative, especially if \(M=0\), when \(X\) is known to be randomized, but it provides essentially no guidance if it is not.

\hypertarget{external-validity-and-inference-aggregation}{%
\chapter{External validity and inference aggregation}\label{external-validity-and-inference-aggregation}}

\hypertarget{transportation-of-findings-across-contexts}{%
\section{Transportation of findings across contexts}\label{transportation-of-findings-across-contexts}}

Say we study the effect of \(X\) on \(Y\) in case 0 (a country, for instance) and want to make inferences to case 1 (another country). Our problem however is that effects are heterogeneous and features that differ across units may be related both to treatment assignment, outcomes, and selection into the sample. This is the problem studied by \citet{pearl2014external}. In particular \citet{pearl2014external} show for which nodes data is needed in order to ``licence'' external claims, given a model.

We illustrate with a simple model in which a confounder has a different distribution in a study site and a target site.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"Case {-}\textgreater{} W  {-}\textgreater{} X {-}\textgreater{} Y \textless{}{-} W"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_restrictions}\NormalTok{(}\StringTok{"W[Case = 1] \textless{} W[Case = 0]"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_parameters}\NormalTok{(}\AttributeTok{statement =} \StringTok{"X[W=1]\textgreater{}X[W=0]"}\NormalTok{, }\AttributeTok{parameters =} \DecValTok{1}\SpecialCharTok{/}\DecValTok{2}\NormalTok{)}\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_parameters}\NormalTok{(}\AttributeTok{statement =} \FunctionTok{complements}\NormalTok{(}\StringTok{"W"}\NormalTok{, }\StringTok{"X"}\NormalTok{, }\StringTok{"Y"}\NormalTok{), }\AttributeTok{parameters =}\NormalTok{ .}\DecValTok{17}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_parameters}\NormalTok{(}\AttributeTok{statement =} \FunctionTok{decreasing}\NormalTok{(}\StringTok{"X"}\NormalTok{, }\StringTok{"Y"}\NormalTok{), }\AttributeTok{parameters =} \DecValTok{0}\NormalTok{) }

\FunctionTok{plot}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\includegraphics{causalmodels_files/figure-latex/extval-1.pdf}

We start by checking some basic quantities in the priors and the posteriors, we will then see how we do with data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{query\_model}\NormalTok{(model,}
            \AttributeTok{queries =} \FunctionTok{list}\NormalTok{(}\AttributeTok{Incidence =} \StringTok{"W==1"}\NormalTok{, }
                           \AttributeTok{ATE =} \StringTok{"Y[X=1] {-} Y[X=0]"}\NormalTok{, }
                           \AttributeTok{CATE =} \StringTok{"Y[X=1, W=1] {-} Y[X=0, W=1]"}\NormalTok{),}
            \AttributeTok{given =} \FunctionTok{c}\NormalTok{(}\StringTok{"Case==0"}\NormalTok{, }\StringTok{"Case==1"}\NormalTok{),}
            \AttributeTok{using =} \FunctionTok{c}\NormalTok{(}\StringTok{"priors"}\NormalTok{, }\StringTok{"parameters"}\NormalTok{), }\AttributeTok{expand\_grid =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
Incidence & Case==0 & priors & FALSE & 0.3321 & 0.2365 & 0.0130 & 0.8435\\
\hline
Incidence & Case==0 & parameters & FALSE & 0.3333 &  & 0.3333 & 0.3333\\
\hline
Incidence & Case==1 & priors & FALSE & 0.6710 & 0.2344 & 0.1745 & 0.9868\\
\hline
Incidence & Case==1 & parameters & FALSE & 0.6667 &  & 0.6667 & 0.6667\\
\hline
ATE & Case==0 & priors & FALSE & -0.0006 & 0.1401 & -0.2792 & 0.2771\\
\hline
ATE & Case==0 & parameters & FALSE & 0.3333 &  & 0.3333 & 0.3333\\
\hline
ATE & Case==1 & priors & FALSE & -0.0024 & 0.1372 & -0.2725 & 0.2639\\
\hline
ATE & Case==1 & parameters & FALSE & 0.5727 &  & 0.5727 & 0.5727\\
\hline
CATE & Case==0 & priors & FALSE & -0.0012 & 0.1689 & -0.3368 & 0.3329\\
\hline
CATE & Case==0 & parameters & FALSE & 0.8121 &  & 0.8121 & 0.8121\\
\hline
CATE & Case==1 & priors & FALSE & -0.0012 & 0.1689 & -0.3368 & 0.3329\\
\hline
CATE & Case==1 & parameters & FALSE & 0.8121 &  & 0.8121 & 0.8121\\
\hline
\end{tabular}

We see that the incidence of \(W\) as well as the ATE of \(X\) on \(Y\) is larger in case 1 than in case 0 (in parameters, though not in priors). However the effect of \(X\) on \(Y\) conditional on \(W\) is the same in both places.

We now update the model \emph{using data on \(X\) and \(Y\) only from one case} (case 1) and data on \emph{W} from both and check inferences on the other.

The function \texttt{make\_data} lets us generate data like this by specifying a multistage data strategy:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{make\_data}\NormalTok{(model, }\AttributeTok{n =} \DecValTok{1000}\NormalTok{, }
                  \AttributeTok{vars =} \FunctionTok{list}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Case"}\NormalTok{, }\StringTok{"W"}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\StringTok{"X"}\NormalTok{, }\StringTok{"Y"}\NormalTok{)), }
                  \AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                  \AttributeTok{subsets =} \FunctionTok{c}\NormalTok{(}\ConstantTok{TRUE}\NormalTok{, }\StringTok{"Case ==1"}\NormalTok{))}

\NormalTok{transport }\OtherTok{\textless{}{-}} \FunctionTok{update\_model}\NormalTok{(model, data)}

\FunctionTok{query\_model}\NormalTok{(transport,}
            \AttributeTok{queries =} \FunctionTok{list}\NormalTok{(}\AttributeTok{Incidence =} \StringTok{"W==1"}\NormalTok{, }
                           \AttributeTok{ATE =} \StringTok{"Y[X=1] {-} Y[X=0]"}\NormalTok{, }
                           \AttributeTok{CATE =} \StringTok{"Y[X=1, W=1] {-} Y[X=0, W=1]"}\NormalTok{),}
            \AttributeTok{given =} \FunctionTok{c}\NormalTok{(}\StringTok{"Case==0"}\NormalTok{, }\StringTok{"Case==1"}\NormalTok{),}
            \AttributeTok{using =} \FunctionTok{c}\NormalTok{(}\StringTok{"posteriors"}\NormalTok{, }\StringTok{"parameters"}\NormalTok{), }\AttributeTok{expand\_grid =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-111}Extrapolation when two sites differ on $W$ and $W$ is observable in both sites}
\centering
\begin{tabular}[t]{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
Incidence & Case==0 & posteriors & FALSE & 0.3318 & 0.0066 & 0.3188 & 0.3448\\
\hline
Incidence & Case==0 & parameters & FALSE & 0.3333 &  & 0.3333 & 0.3333\\
\hline
Incidence & Case==1 & posteriors & FALSE & 0.6769 & 0.0069 & 0.6632 & 0.6902\\
\hline
Incidence & Case==1 & parameters & FALSE & 0.6667 &  & 0.6667 & 0.6667\\
\hline
ATE & Case==0 & posteriors & FALSE & 0.3427 & 0.0115 & 0.3199 & 0.3649\\
\hline
ATE & Case==0 & parameters & FALSE & 0.3333 &  & 0.3333 & 0.3333\\
\hline
ATE & Case==1 & posteriors & FALSE & 0.5772 & 0.0091 & 0.5591 & 0.5943\\
\hline
ATE & Case==1 & parameters & FALSE & 0.5727 &  & 0.5727 & 0.5727\\
\hline
CATE & Case==0 & posteriors & FALSE & 0.7967 & 0.0089 & 0.7784 & 0.8140\\
\hline
CATE & Case==0 & parameters & FALSE & 0.8121 &  & 0.8121 & 0.8121\\
\hline
CATE & Case==1 & posteriors & FALSE & 0.7967 & 0.0089 & 0.7784 & 0.8140\\
\hline
CATE & Case==1 & parameters & FALSE & 0.8121 &  & 0.8121 & 0.8121\\
\hline
\end{tabular}
\end{table}

We do well in recovering the (different) effects both in the location we study and the one in which we do not. In essence querying the model for the out of sample case requests a type of post stratification. We get the right answer, though as always this depends on the model being correct.

Had we attempted to make the extrapolation without data on \(W\) in country 1 we would get it wrong. In that case however we would also report greater posterior variance. The posterior variance here captures the fact that we know things could be different in country 1, but we don't know in what way they are different. Note that we get the CATE right since in the model this is assumed to be the same across cases.

\begin{table}

\caption{\label{tab:unnamed-chunk-112}Extrapolation when two sites differ on $W$ and $W$ is not observable in target country.}
\centering
\begin{tabular}[t]{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
Incidence & Case==0 & posteriors & FALSE & 0.3318 & 0.0067 & 0.3191 & 0.3450\\
\hline
Incidence & Case==0 & parameters & FALSE & 0.3333 &  & 0.3333 & 0.3333\\
\hline
Incidence & Case==1 & posteriors & FALSE & 0.6685 & 0.0065 & 0.6556 & 0.6810\\
\hline
Incidence & Case==1 & parameters & FALSE & 0.6667 &  & 0.6667 & 0.6667\\
\hline
ATE & Case==0 & posteriors & FALSE & 0.3400 & 0.0113 & 0.3178 & 0.3615\\
\hline
ATE & Case==0 & parameters & FALSE & 0.3333 &  & 0.3333 & 0.3333\\
\hline
ATE & Case==1 & posteriors & FALSE & 0.5800 & 0.0089 & 0.5618 & 0.5972\\
\hline
ATE & Case==1 & parameters & FALSE & 0.5727 &  & 0.5727 & 0.5727\\
\hline
CATE & Case==0 & posteriors & FALSE & 0.8163 & 0.0086 & 0.7990 & 0.8325\\
\hline
CATE & Case==0 & parameters & FALSE & 0.8121 &  & 0.8121 & 0.8121\\
\hline
CATE & Case==1 & posteriors & FALSE & 0.8163 & 0.0086 & 0.7990 & 0.8325\\
\hline
CATE & Case==1 & parameters & FALSE & 0.8121 &  & 0.8121 & 0.8121\\
\hline
\end{tabular}
\end{table}

\hypertarget{combining-observational-and-experimental-data}{%
\section{Combining observational and experimental data}\label{combining-observational-and-experimental-data}}

An interesting weakness of experimental studies is that, by dealing so effectively with self selection into treatment, they limit our ability to learn about self selection. Often however we want to know what causal effects would be specifically for people that would take up a treatment in non experimental settings. This kind of problem is studied for example by \citet{knox2019design}.

A causal model can encompass both experimental and observational data and let you answer this kind of question. To illustrate, imagine that node \(R\) indicates whether a unit was assigned to be randomly assigned to treatment assignment (\(X=Z\) if \(R=1\)) or took on its observational value (\(X=O\) if \(R=0\)). We assume the exclusion restriction that entering the experimental sample is not related to \(Y\) other than through assignment of \(X\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"R {-}\textgreater{} X {-}\textgreater{} Y; O {-}\textgreater{} X \textless{}{-} Z; O \textless{}{-}\textgreater{} Y"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  
    \FunctionTok{set\_restrictions}\NormalTok{(}\StringTok{"(X[R=1, Z=0]!=0) | (X[R=1, Z=1]!=1) | (X[R=0, O=0]!=0) | (X[R=0, O=1]!=1)"}\NormalTok{)}

\FunctionTok{plot}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\includegraphics{causalmodels_files/figure-latex/appcombexpob-1.pdf}

The parameter matrix has just one type for \(X\) since \(X\) really operates here as a kind of switch, inheriting the value of \(Z\) or \(O\) depending on \(R\). Parameters allow for complete confounding between \(O\) and \(Y\) but \(Z\) and \(Y\) are unconfounded.

We imagine parameter values in which there is a true .2 effect of \(X\) on \(Y\). However the effect is positive (.6) for cases in which \(X=1\) under observational assignment but negative (-.2) for cases in which \(X=0\) under observational assignment.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}}\NormalTok{ model }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{set\_parameters}\NormalTok{(}\AttributeTok{node =} \StringTok{"Y"}\NormalTok{, }\AttributeTok{given =} \StringTok{"O.0"}\NormalTok{, }\AttributeTok{parameters =} \FunctionTok{c}\NormalTok{(.}\DecValTok{8}\NormalTok{, .}\DecValTok{2}\NormalTok{,  }\DecValTok{0}\NormalTok{,  }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{set\_parameters}\NormalTok{(}\AttributeTok{node =} \StringTok{"Y"}\NormalTok{, }\AttributeTok{given =} \StringTok{"O.1"}\NormalTok{, }\AttributeTok{parameters =} \FunctionTok{c}\NormalTok{( }\DecValTok{0}\NormalTok{,  }\DecValTok{0}\NormalTok{, .}\DecValTok{6}\NormalTok{, .}\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

To parse this expression: we allow different parameter values for the four possible nodal types for \(Y\) when \(O=0\) and when \(O=1\). When \(O=0\) we have \((\lambda_{00} = .8, \lambda_{10} = .2, \lambda_{01} = 0, \lambda_{11} = 0)\) which implies a negative treatment effect and many \(Y=0\) observations. When \(O=1\) we have \((\lambda_{00} = 0, \lambda_{10} = 0, \lambda_{01} = .6, \lambda_{11} = .4)\) which implies a positive treatment effect and many \(Y=1\) observations.

The estimands:

\begin{table}

\caption{\label{tab:unnamed-chunk-115}estimands}
\centering
\begin{tabular}[t]{l|l|l|l|r}
\hline
Query & Given & Using & Case.estimand & mean\\
\hline
ATE & - & parameters & FALSE & 0.2\\
\hline
ATE & R==0 & parameters & FALSE & 0.2\\
\hline
ATE & R==1 & parameters & FALSE & 0.2\\
\hline
\end{tabular}
\end{table}

The priors:

\begin{table}

\caption{\label{tab:appcombexpobs2}priors}
\centering
\begin{tabular}[t]{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
ATE & - & priors & FALSE & 0.003 & 0.2579 & -0.5155 & 0.5266\\
\hline
ATE & R==0 & priors & FALSE & 0.003 & 0.2579 & -0.5155 & 0.5266\\
\hline
ATE & R==1 & priors & FALSE & 0.003 & 0.2579 & -0.5155 & 0.5266\\
\hline
\end{tabular}
\end{table}

Data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{make\_data}\NormalTok{(model, }\AttributeTok{n =} \DecValTok{800}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The true effect is .2 but naive analysis on the observational data would yield a strongly upwardly biased estimate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{estimatr}\SpecialCharTok{::}\FunctionTok{difference\_in\_means}\NormalTok{(Y}\SpecialCharTok{\textasciitilde{}}\NormalTok{X, }\AttributeTok{data =} \FunctionTok{filter}\NormalTok{(data, R}\SpecialCharTok{==}\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-118}Inferences on the ATE from differences in means}
\centering
\begin{tabular}[t]{l|r|r|r|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|) & CI Lower & CI Upper & DF\\
\hline
X & 0.808 & 0.029 & 27.57 & 0 & 0.75 & 0.865 & 181\\
\hline
\end{tabular}
\end{table}

The \texttt{CausalQueries} estimates are:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{posterior }\OtherTok{\textless{}{-}} \FunctionTok{update\_model}\NormalTok{(model, data)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
ATE & - & posteriors & FALSE & 0.176 & 0.031 & 0.1142 & 0.2367\\
\hline
ATE & R==0 & posteriors & FALSE & 0.176 & 0.031 & 0.1142 & 0.2367\\
\hline
ATE & R==1 & posteriors & FALSE & 0.176 & 0.031 & 0.1142 & 0.2367\\
\hline
\end{tabular}

Much better.

This model used both the experimental and the observational data. It is interesting to ask whether the observational data improved the estimates from the experimental data or did everything depend on the experimental data?

To see, lets do updating using experimental data only:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{updated\_no\_O }\OtherTok{\textless{}{-}} \FunctionTok{update\_model}\NormalTok{(model, dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(data, R}\SpecialCharTok{==}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
ATE & - & posteriors & FALSE & 0.1535 & 0.0414 & 0.0698 & 0.2325\\
\hline
ATE & R==0 & posteriors & FALSE & 0.1535 & 0.0414 & 0.0698 & 0.2325\\
\hline
ATE & R==1 & posteriors & FALSE & 0.1535 & 0.0414 & 0.0698 & 0.2325\\
\hline
\end{tabular}

In this case we get a tightening of posterior variance and a more accurate result when we use the observational data but the gains are relatively small. They would be smaller still if we had more data, in which case inferences from the experimental data would be more accurate still.

In both cases the estimates for the average effect in the randomized and the observationally assigned group are the same. This is how it should be since these are, afterall, randomly assigned into these groups.

Heterogeneity in this model lies between those that are in treatment and those that are in control \emph{in the observational} sample. We learn nothing about this heterogeneity from the experimental data alone but we learn a lot from the mixed model, picking up the strong self selection into treatment in the observational group:

\begin{tabular}{l|l|l|l|r|r|r|r}
\hline
Query & Given & Using & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
ATE & R==1 \& X==0 & posteriors & FALSE & 0.1760 & 0.0310 & 0.1142 & 0.2367\\
\hline
ATE & R==1 \& X==1 & posteriors & FALSE & 0.1760 & 0.0310 & 0.1142 & 0.2367\\
\hline
ATE & R==0 \& X==0 & posteriors & FALSE & -0.2148 & 0.0271 & -0.2664 & -0.1622\\
\hline
ATE & R==0 \& X==1 & posteriors & FALSE & 0.6079 & 0.0497 & 0.5074 & 0.7049\\
\hline
\end{tabular}

\hypertarget{a-jigsaw-puzzle-learning-across-populations}{%
\section{A jigsaw puzzle: Learning across populations}\label{a-jigsaw-puzzle-learning-across-populations}}

Consider a situation in which we believe the same model holds in multiple sites but in which learning about the model requires combining data about different parts of the model from multiple studies.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} 
  \FunctionTok{make\_model}\NormalTok{(}\StringTok{"X {-}\textgreater{} Y \textless{}{-} Z {-}\textgreater{} K"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_parameters}\NormalTok{(}
    \AttributeTok{statement =} \StringTok{"(Y[X=1, Z = 1] \textgreater{} Y[X=0, Z = 1])"}\NormalTok{, }\AttributeTok{parameters =}\NormalTok{ .}\DecValTok{24}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_parameters}\NormalTok{(}
    \AttributeTok{statement =} \StringTok{"(K[Z = 1] \textgreater{} K[Z = 0])"}\NormalTok{, }\AttributeTok{parameters =}\NormalTok{ .}\DecValTok{85}\NormalTok{)}


\FunctionTok{plot}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\includegraphics{causalmodels_files/figure-latex/jigsaw-1.pdf}

We imagine we have access to three types of data;

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Study 1 is a factorial study examining the joint effects of \(X\) and \(Z\) on \(Y\), \(K\) is not observed
\item
  Study 2 is an RCT looking at the relation between \(Z\) and \(K\). \(X\) and \(Y\) are not observed.
\item
  Study 3 is an experiment looking at the effects of \(X\) on \(Y\), ancillary data on context, \(K\) is collected but \(Z\) is not observed
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{make\_data}\NormalTok{(model, }\DecValTok{300}\NormalTok{, }\AttributeTok{using =} \StringTok{"parameters"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  
      \FunctionTok{mutate}\NormalTok{(}\AttributeTok{study =} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, }\AttributeTok{each =} \DecValTok{100}\NormalTok{),}
             \AttributeTok{K =} \FunctionTok{ifelse}\NormalTok{(study }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\ConstantTok{NA}\NormalTok{, K),}
             \AttributeTok{X =} \FunctionTok{ifelse}\NormalTok{(study }\SpecialCharTok{==} \DecValTok{2}\NormalTok{, }\ConstantTok{NA}\NormalTok{, X),}
             \AttributeTok{Y =} \FunctionTok{ifelse}\NormalTok{(study }\SpecialCharTok{==} \DecValTok{2}\NormalTok{, }\ConstantTok{NA}\NormalTok{, Y),}
             \AttributeTok{Z =} \FunctionTok{ifelse}\NormalTok{(study }\SpecialCharTok{==} \DecValTok{3}\NormalTok{, }\ConstantTok{NA}\NormalTok{, Z)}
\NormalTok{             )}
\end{Highlighting}
\end{Shaded}

Tables \ref{tab:frank1} - \ref{tab:frank3} show conditional inferences for the probability that \(X\) caused \(Y\) in \(X=Y=1\) cases conditional on \(K\) for each study, analyzed individually

\begin{table}

\caption{\label{tab:frank1}Clue is uninformative in Study 1}
\centering
\begin{tabular}[t]{l|l|r|r|r|r}
\hline
Given & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
X == 1 \& Y == 1 \& K == 1 & FALSE & 0.4963 & 0.1588 & 0.1999 & 0.8024\\
\hline
X == 1 \& Y == 1 \& K == 0 & FALSE & 0.4972 & 0.1544 & 0.2082 & 0.7914\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:frank2}Clue is also uninformative in Study 2 (factorial)}
\centering
\begin{tabular}[t]{l|l|r|r|r|r}
\hline
Given & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
X == 1 \& Y == 1 \& K == 1 & FALSE & 0.5698 & 0.1295 & 0.3047 & 0.8135\\
\hline
X == 1 \& Y == 1 \& K == 0 & FALSE & 0.5681 & 0.1292 & 0.3046 & 0.8108\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:frank3}Clue is also uninformative in Study 3 (experiment studying $K$)}
\centering
\begin{tabular}[t]{l|l|r|r|r|r}
\hline
Given & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
X == 1 \& Y == 1 \& K == 1 & FALSE & 0.4984 & 0.1611 & 0.1975 & 0.8062\\
\hline
X == 1 \& Y == 1 \& K == 0 & FALSE & 0.5000 & 0.1624 & 0.1840 & 0.8100\\
\hline
\end{tabular}
\end{table}

In no case is \(K\) informative. In study 1 data on \(K\) is not available, in study 2 it is available but researchers do not know, quantitatively, how it relates to \(Z\). In the third study the \(Z,K\) relationship is well understood but the joint relation between \(Z,X\), and \(Y\) is not understood.

Table \ref{tab:frank4} shows the inferences when the data are combined with joint updating across all parameters.

\begin{table}

\caption{\label{tab:frank4}Clue is informative after combining studies linking $K$ to $Z$ and $Z$ to $Y$}
\centering
\begin{tabular}[t]{l|l|r|r|r|r}
\hline
Given & Case.estimand & mean & sd & conf.low & conf.high\\
\hline
X == 1 \& Y == 1 \& K == 1 & FALSE & 0.8395 & 0.0587 & 0.7105 & 0.9390\\
\hline
X == 1 \& Y == 1 \& K == 0 & FALSE & 0.4627 & 0.0759 & 0.3098 & 0.6044\\
\hline
X == 1 \& Y == 1 \& K == 1 \& Z == 1 & FALSE & 0.8864 & 0.0560 & 0.7511 & 0.9695\\
\hline
X == 1 \& Y == 1 \& K == 0 \& Z == 1 & FALSE & 0.8864 & 0.0560 & 0.7511 & 0.9695\\
\hline
X == 1 \& Y == 1 \& K == 1 \& Z == 0 & FALSE & 0.4535 & 0.0774 & 0.2985 & 0.5991\\
\hline
X == 1 \& Y == 1 \& K == 0 \& Z == 0 & FALSE & 0.4535 & 0.0774 & 0.2985 & 0.5991\\
\hline
\end{tabular}
\end{table}

Here fuller understanding of the model lets researchers use information on \(K\) to update on values for \(Z\) and in turn update on the likely effects of \(X\) on \(Y\). Rows 3-6 highlight that the updating works through inferences on \(Z\) and there if \(Z\) is known, as in Study 2, there are no additional gains from knowledge of \(K\).

The collection of studies collectively allow for inferences that are not possible from any one study.

\hypertarget{part-notation}{%
\part{Notation}\label{part-notation}}

\hypertarget{notation}{%
\chapter{Notation and syntax}\label{notation}}

\hypertarget{notation-1}{%
\section{Notation}\label{notation-1}}

A guide to key notation used in the \texttt{CausalQueries} package:

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2174}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1594}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6232}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
\textbf{term}
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\textbf{symbol}
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\textbf{meaning}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
nodal type & \(\theta^X\) & The way that a node responds to the values of its parents. Example: \(\theta^Y_{10}\), written \texttt{Y10}: \(Y\) takes the value 1 if \(X=0\) and 0 if \(X=1\). For interpretation of syntax, see \texttt{make\_model("X-\textgreater{}Y")\ \%\textgreater{}\%\ get\_nodal\_types} \\
causal type & \(\theta\) & A causal type is a concatenation of nodal types, one for each node. Example: \((\theta^X_0, \theta^Y_{00})\), written \texttt{X0.Y00}, is a type that has \(X=0\) and \(Y=0\) no matter what the value of \(X\). See: \texttt{make\_model("X-\textgreater{}Y")\ \%\textgreater{}\%\ get\_causal\_types} \\
parameter & \(\lambda\) & An unknown quantity of interest that generates the probability of causal types. In models without confounding parameters are the probabilities of nodal types. In models with confounding, parameters are the conditional probabilities of causal types. Example: \texttt{X.0}\(=\lambda^X_0 = \Pr(\theta^X = \theta^X_0)\) See \texttt{make\_model("X-\textgreater{}Y")\ \%\textgreater{}\%\ get\_parameters} \\
data event type & & A possible set of values on all nodes (including, possibly, NAs). Example: \texttt{X0Y1} \(= (X=0, Y = 1)\) \\
event probability & \(w\) & The probability of a data event type. Example: \(w_{00}=\Pr(X=0, Y=1)\) \\
Dirichlet priors & alpha, \(\alpha\) & Non negative numbers used to characterize a prior distribution over a simplex. The implied mean is the normalized vector \(\mu= \alpha/\sum(\alpha)\) and the variance is \(\mu(1-\mu)/(1+\sum\alpha)\). \\
parameter matrix & \(P\) & A matrix of 0s and 1s that maps from parameters (rows) to causal types (columns). Example: see \texttt{make\_model("X-\textgreater{}Y")\ \%\textgreater{}\%\ get\_parameter\_matrix} \\
ambiguities matrix & \(A\) & A matrix of 0s and 1s that maps from causal types (rows) to data types (columns). Example: see \texttt{make\_model("X-\textgreater{}Y")\ \%\textgreater{}\%\ \ get\_ambiguities\_matrix} \\
families matrix & \(E\) & A matrix of 0s and 1s that maps from partial data events to complete data events. Example: see \texttt{make\_model("X-\textgreater{}Y")\ \%\textgreater{}\%\ get\_data\_families} \\
data strategy & \(S\) & A plan indicating for how many nodes different types of data will be gathered. See \texttt{?\ make\_data} \\
\end{longtable}

\hypertarget{parents}{%
\subsection{Parents, children, and all that}\label{parents}}

The causal models analyzed by \texttt{CausalQueries} all involve \emph{directed} edges between nodes, with cycles over nodes precluded. In turn this implies a partial ordering over nodes which motivates some useful terminology:

\begin{itemize}
\tightlist
\item
  \(X\) is a \emph{parent} of \(Y\) if a change in \(X\) sometimes induces a change in \(Y\) even when all other nodes are fixed. On the graph, there's an arrow from \(X\) to \(Y\).
\item
  \(Y\) is a \emph{child} of \(X\) if a change in \(X\) sometimes induces a change in \(Y\) even when all other nodes are fixed. On the graph, there's an arrow from \(X\) to \(Y\).
\item
  \(A\) is an \emph{ancestor} of \(B\) by analogy: a parent is an ancestor and any parent of an ancestor is an ancestor. On the graph there is a chain of arrows pointing in one direction going from \(A\) to \(B\). Similarly for \emph{descendant}.
\end{itemize}

You should find that the package complains if you try to specify a cyclical graph.

\hypertarget{syntax}{%
\section{Causal syntax}\label{syntax}}

Both model definition and model querying requires a simple way to make arbitrary causal statements.

\begin{itemize}
\tightlist
\item
  You can query \textbf{observational quantities}. For instance:

  \begin{itemize}
  \tightlist
  \item
    \texttt{make\_model("X-\textgreater{}Y")\ \%\textgreater{}\%\ get\_query\_types("Y==1")} Figures out the types that produce \(Y=1\) absent any interventions.
  \end{itemize}
\item
  You can query \textbf{experimental quantities}. For instance:

  \begin{itemize}
  \tightlist
  \item
    \texttt{make\_model("X-\textgreater{}Y")\ \%\textgreater{}\%\ get\_query\_types("Y{[}X=1{]}==1")} figures out the types that produce \(Y=1\) when \(X\) is set to 1.
  \item
    \texttt{make\_model("X-\textgreater{}M-\textgreater{}Y")\ \%\textgreater{}\%\ get\_query\_types("Y{[}X=1{]}\textgreater{}Y{[}X=0{]}")} figures out the types that have a positive causal effect.
  \end{itemize}
\item
  You can make queries with \textbf{complex counterfactuals}. For instance:

  \begin{itemize}
  \tightlist
  \item
    \texttt{make\_model("X-\textgreater{}M-\textgreater{}Y")\ \%\textgreater{}\%\ get\_query\_types("Y{[}M=M{[}X=0{]},\ X=1{]}==1")} looks for the types for which \(Y=1\) when \(X=1\) and \(M\) is held constant at the value it would take if \(X\) were 0.
  \end{itemize}
\item
  You can use \textbf{wild cards} and AND or OR operators. For instance:

  \begin{itemize}
  \tightlist
  \item
    \texttt{make\_model("X-\textgreater{}Y")\ \%\textgreater{}\%\ get\_query\_types("(Y{[}X\ =\ .{]}==1)",\ join\_by\ =\ "\textbar{}")} figures out the causal types for which \(Y=1\) for some value of \(X\).
  \item
    \texttt{make\_model("X-\textgreater{}Y")\ \%\textgreater{}\%\ get\_query\_types("(Y{[}X\ =\ .{]}==1)",\ join\_by\ =\ "\&")} figures out the causal types for which \(Y=1\) for \emph{all} values of \(X\).
  \item
    Note that the use of ``.'' as a wild card also requires placing the causal statement in parentheses, as in these examples.
  \end{itemize}
\item
  You can make \textbf{conditional queries}. For instance, conditioning on observational or counterfactual quantities:

  \begin{itemize}
  \tightlist
  \item
    \texttt{make\_model("X-\textgreater{}Y")\ \%\textgreater{}\%\ query\_model("Y{[}X\ =\ 1{]}\ \textgreater{}\ Y{[}X\ =\ 0{]}",\ subset\ =\ "X==1\ \&\ Y==1")}
    asks what is the probability that \(X\) has a positive effect on \(Y\) given \(X=Y=1\).
  \item
    \texttt{make\_model("X-\textgreater{}M-\textgreater{}Y")\ \%\textgreater{}\%\ query\_model("Y{[}X\ =\ 1{]}\ !=\ Y{[}X\ =\ 0{]}",\ subsets\ =\ "M{[}X=1{]}==M{[}X=0{]}")}
    asks what is the probability that \(X\) matters for \(Y\) given \(X\) doesn't matter for \(M\).
  \end{itemize}
\end{itemize}

We provide a few helpers for common causal statements:

\begin{itemize}
\tightlist
\item
  \texttt{increasing("A",\ "B")} produces the statement \texttt{"B{[}A=1{]}\ \textgreater{}\ B{[}A=0{]}"}
\item
  \texttt{decreasing("A",\ "B")} produces the statement \texttt{"B{[}A=1{]}\ \textless{}\ B{[}A=0{]}"}
\item
  \texttt{interacts("A",\ "B",\ "C")} produces the statement \texttt{"((C{[}A\ =1,\ B\ =\ 1{]})\ -\ (C{[}A\ =\ 0,\ B\ =\ 1{]}))\ !=\ ((C{[}A\ =1,\ B\ =\ 0{]})\ -\ (C{[}A\ =\ 0,\ B\ =\ 0{]}))"}
\item
  \texttt{complements("A",\ "B",\ "C")} produces the statement \texttt{"((C{[}A\ =1,\ B\ =\ 1{]})\ -\ (C{[}A\ =\ 0,\ B\ =\ 1{]}))\ \textgreater{}\ ((C{[}A\ =1,\ B\ =\ 0{]})\ -\ (C{[}A\ =\ 0,\ B\ =\ 0{]}))"}
\item
  \texttt{substitutes("A",\ "B",\ "C")} produces the statement \texttt{"((C{[}A\ =1,\ B\ =\ 1{]})\ -\ (C{[}A\ =\ 0,\ B\ =\ 1{]}))\ \textless{}\ ((C{[}A\ =1,\ B\ =\ 0{]})\ -\ (C{[}A\ =\ 0,\ B\ =\ 0{]}))"}
\end{itemize}

These helpers can be used for setting restrictions, setting confounds, defining priors or parameter values, or querying models.

For instance:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get\_query\_types}\NormalTok{(}\AttributeTok{model =} \FunctionTok{make\_model}\NormalTok{(}\StringTok{"A {-}\textgreater{} B \textless{}{-} C"}\NormalTok{),}
         \AttributeTok{query =} \FunctionTok{substitutes}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"C"}\NormalTok{, }\StringTok{"B"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Causal types satisfying query's condition(s)  

 query =  ((B[A = 1, C = 1]) - (B[A = 0, C = 1])) < ((B[A = 1, C = 0]) - (B[A = 0, C = 0])) 

A0.C0.B0100  A1.C0.B0100
A0.C1.B0100  A1.C1.B0100
A0.C0.B0010  A1.C0.B0010
A0.C1.B0010  A1.C1.B0010
A0.C0.B0110  A1.C0.B0110
A0.C1.B0110  A1.C1.B0110
A0.C0.B1110  A1.C0.B1110
A0.C1.B1110  A1.C1.B1110
A0.C0.B0111  A1.C0.B0111
A0.C1.B0111  A1.C1.B0111


 Number of causal types that meet condition(s) =  20
 Total number of causal types in model =  64
\end{verbatim}

  \bibliography{bib.bib,packages.bib}

\end{document}

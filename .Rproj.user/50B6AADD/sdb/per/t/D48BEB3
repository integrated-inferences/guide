{
    "collab_server" : "",
    "contents" : "# Design: Case selection as a Decision Problem {#caseselection}\n```{r, include = FALSE}\nlibrary(biqq)\nlibrary(knitr)\n```\n\nA critical decision for scholars employing mixed methods is to determine which cases are most valuable for within-case analysis.  \n\nA host of different strategies have been proposed for selecting cases for in-depth study based on the observed values of $X$, $Y$ data. Perhaps the most common strategy is to select cases in which $X=1$ and $Y=1$ and look to see whether in fact $X$ caused $Y$ in the case in question (using some more or less formal strategy for inferring causality from within-case evidence). But many other strategies have been proposed, including strategies to select cases \"on the regression line\" or, for some purposes, cases \"off the regression line\" (e.g., @Lieberman2005nested). Some scholars suggest ensuring variation in $X$ (most prominently, @king1994designing), while others have proposed various kinds of matching strategies. Some have pointed to the advantages of random sampling of cases, either stratified or unstratified by values on $X$ or $Y$ (@FL2008, @HerronQuinn). \n\nWhich cases you should choose may depend, in part, on the purposes to which you want to put the case. \n\nSome texts propose a form of matching, selecting cases that are comparable on many features but that differ on $X$. If $Y$ is not known at the time of case selection, then this strategy replicates at a small scale the kind of inference done by matching estimators with large-$n$ data. The strategy emphasize the inferences to be made from $X,Y$ variation rather than inferences drawn specifically from within case information beyond what is available in the measurement of $X$ and $Y$. (Citations needed.)\n\nOther treatments seek to use qualitative information to check assumptions made in $X, Y$ analysis: for example, is the measurement of $X$ and $Y$ reliable in critical cases? (Citations needed) For such questions with limited resources, it might make sense to focus on cases for which validation plausibly makes a difference to the $X,Y$ inferences: for example influential cases that have unusually extreme values on $X$ and $Y$.\\^[Note: We can say more about why these would be good choices from a Bayesian perspective, based on the idea that measurement is more likely to be wrong in such cases and shifting them to more typical values would make a big difference.] Similar arguments are made for checking assumptions on selection processes, though we consider this a more complex desideratum since this requires making case level causal inferences and not simply measurement claims.\n\nA third purpose is to use a case to generate alternative or richer theories of causal processes, as in Lieberman's \"model-building\" mode of \"nested analysis\" (@Lieberman2005nested). Here it may be cases off the regression line that are of interest.\n\nIn what follows, we focus on a simpler goal: given existing $X, Y$ data for a set of cases and a given clue (or set of clues) that we can go looking for in the intensive analysis of some subset of these cases, for which cases would process tracing yield the greatest learning about the population-level causal effect of $X$ on $Y$?\n\nThe basic insight of this chapter is simple enough: the optimal strategy for case selection for a model-based analysis can be determined by the model and the query, just as we saw for the optimal clue-selection strategy in Chapter @Clues. Using this strategy yields guidance that is consistent with some common advice but at odds with other advice. The main principles that emerge from the analysis can be summarized as:\n\n* sample from $X$ and $Y$ values in proportion to their occurrence in the population,\n* go where the probative value is, and\n* invest in collections of cases that provide complementary learning. \n\nBeyond these general principles, other patterns are more complex and thus more difficult to neatly summarize. The most general message of this chapter is about the general approach: that is, that we can use a causal model to tell us what kinds of cases are likely to yield the greatest learning, given the model and a strategy of inference. We provide a tool for researchers to undertake this analysis, at least for simple problems with $X, Y, K$ data.\n\nFor the general intuition, recall that the probative value of a process-tracing test hinges on the difference in clue likelihoods associated with the alternative hypotheses in play for a given case. Recall that for different values of $X$ and $Y$ cell, we want to use process tracing to help us distinguish between two specific types that are consistent with the $X, Y$ pattern. Which types are in question varies across $X,Y$ combinations. Table \\ref{FP} illustrates.\n\n\\begin{table}[h!]\n\\centering\n\\begin{tabular}{c|cc}\n           &        $Y=0$ &        $Y=1$ \\\\ \\hline\n       $X=0$ &     $b$ or $c$ &     $a$ or $d$ \\\\\n       $X=1$ &     $a$ or $c$ &     $b$ or $d$ \\\\\n\\end{tabular}  \n\\caption{The ambiguity about types in each $X, Y$ cell.}\n\\label{FP} \n\\end{table}\n\nThus, in the $X=0, Y-0$ cell, what would be most useful is a clue that has high probative value in distinguishing between an untreated ($X=0$) $b$ type and and an untreated $c$ type. For a case in the $X=1, Y=0$ cell, on the other hand, what matters is how well the clue can discriminate between treated ($X=1$) $a$ and $d$ types. In our notation, it is the difference in $phi_{jx}$ values for that indicates these cell-specific degrees of leverage. \n\nTo illustrate, consider a situation in which for a given clue we have  $\\phi_{b1}$=0.5^[The probability of observing the clue for a $b$ type (positive causal effect) case with $X=1$.]; $\\phi_{d1}$=0.5 ^[The probability of observing the clue for a $d$ type (zero causal effect, $Y$ fixed at 1) case with $X=1$.]; $\\phi_{b0}$=0.5^^[The probability of observing the clue for a $b$ type (positive causal effect) case with $X=0$.]; and $\\phi_{c0}$=0.1^[The probability of observing the clue for a $c$ type (zero causal effect, $Y$ fixed at 0) case with $X=0$.]. In this situation, searching for the clue in $X=Y=1$ cases will yield no leverage since the clue does not discriminate between the two types ($b$ and $d$) that need to be distinguished given $X=Y=1$. Here there is no additional learning about $\\lambda_b$ that can be gained from looking for the clue. In contrast, $X=0, Y=0$ cases will be informative since the clue is much better at distinguishing between $b$ and $c$ types---the two types in contention for this kind of case. Thus, although process tracing here does not provide information on the prevalence of positive causal effects ($b$ types) for an $X=Y=1$ case, it does provide information when $X=Y=0$. \n\nWhile it is common practice for mixed-method researchers to perform their process tracing \"on the regression line,\" the BIQQ framework suggests that the gains to process tracing for different $X$ and $Y$ values in fact depend on the particular constellations of $\\phi$ values for the potentially available clues. More generally, the framework allows one to assess the expected gains from any given case-selection strategy *ex ante* once priors have been specified. \n\n## Explorations\nMost closely related to our analysis in this chapter is the contribution of @HerronQuinn, who build on @SeawrightGerring2008. While Seawright and Gerring provide a taxonomy of approaches to case selection, they do not provide a strategy for assessing the relative merits of these different approaches.  As we do, @HerronQuinn focus on a situation with binary $X,Y$ data and assess the gains from learning about causal type in a set of cases (interestingly in their treatment causal type, $Z_i$  is called a confounder rather than being an estimand of direct interest; in our setup, confounding as normally understood arises because of different probabilities of different causal types of being assigned to \"treatment\", or an $X=1$ value). @HerronQuinn assume that in any given case selected for analysis a qualitative researcher is able to infer the causal type perfectly.  \n\nOur setup differs from that in @HerronQuinn in a few ways.  @HerronQuinn paramaterize differently, though this difference is not important.^[@HerronQuinn have a parameter $\\theta$ that governs the distribution of data over $X$ and $Y$ and then, conditional on $X,Y$ values, a set of parameters $\\psi_{xy}$ that describe the probability of a case's being of a given causal type. We take both $\\theta$ and $\\psi_{xy}$ to derive from the fundamental distribution of causal types and assignment probabilities. Thus, for example, $\\psi_{00}$ from @HerronQuinn corresponds to $\\frac{(1-\\pi_b)\\lambda_b}{(1-\\pi_b)\\lambda_b + (1-\\pi_c)\\lambda_c}$ in our notation. The difference in  paramaterization does have implications for interpretations of the priors. For example flat priors over $\\theta$ and $\\psi$ implies a tighter distribution that a uniform prior over the causal types. In fact @HerronQuinn use priors with greater variance than uniform in any event.] Perhaps the most important difference between our analysis and that in @HerronQuinn  is that we connect the inference strategy to process-tracing approaches. Whereas @HerronQuinn assume that causal types can be read directly, we assume that these are inferred *imperfectly* from clues. As in our baseline model, our ability to make inferences for causal types can differ by type and as a function of $X$. And, as in the baseline model, not only can we have uncertainty about the probative value of clues, but researchers can learn about the probative value of clues by examining cases.\n\n<!-- Are Herron and Quinn's priors Jeffrey priors? -->\n\nHere we assume that the case selection decision is made after observing the $XY$ distribution and we explore a range of different possible contingency tables. In  @HerronQuinn the distribution from which the contingency tables are drawn is fixed, though set to  exhibit an expected  observed difference in means (though not necessarily a true treatment effect) of 0.2. They assume large $XY$ data sets (with 10,000) units and case selection strategies ranging from 1 to 20 cases.\n\nAnother important difference, is that in many of their analyses, @HerronQuinn take the perspective of an outside analyst who knows the true treatment effect; they then assess the expected bias generated by a research strategy over the possible data realizations. We, instead, take the perspective of a researcher who has *beliefs* about the true treatment effect that correspond to their priors, and for whom there is therefore no *expected* bias. This has consequences also for the assessment of expected posterior variance, as in our analyses the expectation of the variance is taken with respect to the researcher's beliefs about the world, rather than being made conditional on some specific world (ATE). We think that this setup is addressed to the question that a researcher must answer when deciding on a strategy: given what they know now, what will produce the greatest reduction in uncertainty (the lowest expected posterior variance)?\n\nFinally, we proceed somewhat differently in our identification of strategies from Herron and Quinn: rather than pre-specifying particular sets of strategies (operationalizations of those identified by @SeawrightGerring2008) and evaluating them, we define a strategy as the particular distribution over $XY$ cells to be examined and proceed to examine *every possible strategy* given a choice of a certain number of cases in which to conduct process tracing. We thus let the clusters of strategies---those strategies that perform similarly---emerge from the analysis rather than being privileged by past conceptualizations of case-selection strategies.\n\nDespite these various differences, our results will agree in key ways with those in @HerronQuinn.\n\n\n### Procedure\nTo illustrate the procedure, consider a researcher who observes six data points.  Two cases lie in each of the on-diagonal cells of an $X,Y$ table, those where $X=Y$, and one case lies in each of the off-diagonal cells (where $X \\neq Y$). Suppose that the researcher is considering gathering process-tracing clues on 3 cases. She has many different strategies she might pursue and each of these can give rise to different possible data and thus to different possible conclusions. Let's say that she starts with no knowledge about (flat priors over) the distribution of causal types in the population, assumes similar assignment propensities for all types (no confounding), and believes that the clues she is going to look for are all doubly decisive (either seeing the clue or not seeing the clue, when she looks for it, will fully identify the case's type).  \n\nSuppose, now, that she is choosing between two strategies:\n\nA. Select two $X=Y=1$ cases and one $X=Y=0$ case for investigation. She is thus choosing here cases \"on the regression line\" generated by the $X,Y$ data pattern. \n\nB. Take one case from each of the diagonal cells and one from the off-diagonal $X=1, Y=0$ cell\n\nFrom which strategy should she expect to learn more about the average, population-level causal effect of $X$ on $Y$, given her prior beliefs? We answer the question by considering each data pattern that she might see given a strategy and then calculating (a) the probability with which she expects to see such an outcome, given her priors (or more accurately, given her posterior after observing the $XY$ pattern only), and (b) the uncertainty (posterior variance) that she would have upon seeing that pattern. We then calculate the expected variance of the strategy by considering all possible data patterns that could emerge from that strategy. We ignore the fact that in principle the subjective probability of observing one pattern is correlated with the subjective probability of observing another pattern.^[For example, a researcher may be uncertain regarding $\\phi_b$; if it is high then the probability of observing any profile of outcomes with many clues observed is higher than if $\\phi_b$ is low; this introduces a correlation between outcomes that have similar clue observations. However only one of these patterns will be observed in fact. NOTE ON RISK-NEUTRALITY HERE?]\n\nTables 11.1 and 11.2 present the results of our comparison of the researcher's two strategies. Table 11.1 examines strategy A, while Table 11.2 examines strategy B. In each table, the lefthand column lists all possible realizations of clue data that might be found under the strategy in question. In the notation that we use here, each place in the four-digit sequence refers to one cell in the 2-by-2 table implied by a binary $X$ and binary $Y$ variable. The ordering is: \n\n* $X=Y=0$\n* $X=0, Y=1$\n* $X=1, Y=0$\n* $X=Y=1$ \n\nAs an easy mnemonic, the outside digits are \"on-the-diagonal\" (on the regression line consistent with a positive causal effect); the inside digits are off-the-diagonal. Thus, for instance, the sequence $1001$ means that we have seen the clue in two on-the-diagonal cases: an $X=Y=0$ case and a $X=Y=1$ case. The sequence 0200 means that we have seen the clue in two cases in the $X=0, Y=1$ cell.\n\nIn each row, we see information about one possible clue pattern that we could potentially observe under the strategy. The second column indicates the probability that this clue pattern will arise, given the researcher's priors. The final column then indicates the uncertainty---the posterior variance---that we would be left with given the information provided by that clue pattern. In the \"All\" row, we have averaged across the posterior variances for all clue patterns, weighting each clue pattern by its probability of occurring. \n\n```{r, include  = FALSE}\nout1 <- losses(k=3, XY_base = c(2,1,1,2),  strategies =c(\"1-0-0-2\"))\n\nout2 <- losses(k=3, XY_base = c(2,1,1,2),  strategies =c(\"1-1-0-1\"))\n```\n\n```{r check17, include  = FALSE}\nlibrary(biqq)\nlibrary(rstan)\n\nresult <- Expected_Var(k = 3,\n             XY_base = c(2,1,1,2),\n             strategies = c(\"1-0-1-1\", \"2-0-0-1\"),\n             q0_alpha = c(1000,1000,1,1),\n             q0_beta  = c(1,1,1000,1000),  #  b and c distinct \n             q1_alpha = c(1000,1000,1,1),\n             q1_beta  = c(1,1,1000,1000),\n             iter = 200, chains = 2, refresh = 1000)\n\n```\n\n```{r check18, include  = FALSE}\ns1 <- cbind(result$probability_each_pattern[[1]], result$loss_for_each_pattern[[1]]) \ncolnames(s1) <- c(\"Probability of seeing...\", \"Posterior variance\")\ns1 <- rbind(s1, All = c(1, sum(s1[,1]*s1[,2])))\nkable(round(s1,3), caption = \"Inferences from  strategy A: 'on the regression line.' Each row shows a possible clue pattern that one might see from this strategy, where, for example 1000 means that a clue is indeed observed for an X=0, Y=0 case.\")\n\ns2 <- cbind(result$probability_each_pattern[[2]], result$loss_for_each_pattern[[2]]) \ncolnames(s2) <- c(\"Probability...\", \"Posterior variance\")\ns2 <- rbind(s2, All = c(1, sum(s2[,1]*s2[,2])))\nkable(round(s2,3), caption = \"Inferences from  strategy A. Each row shows possible clue patterns one might see from a given strategy, where, for example 1000 means that a clue is indeed observed for an X=0, Y=0 case.\")\n\n```\n\n```{r, echo = FALSE}\nkable(round(s1,3), caption = \"Inferences from  strategy A: one cell from one on-the-regression-line cell; two from the other. Each row shows possible clue patterns one might see from a given strategy, where, for example 1000 means that a clue is indeed observed for an X=0, Y=0 case.\")\n\ns2 <- rbind(s2, All = c(1, sum(s2[,1]*s2[,2])))\nkable(round(s2,3), caption = \"Inferences from  strategy B: one case from each of the diagonal cells and one from the off-diagonal $X=1, Y=0$ cell. Each row shows possible clue patterns one might see from a given strategy, where, for example 1000 means that a clue is indeed observed for an X=0, Y=0 case.\")\n```\n\n\nIn this example, we see that the researcher would expect to be better off---in the sense of having less posterior uncertainty---by focusing her process-tracing efforts where a greater share of the population of cases lies: on the regression line. Taking one observation in each of three cells has her devoting much of her effort to a case that is relatively unrepresentative of the population she wishes to learn about. \n\nIn the experiments that follow, we implement this kind of simulation for all possible clue strategies---for a fixed number of clues sought---and report the expected posterior variance.\n\n## Experiments\n\nIn all of the graphs, we start with 16 \"quantitative\" cases: cases for which we have observed an $X$ and a $Y$ value. We are then choosing some subset of these cases for process tracing. Within each group of nine graphs, we are sampling a fixed number of cases for process tracing: 1 case in the first set, 2 cases in the second set, 3 cases in the third set, and 4 cases in the final set. We treat process tracing in a case as the search for one clue in that case, though this \"one clue\" could be conceived of as a collection of clues that jointly have a given probative value. \n\nWithin each set of graphs, we see how different case selection strategies fare as we vary two features of the research situation. Moving down the rows of graphs, we vary the distribution of the 16 cases over an $XY$ table. In the first row, the 16 cases are spread evenly across the 4 $X,Y$ cells; in the second row, $X$ and $Y$ are positively correlated; in the third row, $Y=1$ is observed only in cases with $X=1$ (i.e., the $X=0, Y=1$ cell is empty). \n\nMoving across the columns of graphs, we vary the probative value of the clue that we are looking for in the process tracing. What is changing is for what kind of a case---in terms of its $X,Y$ values---the clue is most probative (i.e., doubly decisive). Where the clue is doubly decisive, finding the clue present or finding it absent both nail down the type of the case. Wherever the clue is not doubly decisive, it is assumed to be a \"hoop test\" for an $a$ type (in the $X=1, Y=0$ and $X=0, Y=1$ cells) and for a $b$ type (in the $X=0, Y=1$ cells). This means that *not* finding the clue rules out the case's being an $a$ or a $b$ type in the relevant cells; thus, the clue is still informative, but less so than if it were doubly decisive. In the first column, the clue is doubly decisive for all kinds of cases. In the second column, the clue is doubly decisive only for $X=1$ cases, and a hoop test otherwise. In the third column, the clue is doubly decisive only for $Y=1$ cases, and a hoop test otherwise. And in the final column, the clue is doubly decisive only \"on the regression line\" consistent with a positive effect, and a hoop test otherwise.\n\nEach case selection strategy is indicated on each graph using the same four-digit pattern that we used to indicate data realizations in the example above. Throughout, what we seek to estimate is the average causal effect of $X$ on $Y$ (or $\\lambda_b-\\lambda_a$). The strategy's vertical placement on the graph indicates the \"loss\", i.e., expected posterior uncertainty, associated with the strategy. Thus, a lower placement indicates greater learning. Around each dot, we also provide 90 percent simulation error bars, though these are hidden by the dots themselves when the simulation error is very small, as it is in most cases.\n\nIn all simulations, we assume that there is no confounding, and we start (before seeing the $X, Y$ pattern) with flat priors over the distribution of causal types in the population.\n\nIn the graphs with more than one process tracing case, we also color-code and group together families of strategies as indicated in the figure caption.\n\nWe begin with the simplest problem, where only one case is to be chosen for process tracing. In this situation, as seen in our first set of figures, two principle emerge. First, it is better select cases in the $X,Y$ conditions where the probative value lies. More informative clues generate more learning; so if probative value varies across types of cases, this should have direct implications for case selection. Second, it is better to select cases from the largest cells. The second principle is perhaps less obvious, but it derives from a sampling logic: learning about a case drawn randomly from a cell gives you information about that cell and so the larger the cell the more cases there is learning about. \n\nPerhaps just as important is what does not emerge as a principle: all else equal there is no reason to focus on either the $X=Y=1$ cases or on the diagonal ($X=Y$): all four cells are symmetric (ceteris paribus) in that they all exhibit an ambiguity between $a$'s and $b$'s on the one hand and $c$'s and $d$'s on the other. There is thus nothing intrinsically informative about the cases on the diagonal. We do see that choosing on the diagonal is beneficial when $X$ is positively correlated with $Y$, but this is because the  population of cases is concentrated along the diagonal; this is an illustration of the representativeness principle, not of some special feature of the diagonal.\n\n![Gains from different strategies involving process tracing in 1 case. We treat process tracing in a case as the search for one clue in that case, though this \"clue\" could be conceived of as a collection of clues that jointly have the probative value indicated. In each simulation, we start with 16 \"quantitative\" cases. Moving down the rows of graphs, we vary the distribution of these cases over an $XY$ table. Moving across the columns of graphs, we vary the probative value of clue sought via process tracing. ](Figures/K1.pdf)\n\nMore interesting patterns start emerging once we can choose two cases. We see the same two basic principles matter---go for probative value and for representativeness---but we now also see that there are complementarities in learning between different types of cases. In fact, for symmetric problems---as in the upper left panel, where the cases are evenly spread out and probative value is strong everywhere---we see a ranking between four families: first cases that fix  $Y$ (at 0 or 1) and spread on $X$, second cases that fix $X$ and spread on $Y$, third cases that are on or off the diagonal, and fourth cases that focus on a single cell. Seeking clues on the diagonal does emerge as a good strategy (see for example the left figure in the middle row), but this appears to arise because the diagonal is data dense, not because there are particular complementarities of probative value there. In the bottom left figure for example, we see that in the case where there is data for $X=1, Y=0$ but little data for $X=0, Y=1$ , selecting cases distributed over the $Y=0$ cells is about as informative as selecting on the diagonal. Selecting off the diagonal is one of the worst strategies here and is dominated by selecting all data from a dense cell. \n\nSkip now to our graph of results where 4 cases are selected. As in previous figures we see strong gains for strategies that select cases proportionate to the size off cells. This is mot clear in the top left figure where one case per (equally sized) cell is the best strategy. More subtly it can be seen in the bottom left where the 2-0-1-1 strategy dominates --- this is a strategy that spreads roughly proportionately even if that means leaving some cells unrepresented. Strikingly in the base case various hybrid strategies do quite well, likely reflecting the fact that are as close to optimal spreads as possible. On and off diagonal strategies do poorly unless there is a strong diagonal, in which case these can dominate spreading across cells. \n\nOverall relatively simple patterns emerge though these differ in some ways from text book suggestions. First focusing on probative value is key. Second seeking larger cells and balancing cases across cells appears fruitful. Third, and less intuitively, some combinations appear to gain more leverage than others. On and off diagonal strategies for example seem weaker in general than strategies that fix $X$ or that fix $Y$. Strategies that fix $Y$ and allow variation on $X$ seem strong, again, ceteris paribus.\n\nPerhaps the most striking result from the simulations is that the optimal choice depends on many features.  A simple rule, or even these core principles, may not get identify the right strategy. Ye the right strategy can be calculated, at least if one is willing to lay out beliefs on causal structure and probative value.\n\n![Gains from different strategies involving process tracing in 2 cases. We treat process tracing in a case as the search for one clue in that case, though this \"clue\" could be conceived of as a collection of clues that jointly have the probative value indicated. In each simulation, we start with 16 \"quantitative\" cases. Moving down the rows of graphs, we vary the distribution of these cases over an $XY$ table. Moving across the columns of graphs, we vary the probative value of clue sought via process tracing. Families of strategies are grouped and color-coded as follows: red=maximally dispersing across cells; yellow=](Figures/K2.pdf)\n\n\n![Gains from different strategies involving process tracing in 3 cases. We treat process tracing in a case as the search for one clue in that case, though this \"clue\" could be conceived of as a collection of clues that jointly have the probative value indicated. In each simulation, we start with 16 \"quantitative\" cases. Moving down the rows of graphs, we vary the distribution of these cases over an $XY$ table. Moving across the columns of graphs, we vary the probative value of clue sought via process tracing. ](Figures/K3.pdf)\n\n\n\n![Gains from different strategies involving process tracing in 4 cases. We treat process tracing in a case as the search for one clue in that case, though this \"clue\" could be conceived of as a collection of clues that jointly have the probative value indicated. Simulations invole 16 units distributed over an $XY$ table in three patterns (rows) and variation over the probative value of different clues (columns).](Figures/K4.pdf)\n\n\n## Chapter Appendix: Accounting for case selection\n### Independent case selection strategy\nWe have focused on cases in which  the researcher examines a fixed number of cases for clue information. An alternative strategy that produces a simpler likelihood is one in which each case is selected for within-case data gathering with some independent probability. The likelihood below introduces a case selection probability $\\kappa_{xy}$ that covers this case and allows for the possibility that selection probabilities are different for different $X,Y$ combinations.\n\n Thus we assume that $X$, $Y$ data is observed for all cases under study, but that $K$ data may be sought for only a subset of these (we use the wildcard symbol ``$*$'' to denote that the value of the clue is unknown). We let $n_{xyk}$ denote the number of cases of each type. Then, again assuming data is independently and identically distributed, the likelihood is:\n\n$$\\Pr(\\mathcal{D}|\\theta)= {\\text{Multinomial}}(n_{000}, n_{001},n_{00*},n_{010}, n_{010},n_{01*}, n_{100}, n_{101},n_{10*},n_{110},n_{111} ,n_{11*} |\nw_{000}, w_{001},w_{00*},w_{010}, w_{010},w_{01*}, w_{100}, w_{101},w_{10*},w_{110},w_{111} ,w_{11*})$$\n\n\nwhere the event probabilities are now given by:\n\n\n$${\\left( \\begin{array}{c}\nw_{000} \\\\ w_{001} \\\\  \\vdots \\\\ w_{11*} \\end{array} \\right)=\n\\left( \\begin{array}{c}\n\\lambda_b(1-\\pi_b)\\kappa_{00}(1-\\phi_{b0}) + \\lambda_c(1-\\pi_c)\\kappa_{00}(1-\\phi_{c0})\\\\\n\\lambda_b(1-\\pi_b)\\kappa_{00}\\phi_{b0} + \\lambda_c(1-\\pi_c)\\kappa_{00}\\phi_{c0}\\\\\n\\vdots \\\\\n\\lambda_b\\pi_{b}(1-\\kappa_{11}) + \\lambda_d\\pi_{d}(1-\\kappa_{11})\n\\end{array} \\right)}$$\n\nNote we use a Greek symbol for the case selection probabilities to highlight that these may also be unknown and be an object of inquiry, entering into the vector of parameters, $\\theta$.\n\\subsubsection{Non-random $XY$ Sample Selection}\\label{nonrandomcase}\nWhile we have assumed in the canonical model that $X,Y$ cases are selected at random, this need not be the case. Say instead that each case of type $j$ is selected into the study with probability $\\rho_j$. In that case, assuming independent selection of cases for qualitative analysis, the likelihood function is now:\n\n\n$$\\Pr(\\mathcal{D}|\\theta) = {\\text{Multinomial}}(n, w)$$\nwhere: $$n = (n_{000}, n_{001},n_{00*},n_{010}, n_{010},n_{01*}, n_{100}, n_{101},n_{10*},n_{110},n_{111} ,n_{11*})$$\n\nand the event probabilities, $w$, are now, given by:\n\n$$\\left( \\begin{array}{c}\nw_{000} \\\\ w_{001} \\\\  \\vdots \\\\ w_{11*}\n\\end{array} \\right)=\n\\left( \\begin{array}{c}\n\\frac{\\rho_b \\lambda_b}{\\rho_a \\lambda_a+\\rho_b \\lambda_b+\\rho_c \\lambda_c+\\rho_d \\lambda_d}(1-\\pi_b)\\kappa_{00}(1-\\phi_{b0}) +\n\\frac{\\rho_c \\lambda_c}{\\rho_a \\lambda_a+\\rho_b \\lambda_b+\\rho_c \\lambda_c+\\rho_d \\lambda_d}(1-\\pi_c)\\kappa_{00}(1-\\phi_{c0})\\\\\n\\frac{\\rho_b \\lambda_b}{\\rho_a \\lambda_a+\\rho_b \\lambda_b+\\rho_c \\lambda_c+\\rho_d \\lambda_d}(1-\\pi_b)\\kappa_{00}\\phi_{b0}+\n\\frac{\\rho_c \\lambda_c}{\\rho_a \\lambda_a+\\rho_b \\lambda_b+\\rho_c \\lambda_c+\\rho_d \\lambda_d}(1-\\pi_c)\\kappa_{00}\\phi_{c0}\\\\\n\\vdots \\\\\n\\frac{\\rho_b \\lambda_b}{\\rho_a \\lambda_a+\\rho_b \\lambda_b+\\rho_c \\lambda_c+\\rho_d \\lambda_d}\\pi_{b}(1-\\kappa_{11})+\n\\frac{\\rho_d \\lambda_d}{\\rho_a \\lambda_a+\\rho_b \\lambda_b+\\rho_c \\lambda_c+\\rho_d \\lambda_{11}}\\pi_{d}(1-\\kappa_{11})\n\\end{array} \\right)$$\n\nNote we have used a Greek symbol for the selection probabilities to highlight that these probabilities may be unknown and could enter into the set of parameters of interest, $\\theta$.\n\n### Conditional random case selection\n\nFinally consider the likelihood for a design in which a researcher selects to search for clues as a function of the $X,Y$ data. This is a somewhat harder case because the size of each $X,Y$ group is stochastic. Let $n_{xy} = n_{xy0}+n_{xy1}+n_{xy*}$ denote the number of cases with particular values on $X$ and $Y$, and let $n_{XY}=(n_{00},n_{01},n_{10},n_{11})$ denote the collection of $n_{xy}$ values.\n\nSay now that conditional on the $X,Y$ observations, a researcher sets a target of $k_{xy}(n_{XY})$ cases for clue examination (note here that the number of clues sought for a particular $X,Y$ combination can be allowed to depend on what is observed across all $X$, $Y$ combinations). Then the likelihood is:\n$$\\text{Multinomial}(n_{XY}|w_{XY})\\prod_{x\\in\\{0,1\\},y \\in\\{0,1\\}}\\text{Binom}(n_{xy1}|k_{xy}(n_{xy}), \\psi_{xy1})$$\n\nThe multinomial part of this expression gives the probability of observing the particular $X,Y$ combinations; the event probabilities for these depend on $\\lambda$ and $\\pi$ only | for example  $w_{11} = \\lambda_b \\pi_b+\\lambda_d \\pi_d$. The subsequent binomials give the probability of observing the clue patterns conditional on searching for a given number of clues ($k_{xy}(n_{xy})$) and given an event probability $\\psi_{xy1}$ for seeing a clue given that the clue is sought for an $x,y$ combination; thus for example:\n$$ \\psi_{111} = \\frac{\\lambda_b \\pi_b}{\\lambda_b \\pi_b+\\lambda_d \\pi_d} \\phi_{b1} + \\frac{\\lambda_d \\pi_d}{\\lambda_b \\pi_b+\\lambda_d \\pi_d} \\phi_{d1}$$\n",
    "created" : 1493153916355.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3740404314",
    "id" : "D48BEB3",
    "lastKnownWriteTime" : 1493155034,
    "last_content_update" : 1493155035005,
    "path" : "C:/Dropbox/ProcessTracing/8 Book/ii/10-caseselection.Rmd",
    "project_path" : "10-caseselection.Rmd",
    "properties" : {
        "ignored_words" : "unstratified,probative,Seawright,Gerring,confounder,estimand,paramaterize,paramaterization,ceteris,paribus\n"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}
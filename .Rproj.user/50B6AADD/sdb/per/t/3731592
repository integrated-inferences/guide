{
    "collab_server" : "",
    "contents" : "# Design: Going wide and going deep {#wide}\n\n**\\color{blue}Key points: Researchers often need to choose between collecting data on more cases or collecting more data within cases. We discuss the tradeoffs and communicate an intuition that clue data, even on a small number of cases, can be informative even when there is $X, Y$ data on a very large number of cases, but only if it provides information that cannot be gathered from $X,Y$ data, such as selection into treatment. Simulations suggest that going deep is especially valuable for observational research, situations with homogeneous treatment effects, and, of course, when there is strong probative value.**\n\n\nHow does this approach guide researchers in making choices about research designs?\n\nWe address this question with a focus  on characterizing the kind of learning that emerges from different combinations of investment in the collection of correlational as compared with process-tracing data \\textit{under different research conditions}. We report the results here of simulation-based experiments designed to tell us under what research conditions different mixes of methods can be expected to yield more accurate inferences. We also discuss, at a high level, the implications of the framework for strategies of qualitative case-selection.  \n\n\n## Intuitions: Does a sufficiently large $N$ always trump $K$?\n\nWe begin by considering the learning that occurs upon observing outcomes from varying numbers of cases given different $XY$ data ranging from small to quite large. \n\nThe goal here is to build up intuitions on how beliefs change given different observations and hw this affects posterior variance. We address the question is a very controlled setting in which \n\n* a researcher is contronted with balanced $X,Y$ data that exhibits no correlation\n* the  researcher can seek a doubly decisive clue on cases in the $X=Y=1$ cell \n* though not known in advance, it turns out that each time the researcher finds evidence suggesting that the case in question is a $b$ type\n* the selection probabilities are either unknown or known with near certainty\n\nIn this case, we can expect that seeing evidence of $b$ types will shift the researcher to increase her beliefs on the average causal effect. But how strong will these shifts be and how does this depend on the amount of XY data available? Does the signal from the $XY$ data drown out any signal from the  $K$ data?\n\nIntuitions to answer these questions can be gathered from the simulations reported in Figure \\ref{morn}. For these simulations we varied the size of the $XY$ data from 5  observations in each cell to 5000. The key features of the simulations are:\n\n1. When assignment propensities are unknown---as for example with observational data---the clue information shifts beliefs independent of how many $XY$ cases there are. The key insight is that the clue information provides inforation on assignment propensities which are informative about the share of each type in each cell and these shares determine treatment effects no matter how large or small the cells are.\n\n2. When assignment propensities are known with large data there is a lot of learning over the distribution of types in a population (at least up to differences in types rather than the distribution of fundamental types). Clue information shifts beliefs about the types of the particular cases for which clue data is gathered but has almost no effect on estimates of the population estimand. \n\n3. Not visible from the figure however: in the case with large $N$ and known propensities, observation on many $b$ types in the $X=Y=1$ cell, while not changing estimates of *average* treatment effects ($\\lambda_b- \\lambda_s$) does affect beliefs on *heterogneity*, because the data is more consistent with a world with many $a$s and $b$s than one with many $c$s and $d$s.  For example if there were 10,000 data points in each $X,Y$ and clue information on  20 cases in the $X=Y=1$ cell suggest that these are all $b$ types, then the conclusion would be that 95% of the cases are $a$ and $b$ types, in equal proportion.   \n\n```{r largeNhet, cache = TRUE, eval = FALSE}\nlargeNhet <- biqq(XY=c(rep(10000,3), 9980), XYK = c(rep(0, 7), 20), \n     pi_alpha = c(1, 1, 1, 1)*1000,\n     pi_beta = c(1, 1, 1, 1)*1000,\n     doubly_decisive = TRUE, iter = 5000)\n\n\nlargeNhet_confused <- biqq(XY=c(9980, rep(10000,2), 9980), XYK = c(20, rep(0, 6), 20), \n     pi_alpha = c(1, 1, 1, 1)*1000,\n     pi_beta = c(1, 1, 1, 1)*1000,\n     doubly_decisive = TRUE, iter = 5000)\n```\n\n\n\\begin{figure}[h!]\n\\centering\n\\includegraphics[width=\\textwidth]{Figures/Ns.pdf}\n\\caption{{Figure shows posterior estimates of the average treatment effect and posterior variances given a situation with 5, 10, 50, or 5000 observations in each cell of an $X,Y$ table and given collection of data on $0 - 5$ cases in the $X=Y=1$ cell, all of which provide evidence from a doubly decisive clue in favor of causal effects.}}\n\\label{morn}\n\\end{figure}\n\n\n## Evaluating strategies\nAs a metric of the returns from different research strategies we calculate the \\textit{expected} inaccuracy in the estimation of the average treatment effect, as given in equation \\ref{Loss}.\n\\begin{equation}\n\\mathcal{L}=\\mathbb{E}_\\theta(\\mathbb{E}_{\\mathcal{D}|\\theta}(\\tau(\\theta)-\\hat{\\tau}(\\mathcal{D}))^2) \n\\label{Loss}\n\\end{equation}\n\n\nwhere $\\tau(\\theta)$ is the value of $\\lambda_b-\\lambda_a$ (the average treatment effect) given $\\theta$, and $\\hat{\\tau}(\\mathcal{D})$  is the \\textit{estimate} of this treatment effect (the mean posterior value) that is generated following some realization of data $\\mathcal{D}$. Thus, if some $\\theta$ characterized the true state of the world, then $\\mathbb{E}_{\\mathcal{D}|\\theta}(\\tau^\\theta-\\hat{\\tau})^2$ is the expected error in estimation of the causal effect given different realizations of the data, $\\mathcal{D}$,  that could obtain in this state of the world.  $\\mathcal{L}$ is then the expected value of these errors given prior beliefs over possible values of $\\theta$.\n\nNote that, while we focus on errors on estimated average causal effects, similar exercises could assess how cross- and within-case observations distinctively contribute to other estimands | including the causal explanations for individual cases and the validity of causal theories | as well as to learning about inferential assumptions themselves (assignment and clue probabilities). \nFor all simulations, prior distributions are drawn with parameters as described in the Supplementary Materials (\\S \\ref{AppSimNotes}, Table \\ref{sims}). Priors on the type distribution are drawn from a Dirichlet distribution; priors for each of the $\\pi$ and $\\phi$ values are drawn independently from Beta distributions. We note that, while by construction priors on each parameter are independent, this will not generally be the case for posterior distributions. In most cases we simulate the prior distribution using 5200 draws of each parameter. For most experiments we then systematically vary the prior distribution for one parameter of the research situation between two extreme positions. We then calculate the expected posterior from each possible data realization and, in turn, the expected loss in estimates of treatment effects for a range of levels of investment in qualitative and quantitative evidence. \n\n\nA few further features of the experiments below are worth noting. First, our illustrations focus on learning about population-level causal effects; however, the model can yield results about the benefits of alternative research designs for estimating a wide range of other quantities of interest, such as case-specific causal explanations or clue probabilities. Second, while we focus on the search for a \\textit{single} clue in each case, the analysis can be extended to the case of an arbitrarily large set of clues. Third, in many of these experiments, the probative values are set at doubly decisive levels for all $\\phi$ parameters, and thus focus on the very optimistic case of maximally informative process tracing. Fourth, we illustrate tradeoffs at low levels of $n$, but the model can be employed to make choices for arbitrarily large numbers of cases. Finally, we note that some results may be sensitive to the choice of priors. The results below should thus be understood as an illustration of the utility of the BIQQ framework for guiding research choices, rather than as a set of more general prescriptive design rules.\n\n\n\n## Varieties of mixing {#varieties}\nWhat are the marginal gains from additional pieces of correlational and process-tracing evidence for the  accuracy of causal estimates? Figure \\ref{morn} displays the results,  plotting the errors associated with different mixes of correlational and process data. Each dot represents a single possible research design, with the $x$-axis charting the total the number of cases examined. For all cases, $X$ and $Y$ data are collected. The shading of the dots in each column then represents the proportion of cases for which process-tracing is also carried out. An unshaded dot is a design in which \\textit{only} correlational data has been collected for all cases; a black dot is a design in which the process-tracing clue is sought in \\textit{all} cases; and shades of grey, as they darken, indicate process tracing for increasing shares of cases. For $n\\leq 4$ we report results for all designs; for $n>4$ we report only results when within case information is sought for all, half, or none of the cases. \n\nWe see first from the graph that, as one would expect, moving from lower-$n$ to higher-$n$ designs reduces the expected error of estimates. Further, both adding a correlational case and doing process tracing on an additional case improve accuracy. The figure also suggests that there are diminishing marginal returns to both types of data: in particular the grey point reflecting 50\\% process tracing is generally well below the mid point of the white and black dots, and converges toward the black dot (100\\% process tracing) as sample size increases. Other, less obvious results also emerge, including:\n\n\\begin{itemize}\n\\item \\textbf{Qualitative and quantitative data can act as partial {substitutes} for assessing causal effects}. We see, in the smaller sample sizes, that the marginal gains from adding an extra case are lower when there is more within-case information on existing cases. Similarly, the marginal gains from gathering more within-case information are lower when there are more correlational cases (for example adding one case study when $n=1$ has about the same effect as adding 8 cases studies when $n=16$). \n\\item \\textbf{The \\textit{relative} marginal gains from going wider and going deeper vary with the study design}. Suppose that the costs of gathering $X,Y$ data and gathering clue data were the same per case. Suppose further that we have an $n$ of 2 and only correlational data. Then, for the case illustrated in Figure \\ref{morn}, if we have additional resources to invest, it is better to gather $K$-type data for 1 of these cases than to add another case with $X,Y$ data only. However, at this point, the tradeoff shifts. If we now have a choice of gathering $K$-type data on the second case (moving to the black dot) or adding a third case (resulting in $n=4,m=1$), the latter strategy now results in an error rate at least as low as the former. %\n% Sometimes it is better to go deeper than wider. For example an $n$=3 (or $n$=2) study with process tracing on one case produces greater accuracy than a purely correlational $n=4$ (or $n=3$) study. \n\\item \\textbf{Optimal strategies might involve going deep in a subsample of cases only}. Suppose again that the costs for gathering $X,Y$ data and gathering $K$-type data were the same and, now, that researchers can gather four pieces of data of any type. The results in Figure \\ref{morn} suggest that, for the priors chosen here, gathering $X,Y$ data on 3 cases and $K$-type data on one produces more accurate estimates than either going maximally wide (gathering $X,Y$ data on four cases) and at least as accurate an estimate as going maximally deep (gathering $X,Y,K$ data on 2 cases).\n %At the margins and once one has done some process tracing, the results suggest that it is better to invest in additional $X,Y$ data \\textit{without} clues than to seek to do process tracing for all cases on which one has $X,Y$ data.\n\n\\end{itemize}\n\n\n\n\n\\begin{figure}[h!]\n\\centering\n\\includegraphics[width=\\textwidth]{Figures/m_or_n.pdf}\n\\caption{{Expected errors in the estimation of average treatment effects for designs in which $X, Y$, data is sought in $n$ studies (horizontal axis) and clue data is sought within $m$ of these. The shading of dots indicates the proportion of cases for which within-case data is sought (white = none; black = all). For small sample sizes ($n \\in \\{1,2,3,4\\}$) we show results for all designs ($m \\in \\{1,2,\\dots, n\\})$. For larger sample sizes, we show only designs with clues sought in 0, half, and all cases.}}\n\\label{morn}\n\\end{figure}\n\n\n\n\\subsection{Designs in Context}\nMore generally, we would expect that the optimal level of mixing depends on the context|on features of the research situation that affect the problem and available tools of inference. In the next subsections, we report results from experiments in which we vary the researcher's priors about (a.) the probative value of clues, (b.)  heterogeneity of treatment effects (c.) uncertainty regarding assignment processes, and (d.) uncertainty regarding the probative value of clues. In all cases we report the expected loss for the design in question, as given in Equation \\ref{Loss}. %\n\n\\subsubsection{Probative value of clues}\nIf clues have no probative value| in the sense that priors over $\\phi_{jx}$ do not depend on type, $j$, then gathering data on clues does not affect inference. Probative value does not get picked up during analysis, it must be imported. Less clear, however, is the extent to which gains in inference depend on the degree of probative value, defined here as in Section \\ref{PTintro} (see footnote \\ref{fnPV}). Our simulation evidence from full BIQQ estimation (Figure \\ref{experiments}) suggests that in some ranges at least the gains are also convex, that is \\textit{increasingly} more is learned as the gaps between pairs such as $\\phi_{b1}$ and $\\phi_{d1}$ increases. The top left panel of Figure \\ref{experiments} shows an example of these convex gains, showing expected losses for settings where there is not probative value, where all tests are doubly decisive, and a case half way between these extremes.        \n\n\\begin{figure}[h!]\n\\centering\n\\includegraphics[width=\\textwidth]{Figures/experiments.pdf}\n\\caption{Figure shows the expected error in mean posterior estimates of average treatment effects for different designs. In these graphs the horizontal axis denotes some feature of the research setting (captured in priors); the white and black circles represent errors from designs in which within case information is sought for no and all cases, respectively; the numbers marked in the circles indicate the number of data points in the study design.}\n\\label{experiments}\n\\end{figure}\n\n\n\\subsubsection{Effect Heterogeneity}\nWe might expect that the optimal research design for estimating average treatment effects would depend on how \\textit{heterogeneous} the true causal effects are in the population. If we believe that effects are strongly homogeneous, then confidence that one case is affected by treatment provides a great deal of information about population treatment effects. However, if effects are believed to be highly heterogeneous, then knowing that one case is affected by treatment provides less information regarding effects on different cases. \n\nHeterogeneity can be conceptualized in different ways. Here we define heterogeneity as increasing in the amount of \\textit{variance} in causal effects across cases in the population. In the binary environment, for any $\\tau \\in [0,1]$, maximum effect heterogeneity is  obtained when $\\lambda_a=(1-\\tau)/2$ and  $\\lambda_b=(1+\\tau)/2$: i.e., when all cases have either a positive or negative treatment effect, with no destined or chronic cases. For a positive treatment effect, maximum homogeneity occurs when $a=0, b=\\tau$, with the remaining share $1-\\tau$ consisting of types $c$ and $d$.\\footnote{For negative treatment effects, homogeneity is maximized with $\\lambda_b=0$.}. There are two very different ways to have an average treatment effect of 0: there may be no treatment effect for any case (maximal homogeneity), or there is a positive effect for half the cases and a negative effect for the other half (maximal heterogeneity).  \n%Here there is no variance in causal effects across cases. \n\nUsing this conceptualization of heterogeneity, our simulation results confirm that higher heterogeneity increases the marginal value of going ``wide'' rather than ``deep.'' At low levels of heterogeneity, there are considerable gains to collecting clues on cases at a given sample size; but the gains to process tracing diminish and then disappear as heterogeneity rises (see Supplementary Materials, \\S\\ref{AppE2}).\n\n%Above, why is homogeneity defined only for a=0? Also, I don't understand why maximal homogeneity isn't a, b, c, or d = 1. How does learning about one case tell you so much more about the population under the definition of homogeneity above?\n% that's homogeneity of type, but not of treatment effect; e.g. all c is no more homogeneous than half a and half b. the second het analysis looks at a case like this though\n\n\n\\subsubsection{Uncertainty Regarding Assignment Processes}\n\nHere we examine the implications of uncertainty over treatment assignment (confounding). \n\nAny differences in assignment probabilities that are \\textit{known} are built into our priors in a Bayesian setting and do not produce biases (just as known confounds can be controlled for in a standard regression model). However, \\textit{uncertainty} about assignment processes still generates higher variance in posterior estimates \\citep[see][]{GerGreKap04}. % where variance of the prior distribution of bias is infinite. In this case, they show that it is optimal to place resources in strategies that have no uncertainty around sources of bias---such as in experimental data collection rather than observational data collection. Similarly, authors in the process tracing tradition have argued that a key feature of process tracing is its capacity to detect reverse causation, omitted variables, and other confounding processes (e.g., \\citep{Lieberman2005nested}. \nIn the BIQQ framework, however, clues provide discriminatory leverage on case types that is \\textit{independent} of assignment probabilities: with strong probative value, $b$ and $d$ type treated units can be told apart thus eliminating the identification problem that certainty over assignment processes helps to solve.  \n%How does the level of uncertainty about assignment affect the optimal design mix? \nIn our simulations (bottom left panel of Figure \\ref{experiments}), we find that greater uncertainty over assignment processes indeed results in greater errors for correlational analysis | most obviously for higher $n$. However, for the parameter space we examine (and given strong assumptions on the probative value of clues), uncertainty about assignment does not reduce accuracy for mixed methods analysis. %Mixed methods analysis, that is, appears more robust to violations of the ignorability assumption. \n(See Supplementary Materials, \\S\\ref{AppE3}.)\n\n\\subsubsection{Uncertainty regarding the probative value of clues}\n\n%The critical assumption for drawing inferences from clues is that researchers know the likelihood of clues being present as a function of type: that is, that we know the likelihood with which a given piece of process-based evidence should be observed if a given causal effect is truly present. This is clearly a strong assumption. %, dependent on identifying both the right theoretical logic for each causal effect and  the ways in which that logic would make itself observable in within-case data. \n%In this sense, process tracing rests on a set of assumptions that may be just as uncertain as the assumption of particular (conditional) assignment probabilities made in observational quantitative research. \nAs with assignment probabilities, researchers may be uncertain regarding the probative value of clues for discriminating between types. How much does this uncertainty matter for the relative gains to qualitative evidence?\n\nSurprisingly, our simulations suggest that uncertainty over the probative values of clues is unimportant for expected errors (see Supplementary Materials, \\S\\ref{AppE4}). Our experiment fixes the expected probative value of a clue and allows for variance around that expected value. Informally, we are thus comparing a situation in which one believes that a clue has moderate probative value to one in which one believes that it may have strong probative value or it may have none at all. \n\nTo be clear, this analysis does \\textit{not} imply that there is no penalty to being \\textit{wrong} about the probative value of clues. %If a researcher is convinced that a clue has more probative value than it actually has, then the researcher will draw the wrong inferences. \nThe result suggests rather, that having more, rather than less, \\textit{uncertainty} about that probative value may be relatively inconsequential for the choice of research strategy.\n\n\n\n## Notes on Simulations {#AppSimNotes}\nHere we provide statistical details and some further interpretation for the paper's simulations assessing the benefits of different designs conditional on different priors regarding the probative value of clues, the heterogeneity of causal effects, uncertainty regarding assignment probabilities, and uncertainty regarding the probative value of clues. Table \\ref{sims} provides details on all parameters used in simulations and Table \\ref{simdetails} provides detail on the number of runs, iterations, and related information used in the estimation.\n\n### Probative values {#AppE1}\nFor these simulations we simultaneously vary the probative value for tests for all $X,Y$ combinations. Specifically, we vary the differences between $\\phi_{b0}$ and $\\phi_{c0}$ (for $X=Y=0$ cases), between  $\\phi_{a0}$ and $\\phi_{d0}$ (for $X=0, Y=1$ cases); between   $\\phi_{a1}$ and $\\phi_{c1}$ (for $X=1, Y=0$ cases); and between   $\\phi_{b1}$ and $\\phi_{d1}$ (for $X=Y=1$ cases).  For each $X,Y$ combination, we compare the relevant $\\phi$ pairs across values of $(.5,.5)$ (no probative value), $(.25,.75)$ (middling probative value) and $(0.01,0.99)$ (strong probative value). Using the definition of probative value (PV) we provide (Section \\ref{PTintro}, see footnote \\ref{fnPV}), these correspond to cases with probative value of 0, .5, and close to 1 respectively. \n\n\n### Effect heterogeneity {#AppE2}\nWe note that heterogeneity makes going ``wide'' relatively more beneficial for two reasons. First, when all cases are affected either positively or negatively, all of the information needed to identify types is provided by information on $X$ and $Y$. If $X=Y$ then a case was (or could have been) positively affected; if $X \\ne Y$ then a case was (or could have been) negatively affected. In this extreme case of maximal heterogeneity, causal process information provides no additional inferential gains. Where there is high homogeneity, on the other hand, the core difficulty is distinguishing $a$ and $b$ types, from $c$ and $d$ types. Then, the information contained in clues may provide greater benefits (see Table \\ref{FP}). Second, the more heterogeneous effects are across cases, the less we learn about \\textit{population-level} causal effects by getting an individual case right. Thus, again, we would expect greater relative gains to more extensive analysis as heterogeneity increases.\n\n### Uncertainty about assignment processes {#AppE3}\nNote that in our binary setup, infinite bias cannot arise, and the harm done by uncertainty over selection processes can be more moderate.  In this set of simulations, the expected value of $\\pi_j$ is fixed at $0.5$ and we vary the variance in  $\\pi_j$ between 0 and a maximum of 0.289.     \n\n\n### Uncertainty regarding the probative value of clues {#AppE4}\nIn this experiment, the expected probability that a clue will be observed is set to 0.75 if one hypothesis is right, and 0.25 if the alternative hypothesis is correct. The simulations vary from a situation in which those probabilities are known with certainty (uncertainty low) to a situation in which the researcher admits the possibility of many possible values of $\\phi$ (uncertainty set to its maximum of 0.25). Uncertainty is simultaneously varied for all pairs of $\\phi$ values (see \\S\\ref{AppE1}). The displayed results suggest that uncertainty about the probative value of clues plays little role in the assessment of optimal strategies. Fixing the penumbra of uncertainty around a given expected $\\phi$ value allows for the possibility that the clue may have weak probative value, but also that it may have exceptionally strong probative value. The effects of these possibilities appear to wash out when we update beliefs about causal effects upon observation of the clue (or its absence).\n\n\n### Details on simulation experiments\n\n\\begin{table}[htbp]\n  \\centering\n    \\begin{tabular}{ccc|c|ccc|ccc|ccc|ccc}\n          &       &       & 1: m or n & \\multicolumn{3}{|c|}{2:  Probative Value} & \\multicolumn{3}{|c|}{3: Effect Heterogeneity} & \\multicolumn{3}{|c|}{4: Assignment Uncertainty} & \\multicolumn{3}{c}{5: Clue Uncertainty}  \\\\\n\\hline\n    $\\theta$ & Dist & arg   &       & Low     & $\\rightarrow$ & High     & Low      & $\\rightarrow$ & High     & Low     & $\\rightarrow$ & High     & Low     & $\\rightarrow$ & High \\\\ \\hline \n    $\\lambda_a$ & Dirichlet& $\\alpha_a$   & 1  & 0.20  & $\\rightarrow$ & 0.20       & 0.10  & $\\rightarrow$ & \\textbf{2.00}  & 1  & $\\rightarrow$ & 1  & 1  & $\\rightarrow$ & 1 \\\\\n    $\\lambda_b$      &       & $\\alpha_b$ & 1  & 0.20  & $\\rightarrow$ & 0.20  \t& 02.10  & $\\rightarrow$ & \\textbf{4.00}   & 1  & $\\rightarrow$ & 1  & 1  & $\\rightarrow$ & 1 \\\\\n    $\\lambda_c$      &       & $\\alpha_c$ & 1  & 0.20  & $\\rightarrow$ & 0.20 \t& 02.00  & $\\rightarrow$ & \\textbf{0.10}   & 1  & $\\rightarrow$ & 1  & 1  & $\\rightarrow$ & 1 \\\\\n    $\\lambda_d$      &       & $\\alpha_d$ & 1  & 0.20  & $\\rightarrow$ & 0.20  \t& 02.00 \t& $\\rightarrow$ & \\textbf{0.10}   & 1  & $\\rightarrow$ & 1  & 1  & $\\rightarrow$ & 1 \\\\\n    \\hline\n    $\\pi_a$  & Beta  & $\\mu$  \t& 0.50   & 0.50   & $\\rightarrow$ & 0.50  \t\t\t\t & 0.50  & $\\rightarrow$ & 0.50   & 0.50   & $\\rightarrow$ & 0.50   & 0.50   & $\\rightarrow$ & 0.50 \\\\\n          &       & $\\sigma$    & 0.10 & 0.10 & $\\rightarrow$ & 0.10 & 0.10 \t\t\t& $\\rightarrow$ & 0.10   & 0.01 & $\\rightarrow$ & \\textbf{0.289} & 0.10 & $\\rightarrow$ & 0.10\\\\ \\hline\n    $\\pi_b$  & Beta  & $\\mu$  \t& 0.50   & 0.50   & $\\rightarrow$ & 0.50   \t\t& 0.50   & \t$\\rightarrow$ & 0.50   & 0.50   & $\\rightarrow$ & 0.50   & 0.50   & $\\rightarrow$ & 0.50 \\\\\n          &       & $\\sigma$    & 0.10 & 0.10 & $\\rightarrow$ & 0.10 & 0.10 & \t\t\t$\\rightarrow$ & 0.10 & 0.01 & $\\rightarrow$ & \\textbf{0.289} & 0.10 & $\\rightarrow$ & 0.10 \\\\ \\hline\n    $\\pi_c$  & Beta  & $\\mu$  \t& 0.50   & 0.50   & $\\rightarrow$ & 0.50   & 0.50   \t\t\t& $\\rightarrow$ & 0.50   & 0.50   & $\\rightarrow$ & 0.50   & 0.50   & $\\rightarrow$ & 0.50 \\\\\n          &       & $\\sigma$    & 0.10 & 0.10 & $\\rightarrow$ & 0.10 & 0.10 \t\t\t& $\\rightarrow$ & 0.10 & 0.01 & $\\rightarrow$ & \\textbf{0.289} & 0.10 & $\\rightarrow$ & 0.10 \\\\ \\hline\n    $\\pi_d$  & Beta  & $\\mu$  \t& 0.50   & 0.50   & $\\rightarrow$ & 0.50   & 0.50   \t\t\t& $\\rightarrow$ & 0.50   & 0.50   & $\\rightarrow$ & 0.50   & 0.50   & $\\rightarrow$ & 0.50 \\\\\n          &       & $\\sigma$    & 0.10 & 0.10 & $\\rightarrow$ & 0.10 & 0.10 \t\t\t& $\\rightarrow$ & 0.10 & 0.01 & $\\rightarrow$ & \\textbf{0.289} & 0.10 & $\\rightarrow$ & 0.10 \\\\\n\\hline\n    $\\phi_{a0}$ & Beta  & $\\mu$  \t& 0.01  & 0.50  & $\\rightarrow$ & \\textbf{0.01}  \t& 0.01  & $\\rightarrow$ & 0.01  & 0.01  & $\\rightarrow$ & 0.01  & 0.01  & $\\rightarrow$ & 0.25 \\\\\n          &       & $\\sigma$   \t& 0.01 & 0.01 & $\\rightarrow$ & 0.01 \t\t\t& 0.01 & $\\rightarrow$ & 0.01 & 0.01 & $\\rightarrow$ & 0.01 & 0.001 & $\\rightarrow$ & \\textbf{0.25} \\\\\\hline\n    $\\phi_{a1}$ & Beta  & $\\mu$  \t& 0.99  & 0.50  & $\\rightarrow$ & \\textbf{0.99}  \t& 0.99  & $\\rightarrow$ & 0.99  & 0.99  & $\\rightarrow$ & 0.99  & 0.75  & $\\rightarrow$ & 0.75 \\\\\n          &       & $\\sigma$      & 0.01 & 0.01 & $\\rightarrow$ & 0.01 \t\t\t& 0.01 & $\\rightarrow$ & 0.01 & 0.01 & $\\rightarrow$ & 0.01 & 0.001 & $\\rightarrow$ & \\textbf{0.25} \\\\\\hline\n    $\\phi_{b0} $& Beta  & $\\mu$  \t& 0.01  & 0.50  & $\\rightarrow$ & \\textbf{0.01}  \t& 0.01  & $\\rightarrow$ & 0.01  & 0.01  & $\\rightarrow$ & 0.01  & 0.25  & $\\rightarrow$ & 0.25 \\\\\n          &       & $\\sigma$    & 0.01 & 0.01 & $\\rightarrow$ & 0.01 \t\t\t& 0.01 & $\\rightarrow$ & 0.01 & 0.01 & $\\rightarrow$ & 0.01 & 0.001 & $\\rightarrow$ & \\textbf{0.25} \\\\\\hline\n    $\\phi_{b1}$ & Beta  & $\\mu$  \t& 0.99  & 0.50  & $\\rightarrow$ & \\textbf{0.99}  \t& 0.99  & $\\rightarrow$ & 0.99  & 0.99  & $\\rightarrow$ & 0.99  & 0.75  & $\\rightarrow$ & 0.75 \\\\\n          &       & $\\sigma$    & 0.01 & 0.01 & $\\rightarrow$ & 0.01 \t\t\t& 0.01 & $\\rightarrow$ & 0.01 & 0.01 & $\\rightarrow$ & 0.01 & 0.001 & $\\rightarrow$ & \\textbf{0.25} \\\\\\hline\n    $\\phi_{c0}$ & Beta  & $\\mu$  \t& 0.99  & 0.50  & $\\rightarrow$ & \\textbf{0.99}  \t& 0.99  & $\\rightarrow$ & 0.99  & 0.99  & $\\rightarrow$ & 0.99  & 0.75  & $\\rightarrow$ & 0.75 \\\\\n          &       & $\\sigma$    & 0.01 & 0.01 & $\\rightarrow$ & 0.01 \t\t\t& 0.01 & $\\rightarrow$ & 0.01 & 0.01 & $\\rightarrow$ & 0.01 & 0.001 & $\\rightarrow$ & \\textbf{0.25} \\\\\\hline\n    $\\phi_{c1}$ & Beta  & $\\mu$  \t& 0.01  & 0.50  & $\\rightarrow$ & \\textbf{0.01}  \t& 0.01  & $\\rightarrow$ & 0.01  & 0.01  & $\\rightarrow$ & 0.01  & 0.25  & $\\rightarrow$ & 0.25 \\\\\n          &       & $\\sigma$    & 0.01 & 0.01 & $\\rightarrow$ & 0.01 \t\t\t& 0.01 & $\\rightarrow$ & 0.01 & 0.01 & $\\rightarrow$ & 0.01 & 0.001 & $\\rightarrow$ & \\textbf{0.25} \\\\\\hline\n    $\\phi_{d0}$ & Beta  & $\\mu$  \t& 0.99  & 0.50  & $\\rightarrow$ & \\textbf{0.99}  \t& 0.99  & $\\rightarrow$ & 0.99  & 0.99  & $\\rightarrow$ & 0.99  & 0.75  & $\\rightarrow$ & 0.75 \\\\\n          &       & $\\sigma$    & 0.01 & 0.01 & $\\rightarrow$ & 0.01 \t\t\t& 0.01 & $\\rightarrow$ & 0.01 & 0.01 & $\\rightarrow$ & 0.01 & 0.001 & $\\rightarrow$ & \\textbf{0.25} \\\\ \\hline\n    $\\phi_{d1}$& Beta  & $\\mu$  \t& 0.01  & 0.50  & $\\rightarrow$ & \\textbf{0.01}  \t& 0.01  & $\\rightarrow$ & 0.01  & 0.01  & $\\rightarrow$ & 0.01  & 0.25  & $\\rightarrow$ & 0.25 \\\\\n          &       & $\\sigma$    & 0.01 & 0.01 & $\\rightarrow$ & 0.01 \t\t\t& 0.01 & $\\rightarrow$ & 0.01 & 0.01 & $\\rightarrow$ & 0.01 & 0.001 & $\\rightarrow$ & \\textbf{0.25} \\\\\n\\hline\n    \\end{tabular}%\n  \\caption{Simulation parameters. Each column details parameters used to generate prior distributions for one of the simulations below. The prior distribution for the full parameter vector is formed from independent draws from Beta distributions for all probabilities and the Dirichlet distribution for shares. Note that the mean and standard deviation parameterization we provide for Beta distributions can be mapped directly to the more standard $\\alpha, \\beta$ parameterization. \n }\n  \\label{sims}%\n\\end{table}%\n\n\n\\newpage\n\n\n\\normalsize \n\n\\begin{table}\n\\footnotesize\n\\centering\n\\begin{tabular}{lccp{8cm}} \n\t\t\t\t\t\t& $j$ steps \t\t\t& $k$ sims\t\t\t\t\t&  \t\t\t\t\t\\\\ \nExperiment \t\t\t\t& per exp.\t\t\t\t& per step\t\t\t\t\t& Comments \t\t\t\t\t\\\\ \\hline\n\\textbf{1:}\nVarying $N$ or $m$\t\t& 29\t\t\t\t\t& 5,200\t\t\t\t\t\t\t\t& The 5,200 $k$ simulations for each $\\theta_j$ were split into 26 runs of 200 $k$ sims, and then\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  compiled through averaging. \\\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \n&&&\t\t\t\t\tDatapoints at N=3 and N=4 were added using 14,200 $k$ simulations. \\\\\\\\\n\\textbf{2:}\nProbative Value \t\t& 30\t\t\t\t\t& 5,200\t\t\t\t\t\t\t\t& The 5,200 $k$ simulations for each $\\theta_j$ were split into 26 batches of 200 $k$ sims, and then\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  compiled through averaging.  \\\\ \\\\\n\\textbf{3:}\nEffect Heterogeneity\t& 30\t\t\t\t\t& 5,200\t\t\t\t\t\t\t\t& The 5,200 $k$ simulations for each $\\theta_j$ were split into 26 batches of 200 $k$ sims, and then\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  compiled through averaging.  \\\\ \\\\\n\\textbf{4:}\nAssignment Uncertainty\t& 30\t\t\t\t\t& 5,200\t\t\t\t\t\t\t\t& The 5,200 $k$ simulations for each $\\theta_j$ were split into 26 batches of 200 $k$ sims, and then\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  compiled through averaging.  \\\\ \\\\\n\\textbf{5:}\nClue Uncertainty    \t& 30\t\t\t\t\t& 10,200\t\t\t\t\t\t\t& The 10,200 $k$ simulations for each $\\theta_j$ were split into 26 batches of 200 $k$ sims and \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  10 batches of 500, then\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  compiled through averaging.  \\\\\n\\end{tabular}\n\\caption{\\footnotesize \\textit{Note:} \nEach experiment takes $j$ steps through different values of $\\theta$. At each $\\theta_j$, the data is simulated $k$ times. For each simulation,   \na call is made to the Stan model and HMC (Hamiltonian Monte Carlo) sampling is used\nto approximate the posterior distribution. \nIn each such call to Stan, we run 4 chains with 6000 iterations, and \n1000 warmup draws.}\n\\label{simdetails}\n\\end{table}\n",
    "created" : 1493153756392.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "662698311",
    "id" : "3731592",
    "lastKnownWriteTime" : 1493883251,
    "last_content_update" : 1494067910329,
    "path" : "C:/Dropbox/ProcessTracing/8 Book/ii/09-wideordeep.Rmd",
    "project_path" : "09-wideordeep.Rmd",
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}
{
    "collab_server" : "",
    "contents" : "# (PART) Model-Based Process Tracing  {-} \n\n\n```{r, include = FALSE}\nsource(\"hj_dag.R\")\nsource(\"function_perm.R\")\n```\n\n\n# DAGs and clues {clues}\nWe have described the general problem of process tracing as using observable data to make inferences about unobserved, or unobservable parts of a causal structure. Can we say when, in general, this is possible? The literature on probabilistic DAGs gives a positive answer to this question. \n\n## A Condition for probative value\n\n<!-- Rules for inferring information about one variable from another are th stuff of graphoids  [@pearl1985graphoids] see also [@geiger1987non] and [@pearl1987logic]...  -->\n\nAs we have argued, causal estimands can be expressed as the values of exogenous nodes in a causal graph. Case-level causal effects and causal paths can be defined in terms of response-type nodes; average effects and notable causes in terms of population-level parameter nodes (e.g., $\\pi$ or $\\lambda$ terms); and questions about actual causes in terms of exogenous conditions that yield particular endogenous values (conditioning on which makes some variable a counterfactual cause). \n\nWe thus define causal inference more generally as *the assessment of the value of one or more unobserved (possibly unobservable) exogenous nodes on a causal graph, given observable data.*  To think through the steps in this process, it is useful to distinguish among three different features of the world, as represented in our causal model: there are the things we want to learn about; the things we have already observed; and the things we could observe. As notation going forward, let:\n\n* $\\mathcal Q$ denote the exogenous variables that define our *query*; we generally assume that $\\mathcal Q$ cannot be directly observed so that its values must be inferred\n* $\\mathcal W$ denote a set of previously observed nodes in the causal model, and \n* $\\mathcal K$ denote a set of additional  variables---clues---that we have not yet observed but could observe.\n\nNow suppose that we seek to design a research project to investigate a causal question. How should the study be designed? Given that there are some features of the world that we have already observed, which additional clues should we seek to collect to shed new light on our question? In terms of the above notation, what we need to figure out is whether a given $\\mathcal K$ might be informative about---might provide additional leverage on---$\\mathcal Q$ given the prior observation of $\\mathcal W$. \n\nTo ask whether one variable (or set of variables) is informative about another is to ask whether the two (sets of) variables are, on average, *correlated* with one another, given whatever we already know. Likewise, if two variables' distributions are fully *independent* of one another (conditional on what else we have observed), then knowing the value of one variable can provide no new information about the value of the other. \n\nThus, asking whether a set of clues, $\\mathcal K$, is informative about $\\mathcal Q$ given the prior observation of $\\mathcal W$, is equivalent to asking whether $\\mathcal K$ and $\\mathcal Q$ are conditionally independent given $\\mathcal W$. That is, $\\mathcal K$ can be informative about $\\mathcal Q$ given $\\mathcal W$ only if $\\mathcal K$ and $\\mathcal Q$ are *not* conditionally independent of one another given $\\mathcal W$. \n\nAs we have shown, as long as we have built $\\mathcal Q$, $\\mathcal K$, and $\\mathcal W$ into our causal model of the phenomenon of interest, we can answer this kind of question by inspecting the structure of the model's DAG. In particular, what we need to go looking for are relationships of *$d$-separation*. The following proposition, with only the names of the variable sets altered, is from @pearl2009causality (Proposition 1.2.4): \n\n**Proposition 1:**  If sets $\\mathcal Q$ and $\\mathcal K$ are $d$-separated by $\\mathcal W$ in a DAG, $\\mathcal G$, then $\\mathcal Q$ is independent of $\\mathcal K$ conditional on $\\mathcal W$ in every distribution compatible with $\\mathcal G$. Conversely, if $\\mathcal Q$ and $\\mathcal K$ are *not* $d$-separated by $\\mathcal W$ in DAG $\\mathcal W$, then $\\mathcal Q$ and $\\mathcal K$ are dependent conditional on $\\mathcal W$ in at least one distribution compatible with $\\mathcal G$.\n\nWe begin with a causal graph and a set of nodes on the graph ($W$) that we have already observed. Given what we have already observed, *a collection of clue nodes, $\\mathcal K$, will be uninformative about the query nodes, $\\mathcal Q$, if $\\mathcal K$ is  $d$-separated from $\\mathcal Q$ by $\\mathcal W$ on the graph.* When $\\mathcal W$ $d$-separates $\\mathcal K$ from $\\mathcal Q$, this means that what we have already observed already captures all information that the clues might yield about our query. On the other hand, if $\\mathcal K$ and $\\mathcal Q$ are $d$-connected (i.e., not $d$-separated) by $W$, then $K$ is *possibly* informative about $Q$.$K$ is not  $d$-separated from $\\mathcal Q$ by $\\mathcal W$.^[This proposition is almost coextensive with the definition of a DAG. A DAG is a particular kind of dependency  model (\"graphoid\") that is a summary of a  collection of \"independency statements\", $(I)$, over distinct subsets of $\\mathcal V$ (Pearl and Verma 1987), where $I(\\mathcal Q,\\mathcal W,\\mathcal K)$ means  \"we learn nothing about $\\mathcal Q$ from $\\mathcal K$ if we already know $\\mathcal W$\". More formally:\n$$I(\\mathcal K, \\mathcal W,\\mathcal Q) \\leftrightarrow P(\\mathcal K,\\mathcal Q|\\mathcal W)=P(\\mathcal K|\\mathcal W)P(\\mathcal Q|\\mathcal W)$$\nA Directed Acyclic Graph Dependency model is one where the set of independencies correspond exactly to the relations that satisfy $d$-separation  (Pearl and Verma 1987, p376).  Thus on DAG $\\mathcal G$, $I(\\mathcal K,\\mathcal W,\\mathcal Q)_\\mathcal G$ implies that $\\mathcal K$ and $\\mathcal Q$ are $d$-separated by $\\mathcal W$.] Note, moreover, that under quite general conditions (referred to in the literature as the *faithfulness* of a  probability distribution) then there are at least *some* values of $\\mathcal W$ for which $\\mathcal K$ *will* be informative about $\\mathcal Q$.^[Put differently, there will not be any conditional independencies that are *not* captured in the DAG.] \n\n\n<!-- \\footnote{In Pearl's terminonology, the graph may *represent* the probability distribution but not be *faithful* to it.} -->\n\n<!-- This can be put another way. An $I$-map of $M$ is a model with no extra independencies; a $D$-map is a model that contains all of $M$ with,  possibly aditional independencies; a *perfect* map is a model with the same set of dependencies. Given an independency model $M$, a DAG, $G$, may be an $I$-map of $M$ in the sense that whenever $D$ separates $K$ from $Q$ then $I(K,D,Q)_M$, yet there may still be indepenencies in $M$ not captured by $G$; that is, it may also be htat $I(K,D,Q)_M$ but not $I(K,D,Q)_G$. Pearl refers to such cases as instances of a violation of *stability*, though in simple graphs with discrete variables such violations may be plausible.  -->\n\n<!-- In the example given by Pearl with two matching pennies, $X_1$ and $X_2$ and $Y$ is 1 if the pennies match, $X_1$ adn $X_2$ are probabilisitcally independent of $Y$, yet $Y$ depends on both of them.  -->\n<!-- The problem is that $d$-separation satisfies composition, that is, if $I(X_1, D, Q)$ and $I(X_2, D, Q)$ then $I(X_1X_2, D, Q)$; but since we cannot have $I(X_1X_2, D, Q)$ then we cannot have   $I(X_1, D, Q)$ and $I(X_2, D, Q)$ either (see also [@bouckaert1994conditional]). -->\n\n<!-- Note that this example depends on infomration about the probability distribution over $V$, that is, the functional equations, and cannot be inferred from the structure of $S$ alone.   -->\n\n<!-- [Note for us: We seek a  related proposition holds however using $d-separation$ on partially discovered submodels.] -->\n\nLet us examine Proposition 1 in practice. We begin with the simplest case possible, and then move on to more complex models. \n\nThe very simplest probabilistic causal graph has $X$ influencing $Y$, with $X$ determined by a coin flip. Assuming that there is some causal heterogeneity---that is, it is unknown in any particular case whether $X$ causes $Y$---we also include a response-type variable, $Q$, pointing into $Y$, as shown in Figure \\ref{fig:d-sepsimple}. Here, $Q^Y$ determines the value of $Y$ that will be generated by $X$. Asking about the causal effect of $X$ in a case thus means learning the value of $Q^Y$ in that case. As will be recalled, in a binary setup with one causal variable, a response-type variable can take on one of four values, $q^Y_{00}$, $q^Y_{10}$, $q^Y_{01}$ and $q^Y_{11}$,^[As a reminder, we read $q^Y_{ij}$ (when $X$ is binary) as meaning that $Y$ will take on value $i$ when $X=0$ and value $j$ when $X=1$.] corresponding to the four possible causal types in this setting.\n\n```{r, echo = FALSE, fig.width = 5, fig.height = 3,  fig.align=\"center\", out.width='.5\\\\textwidth', fig.cap = \"\\\\label{fig:d-sepsimple} A simple causal setup in which the effect of $X$ on $Y$ in a given case depends on the case's response type for $Y$.\"}\npar(mar=c(1,1,1,1))\nhj_dag(x = c(0, 1, 1),\n       y = c(1, 1, 2),\n       names = c(\"X\", \"Y\", expression(paste(Q^Y))),\n       arcs = cbind( c(1, 3),\n                     c(2, 2)),\n       title = \"A DAG with Response Type, Q^Y\",\n       padding = .4, contraction = .15) \n\n```\n\nLet us assume that we have observed nothing yet in this case and then ask what clue(s) might be informative about $Q^Y$, the node of interest. The other two nodes in the graph are $X$ and $Y$: these are thus the possible clues that we might go looking for in our effort to learn about $Q^Y$ (i.e., they are the possible members of $\\mathcal K$). \n\n\n\n\nFirst, can we learn about $Q^Y$ by observing $X$? We can answer this question by asking whether $X$ is $d$-connected to $Q^Y$ on the graph given what we have already observed (which is nothing). We can see visually that there is no active path from $X$ to $Q^Y$: the only path between $X$ and $Q$ is blocked by colliding arrow heads. Thus, $X$ and $Q^Y$ are $d$-separated, meaning that $X$ will not be informative about $Q^Y$: observing the value that a causal variable takes on in a case---having seen nothing else in the case---tells us nothing whatsoever about that variable's effect on the outcome. If we want to know whether a case is of a type in which the presence of natural resources would cause civil war, observing only that the case has natural resources does not help answer the question.\n\n**LONG FOOTNOTE STARTING HERE....**\nIn the case where we observe only $X$, the posterior on $Q^Y$ is:\n\\begin{eqnarray*}\nP(Q^Y=q^Y | X=x) &=& \\frac{\\sum_{j=0}^1p(X=x)P(Q^Y=q^Y)P(Y=j|X=x, Q^Y=q^Y)}{\\sum_{q^{Y'}}\\sum_{j=0}^1p(X=x)P(Q^Y=q^{Y'})P(Y=j|X=x, Q^Y=q^{Y'})}\\\\\n&=&\\frac{P(Q^Y=q^Y)}{\\sum_{q^{Y'}}P(Q^Y=q^{Y'})}\n\\end{eqnarray*}\nwhich is simply the prior on $Q^Y$. Thus, nothing is learned about $Q^Y$ from observing $X$ only.] \n<!-- &=& \\frac{p(Q=q)\\sum_{j=0}^1p(Y=j|X=x, Q=q)}{\\sum_{q'}p(Q=q')\\sum_{j=0}^1p(Y=j|X=x, Q=q')}\\\\ -->\n**...ENDING HERE**\n\nWhat, then, if we instead were to observe only $Y$? Is $Y$ $d$-connected to $Q$ given what we have already observed (which, again, is nothing)? It is: the arrow from $Q^Y$ to $Y$ is an active path. Observing the only the *outcome* in a case does tell us something about causal effects. Returning to the natural resources and civil war example, observing only that a country has had a civil is informative about the case's causal type (the value of $Q^Y$). In particular, it rules out the possibility that this is a case in which nothing could cause a civil war: that is, it excludes $q^Y_{00}$ (i.e., $c$-type) as a possible value of $Q^Y$.\n\n**LONG FOOTNOTE STARTING HERE....**\nIn the case where we observe $Y$ only we have:\n$$P(Q=q | Y=y) = \\frac{\\sum_{j=0}^1p(X=j)P(Q=q)P(Y=y|X=j, Q=q)}{\\sum_{q'}\\sum_{j=0}^1p(X=j)P(Q=q')P(Y=y|X=j, Q=q')}$$\nHere terms involving $Y$ and $Q$ cannot be separated, so the same kind of reduction is not possible. This implies scope for learning about $Q$ from $Y$.  For instance, if  we have $P(Q=j) = 1/4$ for type $j \\in \\{a,b,c,d\\}$  and $P(X=j) = \\frac{1}{2}$, then we have $P(Q=a | Y=1)=P(Q=b | Y=1) =\\frac{1}{4}$, $P(Q=c | Y=1)=0$ and $P(Q=d | Y=1)=1$.\n**...ENDING HERE**\n\nSuppose now, having observed $Y$, that we were to consider also observing $X$. Would we learn anything further about $Q^Y$ from doing so? We have already seen that observing $X$ alone yields no information about $Q^Y$ because the two nodes are unconditionally $d$-separated, the path between them blocked by the colliding arrowheads at $Y$. However, as we have seen, observing a collider variable (or one of its descendants) *unblocks* the flow of information, generating relations of conditional dependence across the colliding arrowheads. Here, $X$ and $Q^Y$ are $d$-connected by $Y$: thus, if we have *already* observed $Y$, then observing $X$ does confer additional information about $Q^Y$. Knowing only that a country has natural resources tells us nothing about those resources' effect on civil war in that country. But if we already know that the country has a civil war, then learning that the country has natural resources helps narrow down the case's possible response types. Having already used the observation of $Y=1$ to rule out the possibility that $Q^Y=q^Y_{00}$, observing $X=1$ *together with* $Y=1$ allows us to additionally rule out the possibility that natural resources *prevent* civil war, i.e., that $Q^Y=q^Y_{01}$.^[That is, we can rule out that the case is an $a$ type, or one with a negative causal effect.]\n\n**LONG FOOTNOTE STARTING HERE....**\nWhere we observe both $Y$ and $X$,  we have:\n$$P(Q=q | Y=y, X=x) = \\frac{P(X=x)P(Q=q)P(Y=y|X=x, Q=q)}{\\sum_{q'}P(X=x)P(Q=q')P(Y=y|X=x, Q=q')}$$\nwhich does not allow separation either of  $Q$ and $X$ or of $Q$ and $Y$. Thus, there is again learning from $Y$ and, given $Y$, there is *also* learning from $X$. Put differently, we have $P(Q|Y,X) \\neq P(Q|Y)$. \n\n**...ENDING HERE**\n\nFinally, what if we observe $X$ first and are considering whether to seek information about $Y$? Would doing so be informative? $X$ does not $d-$separate $Q^Y$ from $Y$; thus, observing $Y$ will be informative about $Q^Y$. In fact, observing $Y$ if we have already seen $X$ is *more* informative than observing $Y$ alone. The reasoning follows the  logic of collision discussed just above. If we observe $Y$ having already seen $X$, not only do we reap the information about $Q^Y$ provided by $Y$'s correlation with $Q^Y$; we simultaneously open up the path between $X$ and $Q^Y$, learning additionally from the conditional dependence between $X$ and $Q^Y$ given $Y$. \n\n\n```{r, echo = FALSE, include = FALSE, errors = FALSE}\n# Graphing all dags in some class, X,Y,K,Q\n\nlibrary(expm)\n\nlibrary(dagitty)\n\n\n# Append to list\nlappend <- function(L, obj) {\n  L[[length(L)+1]] <- obj\n  return(L)\n  }\n\n\ntranslate_matrix <- function(A, var_names = c(\"K\", \"X\", \"Y\", \"Q\")){\n paste(var_names[which(A==1, arr.ind = TRUE)[,1]], \"causes\", var_names[which(A==1, arr.ind = TRUE)[,2]])\n  }\n\ntranslate_dagitty <- function(A, var_names = c(\"K\", \"X\", \"Y\", \"Q\")){\n  paste(var_names[which(A==1, arr.ind = TRUE)[,1]], \"->\", var_names[which(A==1, arr.ind = TRUE)[,2]])\n  }\n\nmake_daggity     <- function(A) dagitty(paste(\"dag{\", paste(translate_dagitty(A), collapse = \";\"), \"}\"))\n\nkinformative2 <- function(A){\n   paste(c(\n       dseparated(make_daggity(A), \"Q\", \"K\", c()),\n       dseparated(make_daggity(A), \"Q\", \"K\", c(\"X\")),\n       dseparated(make_daggity(A), \"Q\", \"K\", c(\"Y\")),\n       dseparated(make_daggity(A), \"Q\", \"K\", c(\"X\",\"Y\"))\n       ), collapse = \",\")\n   }\n\nkinformative <- function(A){\n  paste(c(\"\\U2205\", \"X\", \"Y\", \"XY\")[!c(\n    dseparated(make_daggity(A), \"Q\", \"K\", c()),\n    dseparated(make_daggity(A), \"Q\", \"K\", c(\"X\")),\n    dseparated(make_daggity(A), \"Q\", \"K\", c(\"Y\")),\n    dseparated(make_daggity(A), \"Q\", \"K\", c(\"X\",\"Y\"))\n  )], collapse =\",\")\n}\n\n\n# 34 graphs\n# All paths. All have Q --> Y. None have Y --> X\n#   KXYQ\n# K 0???\n# X ?0??\n# Y ?000\n# Q ??10\nvar_names <- c(\"K\", \"X\", \"Y\", \"Q\")\n\ncheck.A <- function(A, iterations = 4){\n  x <- 1\n  if( sum(sapply(1:iterations, function(j) sum(diag((A %^% j))))) > 0) x<- 0 # Cycles\n  if( sum(sapply(1:iterations, function(j) (A %^% j)[2,3])) == 0) x<- -1 # \"No path from X to Y (2 to 3)\"\n  if(min(rowSums(A) + colSums(A))==0) x<- -2 # \"Unconnected element\"\n  if(sum(A[,4])>0) x<- -3 # \"No causes of Q allowed in graph\"\n  x}\n\n.A <-  matrix(c( 0,NA,NA,NA, \n                 NA, 0,NA,NA,\n                 NA, 0, 0, 0,\n                 NA,NA, 1, 0), 4, 4, byrow = TRUE)\nrownames(.A) <- var_names; colnames(.A) <- var_names\n\npossibilities <- perm(rep(2,9))\n\nnew_A <- function(j) {A <- .A; A[c(2:4, 5, 8:10, 13:14)] <- possibilities[j,]; A}\n\naccept <- list()\nfor(i in 1:nrow(possibilities)) {\n  A <- new_A(i)\n  if(check.A(A)==1) accept <- lappend(accept, A)\n}\n\n```\n\nWe put Proposition 1 to work in a slightly more complex set of models in Figure \\ref{fig:34graphs}. Here we investigate the informativeness of a clue that is neither $X$ nor $Y$. Each graph in Figure \\ref{fig:34graphs} has four variables: $X$; $Y$; a possible clue, $K$; and a response-type variable, $Q$. We draw all 34 possible graphs with variables $X$, $Y$, $K$, and $Q$ for causal models in which (a) all variables are connected to at least one other variable, (b) $X$ causes $Y$ either directly or indirectly, and (c) $Q$ is a direct cause of $Y$ but is not caused by any other variable in the model and is thus exogenous. The title of each panel reports $K$'s conditional informativeness using principles of $d$-separation: it tells us when $K$ is possibly informative about $Q$ depending on whether $X$, $Y$, both or none are observed.^[Note the \"possibly\" can be dropped under the assumption that the underlying probability model is \"stable\" (Pearl 2009, section 2.9.1) and with the interpretation that $K$ is informative about $Q$ for some, but not necessarily all, values of $W$.]\n\n<!-- Above footnote: do we want to say \"faithful\" rather than stable, as we do earlier? -->\n\n```{r, echo = FALSE, fig.width = 11, fig.height = 12, fig.cap = \"\\\\label{fig:34graphs} All connected directed acyclic graphs over $X,Y,K,Q$, in which $Q$ is an exogenous variable that directly causes $Y$, and $X$ is a direct or indirect cause of $Y$. The title of each graph indicates the conditions under which $K$ can be informative about (i.e., is not $d$-separated from) $Q$, given the prior observation of $X$, $Y$, both, or neither (...).\", errors = FALSE, warning = FALSE, message = FALSE, comment = FALSE}\n\n\npar(mfrow = c(6, 6) )\n  par(mar=c(1,1,3.5,1))\n  for(i in 1:length(accept)){\n    kinf <- kinformative(accept[[i]])\n    hj_dag(x = c(0,0,1,1), y = c(1,0,0,1), names = var_names, \n           arcs = which(accept[[i]]==1, arr.ind = TRUE), \n           title = title(ifelse(\n                         length(kinf)==0, \n                         paste0(i, \". K never informative\"),\n                         paste0(i, \". K possibly informative\\n given: \", kinf))), \n           padding = .15, cex = 1.2, length = .15) \n    }\n\n```\n\nThe results show us not just what kinds of variables can be informative about a case's response-type but also what combinations of observations yield leverage on case-level causal effects. A number of features the graphs are worth highlighting:\n\n* **Clues at many stages.** Process tracing has focused a great deal on observations that lie \"along the path\" between suspected causes and outcomes. What we see in Figure \\ref{fig:34graphs}, however, is that observations at many different locations in a causal model can be informative about causal effects. We see here that $K$ can be informative when it is pre-treatment (causally prior to $X$---e.g. panel (3)), post-treatment but pre-outcome (that is, \"between\" $X$ and $Y$ as, e.g., in panel (20)), an auxiliary effect of $X$ that itself has no effect on $Y$ (e.g., in panel (19)), post-outcome (after $Y$---e.g., in panel (15)), or a joint effect of both the suspected cause and the outcome (e.g., panel (31)). \n\n* **Mediator Clues**. While clues that lie in between $X$ and $Y$ may be informative, they can only be informative under certain conditions. For instance, when a clue serves *only* as a mediator in our model (i.e., its only linkages are being caused by $X$ and being affected by $Y$) and $Q$ only affects $Y$, as in panels (20) and (21), the clue is only informative about $Q$ if we have also observed the outcome, $Y$. Of course, this condition may commonly be met---qualitative researchers usually engage in retrospective research and learn the outcome of the cases they are studying early on---but it is nonetheless worth noting why it matters: in this setup, $K$ is unconditionally $d$-separated from $Q$ by the collision at $Y$; it is only by observing $Y$ (the collider) that the path between $K$ and $Q$ becomes unblocked. (As we saw above, the very same is true for observing $X$; it is only when we know $Y$ that $X$ is informative about $Q$.)\n\nIn short, observations along a causal paths are more helpful in identifying causal effects to the extent that we have measured the outcome. Importantly, this is not the same as saying that mediator clues are *only* informative about causal effects where we have observed the outcome. Observing $Y$ is necessary for the mediator to be informative about a $Q$ term that is connected only to $Y$. Observing a mediator without the outcome, however, could still be informative about the overall effect of $X$ on $Y$ by providing leverage on how the mediator responds to $X$, which is itself informative about $X$'s effect on $Y$ via the mediator.^[In other words, the clue would then be providing leverage on a response-type variable pointing into the mediator itself.] Moreover, observing the mediator could be informative without the observation of $Y$ if, for instance, $Q$ also points into $K$ itself or into a cause of $K$. As we discuss below, the clue then is informative as a \"symptom\" of the case's response type, generating learning that does not hinge on observing the outcome.\n\n* **Symptoms as clues.** Some clues may themselves be affected by $Q$: that is to say, they may be symptoms of the same conditions that determine causal effects in a case. For instance, in our illustrative model involving government survival, government sensitivity functions as a response-type variable for the effect of a free press ($X$) on government removal ($Y$): a free press only generates government removal when the government is non-sensitive to public opinion.  Sensitivity to public opinion thus represents our query variable, $Q$, if we seek to learn whether a free press causes government removal in a case. While it may not be possible to observe or otherwise measure the government's sensitivity, there may be *consequences* of government sensitivity that are observable: for instance, whether government officials regularly consult with civil-society actors on policy issues. While consultations would not be part of the causal chain generating the free press's effect, observing consultations (or the lack of them) would be informative about that effect because consultations are a symptom of the same conditions that enable the effect. \n\nWe see that $K$ is a child or descendant of $Q$ in several of the graphs in Figure \\ref{fig:34graphs}: $Q$ directly causes $K$ in panels (7) through (14), (17), (18), (25)-(30), (33), and (34); $Q$ causes (K) only indirectly through $X$ in panels (22) through (24); $Q$ causes (K) only indirectly through $Y$ in panels (15), (16), and (31); and $Q$ causes $K$ only indirectly through $X$ and through $Y$ in panel (32). We can then use the principle of $d$-separation to figure out when the symptom clue is potentially informative, given what we have already observed. It is easy to see that $K$ is potentially informative, no matter what we have already observed, if $K$ is directly affected by $Q$; there is nothing we could observe that would block the $Q \\rightarrow K$ path. Thus, $Q$'s \"symptom\" can, in this setup, contain information about type above and beyond that contained in the $X$ and $Y$ values. However, where $Q$ affects $K$ only through some other variable, observing that other variable renders $K$ uninformative by blocking the $Q$-to-$K$ path. For instance, where $Q$ affects $K$ indirectly through $X$, once we observe $X$, we already have all the information about $Q$ that would be contained in $K$. \n\n* **Surrogates as clues.** Clues may be consequences of the outcome, as in graphs (15) and (16). If $K$ is a consequence *only* of $Y$, then it will contain no new information about $Q$ where $Y$ is already known. However, in situations where the outcome has not been observed, $K$ can act as a \"surrogate\" for the outcome and thus yield leverage on $Q$ (@frangakis2002principal). A researcher might, for instance, seek to understand causal effects on an outcome that is difficult to directly observe: consider, for instance, studies that seek to explain ideational change. Ideas themselves, the $Y$ in such studies, are not directly observable. However, their consequences---such as statements by actors or policy decisions---will be observable and can thus serve as informative surrogates for the outcome of interest.\n\nClues may similarly serve as surrogates of a cause, as in graphs (19) and (22). Here $X$ causes $K$, but $K$ plays no role in the causal process generating $Y$. $K$ is of no help if we can directly measure $X$ since the latter $d$-separates $K$ from $Q$. But if an explanatory variable cannot be directly measured---consider, e.g., ideas or preferences as causes---then its consequences, including those that have no relationship to the outcome of interest, can provide leverage on the case-level causal effect.\n\nClues can also be a consequence of both our suspected cause and the outcome of interest, thus serving as what we might call \"double surrogates,\" as in panels (31) and (32). Here $X$ is a direct cause of $Y$, and $K$ is a joint product of $X$ and $Y$. A double surrogate can be informative as long as we have not already observed both $X$ and $Y$. Where data on either $X$ or $Y$ are missing, there is an open path between $K$ and $Q$. If we have already observed both, however, then there is nothing left to be learned from $K$.\n\n* **Instruments as clues.** Clues that are causally prior to an explanatory variable, and have no other effect on the outcome, can sometimes be informative. Consider, for instance, graph (3). Here $K$ is the only cause of $X$. It can thus serve as a proxy. If we have seen $X$, then $X$ blocks the path between $K$ and $Q$, and so $K$ is unhelpful. $K$ can be informative, though, if we have *not* observed $X$. Note that informativeness here still requires that we observe $Y$. Since $Y$ is a collider for $Q$ and the $K \\rightarrow X \\rightarrow$ chain, we need to observe $Y$ in order to $d$-connect $K$ to $Q$.\n\nA rather different setup appears in graph (5), where both $K$ and $Q$ cause $X$. Now the conditions for $K$'s informativeness are broader. Observing $X$ still makes $K$ uninformative as a proxy for $X$ itself. However, because $X$ is a collider for $K$ and $Q$, observing $X$ *opens up* a path from $K$ to $Q$, rendering a dependency between them. Still, we have to observe at least one of $X$ or $Y$ for the instrument to be informative here. This is because both of $K$'s paths to $Q$ run through a collision that we need to unblock by observing the collider. For one path, the collider is $X$; for the other path, the collider is $Y$.^[As a simple example one might imagine a system in which $X = K$ if  $q \\in {a,b}$  and $X = 1-K$ if  $q \\in {c,d}$. Then if we observe, say, $X=Y=K=1$, we can infer that $q = b$. Another way to think about what is happening in graph (5) is that $K$ is providing information about the *assignment process*. In this graph, the causal effect ($Y$'s potential outcomes, determined by $Q$) is also a partial determinant of the assignment of cases to values on $X$. In terms of cross-case correlational inference, then, we would think of this as a situation of confounding. Observing another cause of $X$, then, allows us to more fully characterize the process of assignment.] \n\n<!-- Graph (5) is similar to one discussed in [@hausman1999independence] in which there is learning from a pretreatment clue because $X$ is a collider for $K$ and $Q$.  -->\n\n<!-- To return to our government-removal model, government sensitivity to public opinion is a response-type variable (a $Q$ term), with non-sensitivity a pre-condition for the positive effect of a free press on removal. Yet it is possible (though we did not include it in our original model) that government sensitivity also affects whether or not a government gets a free press: more sensitive governments may impose tighter media restrictions. In that case, when governments are not sensitive, we would expect to see a free press and government removal.   -->\n\nOther patterns involving instrumentation are also imaginable, though not graphed here. For example, we might have a causal structure that combines instrumentation and surrogacy. Suppose that $X$ is affected by $Q$ and by an unobservable variable $U_X$; and that $U_X$ has an observable consequence, $K$. Then $K$, though not a cause of $X$, is a \"surrogate instrument\" [@hernan2006instruments] as it is a descendant of an unobserved instrument, $U$, and thus allows us to extract inferences similar to those that we could draw from a true instrument.\n\n* **Confounders as clues.** In several of the graphs, $K$ is a confounder in that it is a direct cause of both $X$ and $Y$ (panels (4), (6), (12), and (14)). Let us focus on graph (4), which isolates $K$'s role as a confounder. Here $K$ can be informative via two possible paths. First, if $X$ is not observed but $Y$ is, then $K$ is $d$-connected to $Q$ along the path $K \\rightarrow X \\rightarrow Y \\leftarrow Q$. $K$ is in this sense serving as a proxy for $X$, with its path to $Q$ opened up by the observation of the collider, $Y$. Second, with $Y$ observed, $K$ can provide information on $Q$ via the more direct collision, $K \\rightarrow Y \\leftarrow Q$. If $X$ *is* observed, then the first path is blocked, but the second still remains active. As with any pre-outcome variable, for a confounder clue to provide purchase on $Y$'s response type, $Y$ itself must be observed.\n\nIn a sense, then, the role of confounders as clues in case-level inference is the mirror image of the role of confounders as covariates in cross-case correlational inference. In a correlational inferential framework, controlling for a variable in $K$'s position in graph (5) renders the $X, Y$ correlation (which we assume to be observed) informative about $X$'s average causal effect. When we use confounders as evidence in within-case inference, it is our observations of other variables that determine how informative the confounder *itself* will be about $X$'s causal effect.\n\n\nIt is important to be precise about the kinds of claims that one can make from graphs like those in Figure \\{fig:34graphs}. The graphs in this figure allow us to identify informativeness about an unobserved node $Q$ that is a parent of $Y$. This setup does not, however, capture all ways in which clues can be informative about the causal effect of $X$ on $Y$ or about other causal estimands of interest. For instance, as noted above, even if a clue is uninformative about a $Q$ node pointing into $Y$, it may still help establish whether $X$ causes $Y$: the statement that $X$ causes $Y$ will for some graphs be a statement about a *collection* of nodes that form the set of query variables $\\mathcal Q$. This is the case, for instance, in any graph of the form $X \\rightarrow M  \\rightarrow Y$, where we are interested not just in $Y$'s response to $M$ (the mediator) but also in $M$'s response to $X$. Of interest, thus, are not just a $Q^Y$ response-type node pointing into $Y$ but also a $Q^M$ response-type node that is a parent of $M$. Observations that provide leverage on either $Q$ term will thus aid an inference about the overall causal effect. A clue $K$ that is $d-$separated from $Q^Y$ may nevertheless be informative about $X$'s effect on $Y$ if it is not $d-$separated from $Q^M$; this opens up a broader range of variables as informative clues. \n\nAdditionally, as our discussion in Chapter 2 makes clear, estimands other than the case-level causal effect---such as average causal effects, actual causes, and causal paths---involve particular features of context: particular sets of exogenous nodes as members of our query set, $\\mathcal Q$. Thus, even for the same causal model, informativeness will be defined differently for each causal question that we seek to address. The broader point is that we can identify what kinds of observations may address our estimand if we can place that estimand on a causal graph and then assess the graph for relationships of $d$-separation and -connection.\n\nFurther, we emphasize that a DAG can only tell us when a clue *may* be informative (conditional some prior observation): $d-$connectedness is necessary but not sufficient for informativeness. This fact derives directly from the rules for drawing a causal graph: the absence of an arrow between two variables implies that they are *not* directly causally related, while the presence of an arrow does not imply that they always are. As we saw in our analysis of the government-removal example in Chapter 2, whether variables connected to one another by arrows in the original DAG were in fact linked by a causal effect depended on the context. Likewise, whether a clue $K$ is in fact informative may depend on particular values of $\\mathcal W$---the variables that have already been observed. As a simple example, let $q = k_1w + (1-w)k_2$, where $W$ is a variable that we have already observed and $K_1$ and $K_2$ are clues that we might choose to observe next. Here, if $w=1$ then learning $K_1$ will be informative about $Q$, and learning $K_2$ will not; but if $w=0$, then $K_1$ will be uninformative (and $K_2$ informative). \n\n**DO WE NEED TO SAY SOMETHING ABOUT STABILITY/FAITHFULNESS HERE?**\n\nIn general, then, graphical analysis alone can help us exclude unhelpful research designs, given our prior observations and a fairly minimal set of prior beliefs about causal linkages. This is no small feat. But identifying those empirical strategies that will yield the greatest leverage requires engaging more deeply with our causal model, as we explore next.\n\n\n## Probative value from lower level models of moderation and mediation\n\nSo far, we have demonstrated principles of inference from clues, given causal graphs, without drawing on further elements of the causal model from which a graph has been derived. In particular, we have not drawn at all on the underlying structural equations the define the relationships among variables. The structure of a DAG turns only on beliefs about which variables are (conditionally) independent of one another. It contains no information about the direction of effects (e.g., whether effects are positive or negative) or any other feature of the functional forms characterizing causal dependencies. Usually, however, we go into a study with stronger prior beliefs about the world than those contained in a causal graph. We now drill deeper into those belief sets by placing a minimal structure on the functional forms characterizing causal relationships. To do so, we return to the two examples of moderation and mediation discussed in Chapter 3. \n\n### Inference from a lower level model of mediating effects\n\nSuppose that, in a binary setting, we have observed $X=1$ and $Y=1$ in a case, but we do not know and want to learn if $X$ caused $Y$ here. So, for instance, we see that a country has a free press, and that its government was removed; but we want to find out whether the free press caused government removal. In other words, we want to know the value of $U_Y^{higher}$. Suppose, further, that we have a theory---a lower-level model---of the effect of $X$ on $Y$ represented by the graph in Figure \\ref{fig:Kinf}. Say, a free press causes government removal through media reports of corruption. What can be learned, then, from observing the moderator, $K$, given this lower-level model?\n\n\n```{r, echo = FALSE, fig.width = 7, fig.height = 5, fig.align=\"center\", out.width='.7\\\\textwidth', fig.cap = \"\\\\label{fig:Kinf} A model with one explanatory variable and one mediator.\"}\n\n\nhj_dag(x = c(1,1,2,2, 1.5, 1.5),\n       y = c(1,2,1,2, 1  , 2),\n       names = c(\n         expression(paste(X)),\n         expression(paste(U[X])),  \n         expression(paste(\"Y\")),  \n         expression(paste(U[Y]^{lower})),\n         expression(paste(K)),\n         expression(paste(U[K])) \n         ),\n       arcs = cbind( c(2,1, 4, 6, 5),\n                     c(1,5, 3, 5, 3)),\n       title = \"Lower level graph:\\nMediation\",\n       add_functions = 0, \n       contraction = .16, \n       padding = .2\n)\n\n\n```\n\n\nObserving $X=1, Y=1$ already tells us that $U_Y^{higher}$, the higher-level type variable for $X$'s effect on $Y$, takes on either $t_{01}$ or $t_{11}$: either $X$ has had a positive effect on $Y$ or it has no effect ($Y$ would be $1$ regardless of $X$'s value). We now want the mediator clue, $K$, to help us distinguish between these possibilities. What could we learn if we went looking for information on $K$ and observed $K=1$? \n\nWhen we combine this observation for $K$ with the observations we already have for $X$ and for $Y$, we can begin to eliminate values for the type nodes in the lower-level model. First, seeing $K=1$ with $X=1$ eliminates two possible values of $K$'s response type, $U_K$: $t_{00}^{K}$ and $t_{10}^{K}$. That is, by observing $X=1$ and $K=1$, we know that $K$ is not fixed at $0$, and we know that $X$ does not have a negative effect on $K$. So, for instance, if we observe a high number of media reports in corruption (along with a free press and government removal), we know (1) that reports of corruption are not fixed at a low level in this case and (2) that a free press does not cause a low level of media reports in this case. \n\nSecond, observing $K=1$ with $Y=1$ allows us to rule out the values $t_{00}^{Y}$ and $t_{10}^{Y}$ for $Y$'s response type node, $U_Y^{lower}$: we have learned that $Y$ is not \"stuck\" at $0$, regardless of $K$'s value; and we have learned that $K$ does not exert a negative effect on $Y$. Observing together many corruption reports and government removal tells us (1) that government removal can occur in this case and (2) that media reports of corruption do not prevent government removal.\n\nSo what have we learned? By eliminating these two pairs of lower-level types we have excluded three possible causal sequences that could be consistent with our initial observation of $X=Y=1$---that is, with the two higher-level types that are in contention ($t_{01}$, or $b$-type, and $t_{11}$), or $d$-type). The first excluded sequence---which would be theoretically consistent with a $d$ response-type at the higher level---is that $K$ is fixed at $0$, followed by $Y$ being $1$ for some other reason. Second, we have ruled out the possibility that---also consistent with a higher-level $d$ type---that $X$ has a negative effect on $K$, followed by no effect of $K$ on $Y$ because $Y$ is fixed at $0$. Third, we have excluded the possibility---consistent with a positive effect of $X$ on $Y$---that $X$ has a negative effect on $K$ and that $K$ in turn has a negative effect on $Y$. \n\nAt this point, however, the learning we have achieved is purely formal, stuck in the realm of lower-level types. While we have eliminated values of lower-level type variables and their associated sequences, doing so does not yet speak to the question of interest: it does not eliminate either of the original two *higher*-level types, $t_{01}$ and $t_{11}$, that could define $Y$'s response to $X$. The observed data ($X=K=Y=1$) are consistent with $X$ having a positive effect on $Y$ via linked positive effects ($u_K=t_{01}^K$ and $u_Y^{lower}=t_{01}^Y$); and they are consistent with $X$ having no effect on $Y$, whether because $K$ is always $1$ ($U_K=t_{11}^K$), because $Y$ is always $1$ ($U_Y^{lower}=t_{11}^Y$), or both. Tracing out a series of events between possible cause and outcome cannot by itself deliver a causal inference. We might observe a free press, corruption reports, and government removal, picking up clues along a possible causal path; but that cannot tell us whether the free press caused government removal. Corruption reports may have happened without a free press, and government removal might have occurred without corruption reports.\n\nTo break the inferential logjam we need to introduce a \"thicker\" base of substantive, prior knowledge about the phenomenon under scrutiny. The DAG alone---the map of possible causal links---is not enough to permit causal inference. We need to impose structure on the problem in a way that allows us to weight some causal possibilities consistent with the data more heavily than other causal possibilities consistent with the data. We can do this by saying something about the probability distributions governing our query nodes: that is, by drawing on prior beliefs about the *probabilities* of the different lower-level types captured by $U_K$ and $U_Y^{lower}$. \n\nTo see how this might work, let us posit some rough probabilities we might have placed, prior to observing $K$, on each of the possible causal effects at the two stages in the causal chain. We can think of these probabilities as the degree of confidence that we might have had in the existence of these effects in this case, given everything we know about the case before we search for the clue. This includes its $X$ and $Y$ values (that it has a free press and its government was removed) as well anything else that might affect the causal process and outcome of interest (say, regime type or economic conditions). We might alternatively think about these probabilities as our beliefs about how common each of these effects are in the  population of cases from which this case has been \"drawn\"---i.e., in cases that are, in relevant ways, like this case (again, using only information about this case that we have prior to observing $K$, including its $X=Y=1$ status). Thinking about things in this second way, we can thus think of the probabilities as *shares* of the population containing each type. Using the $\\lambda$ notation introduced in Chapter 2 to denote population-level type shares, we can thus write our belief about the probability of a case's being of type $t_{ij}^K$ as $\\lambda_{ij}^K$, and of type $t_{ij}^Y$ as $\\lambda_{ij}^Y$. Of course, the beliefs that we posit here are intended to be purely illustrative. \n\n**WE SHOULD BUILD AND LINK TO AN ONLINE WIDGET THAT ALLOWS PEOPLE TO ADJUST THESE PRIORS AND SEE EFFECT ON POSTERIORS.**\n\n**$U_K$: $X$'s effect on $K$**^[The proportions across types for a given node must sum to $1$ as we are characterizing a distribution across the variable's full range.]\n\n  * $t_{10}^{K}$: A free press reduces the number of media reports of corruption in this case: **Very unlikely/uncommon (say, $\\lambda^K_{10}=0.1$)**^[We think it very unlikely that governmental restrictions on the press could generate more reporting on malfeasance by the government itself.]\n  \n  * **$t_{00}^{K}$**: The number of media reports of corruption will be low in this case regardless of whether the press is free: **Somewhat likely/common ($\\lambda^K_{00}=0.4$)**^[There might be factors other than government restrictions that could prevent the press from reporting on public corruption.]\n  \n  * **$t_{01}^{K}$**: A free press increases the number of media reports of corruption in this case: **Somewhat likely/common ($\\lambda^K_{01}=0.4$)**^[Government restrictions might plausibly prevent the press from reporting on corruption that would otherwise report on.]\n \n  * **$t_{11}^{K}$**: The number of media reports of corruption will be high in this case regardless of whether the press is free: **Very unlikely/uncommon ($\\lambda^K_{11}=0.1$)**^[It seems unlikely that a press controlled by the government itself could report regularly on governmental corruption.]\n  \n\n**$U_Y^{lower}$: $K$'s effect on $Y$**^[The probabilities across types for a given node must sum to 1.0 since we are characterizing the distribution across the variable's full range.]\n\n  * **$t_{10}^{Y}$**: A high number of media reports of corruption prevents government removal in this case: **Rather unlikely/uncommon ($\\lambda^Y_{10}=0.2$)**^[We might for the most part think it unlikely that more reporting on corruption could positively enhance a government's staying power. However, we allow for a non-trivial possibility that corruption reports might lead a government to respond to the threat with repressive measures that weaken any potential opposition.]\n \n  * **$t_{00}^{Y}$**: Government removal will not happen in this case regardless of the number of media reports of corruption. **Impossible/excluded** from relevant population since we have already observed $Y=1$ before we do the process tracing: **($\\lambda^Y_{00}=0$)**.\n\n  * **$t_{01}^{Y}$**: A high number of media reports of corruption causes government removal in this case: **More likely/common than not ($\\lambda^Y_{01}=0.6$)**^[We believe that governments usually difficulty surviving in the face of public evidence of rampant corruption.]\n  \n  * **$t_{11}^{Y}$**: Government removal will happen in this case regardless of the number of media reports of corruption: **Rather unlikely/uncommon ($\\lambda^Y_{11}=0.2$)**^[We might for the most part think it unlikely that a government of this kind will fall in the absence of public reports of major malfeasance; but we allow for a non-trivial possibility that other forces could topple the government without corruption reports.]\n\nThese prior beliefs about lower-level causal effects imply a prior belief about the higher-level effect---that is, about whether $X$ has a positive effect on $Y$ in this case. We know that we can only get a positive causal effect at the higher level if we have either linked positive effects ($u_K=t_{01}^{K}$ and $u_Y^{lower}=t_{01}^{Y}$) or linked negative effects ($u_K=t_{10}^{K}$ and $u_Y^{lower}=t_{10}^{Y}$). So, to calculate the prior probability of a positive higher-level effect in our case, we simply need to calculate the share of cases in the population that have  either of these two combinations of values as a proportion of the share of cases in the population containing *all* combinations of types that are consistent with our prior observation of $X=1, Y=1$.^[Formally, for case $i$, $$P(u_i^{higher} = t^{higher}_{01}| X=Y=1 ) = \\frac{\\lambda_{01}^{K}\\lambda_{01}^{Y} + \\lambda_{10}^{K}\\lambda_{10}^{Y}}\n {\\lambda_{01}^{K}\\lambda_{01}^{Y} + \\lambda_{10}^{K}\\lambda_{10}^{Y}  +\\lambda_{11}^{Y} + \\lambda_{00}^{K}\\lambda_{10}^{Y} + \\lambda_{11}^{K}\\lambda_{01}^{Y}}$$ Note that this calculation assumes independence between $U_K$ and $U_Y^{lower}$. In our DAG, conditional on observing $X$ and $Y$, $U_K$ is d-connected to $U_Y^{lower}$, meaning that there is a conditional dependency between them. However, we have taken this dependency into account by only permitting those combinations of values that are consistent with the prior observation $X=Y=1$. For instance, having observed $Y=1$, we know that if $U_K=t^K_11$ or $U_K=t^K_01$, then $U_Y^{lower}$ cannot take on $t^Y_{10}$ since these combinations would yield $Y=0$. These exclusions restore independence between $U_K$ and $U_Y^{lower}$.] This calculation yields a prior probability of a positive causal effect of $0.433$.\n \nNow, conducting the process tracing and observing $K$ shifts our beliefs. As noted, observing $K=1$ allows us to eliminate from contention---i.e., assign a 0 posterior probability to---the three lower-level types $t_{00}^{K}$, $t_{10}^{K}$, and $t_{10}^{Y}$ inconsistent with this observation (with $t_{00}^{Y}$ eliminated already by seeing $Y=1$). In calculating our *posterior* probability of a positive higher-level effect, we again take the share of cases containing combinations of types consistent with a positive higher-level effect as a proportion of all combinations---but this time *excluding* any combination containing an eliminated lower-level type.^[$$P(u_i^{higher} = t^{higher}_{01}| X=Y=K=1 ) = \\frac{\\lambda_{01}^{K}\\lambda_{01}^{Y}}\n {\\lambda_{01}^{K}\\lambda_{01}^{Y} + \\lambda_{11}^{Y} + \\lambda_{11}^{K}\\lambda_{01}^{Y}}$$.] This calculation gives a posterior probability that $X$ had a positive on $Y$ in this case of $0.48$--a small upward shift in our belief in this effect. \n \nWe thus still think it is slightly more likely that the government would have been replaced anyway than that its removal was caused by the free press. However, we are now more confident that the free press mattered than we were before we observed the media reports of corruption. \n\nIf we were to look for the clue and instead find $K=0$---few media reports of corruption---then our beliefs would shift in the opposite direction. We would now remove from contention all those types in which $K$ is $1$ when $X=1$ or in which $Y=0$ when $K=0$: $t_{01}^{K}$, $t_{11}^{K}$, and $t_{11}^{Y}$ (again, with $t_{00}^{Y}$ previously excluded). The resulting calculation---the share represented by the remaining combinations of types consistent with a positive higher-level effect divided by all remaining combinations of types---now generates a dramaticaly lower posterior probability of $0.067$ that $X$ had a positive effect on $Y$. Having observed few media reports of corruption, we now infer it to be highly unlikely that the free press caused government removal in this case.\n\nThe reason we are able to learn from the clue in this setup is that we have brought prior knowledge to the problem in a way that allows us to weight some possibilities consistent with the data more heavily than others. Based on what we already knew about the phenomenon of interest, we consider some lower-level response types more likely than others before we search for the clue. Then if we see the clue ($K=1$), we eliminate both some lower-level causal types that are consistent with a positive $X \\rightarrow Y$ causal effect ($t_{10}^{K}$ and $t_{10}^{K}$) and a causal type that would yield zero effect ($t_{00}^{K}$). However, the zero-effect type that we eliminate was one that we had initially (before the process tracing) considered somewhat likely; while the positive-effect types that the clue eliminates were ones that prior knowledge had already told us were quite unlikely. The net result is to leave us with a set of beliefs a little less favorable to a zero-effect conclusion than where we had started. If instead we see $K=0$, in contrast, we must eliminate from contention that causal sequence---linked positive effects of $X$ on $K$ and of $K$ on $Y$---which we thought the much more likely route to a positive causal effect. The only sequence consistent with the $X, Y, K$ data that could generate a positive effect---the free press reduces corruption reports, which causes government removal---is one that prior knowledge already told us was very unlikely. Meanwhile, the no-effect types that we have eliminated were among those that we thought least likely to begin with. We thus end up with a belief set heavily tilted in favor of no effect.\n\nThe most general point here is that it is our prior knowledge, embedded in a probabilistic causal model, that tells us the probative value of possible observations. As readers familiar with Van Evera's test types will have noticed, the clue in the above example, given the prior beliefs we specified, functions as a \"hoop test\" for the claim that $X=1$ caused $Y=1$ (@Van-Evera:1997): finding the clue only slightly strengthens our confidence in the claim while failing to find the clue greatly weakens that confidence. \n\nYet the particular way in which we can learn from a clue---and whether we can learn from it at all---will depend on the prior probabilities that we place on possible values of the query nodes. To take an extreme example, $K$ would be ``doubly decisive'' if our prior beliefs are:\n \n 1. $\\lambda^K_{01}, \\lambda^Y_{01}>0$:  It is possible that $X=1$ causes $K=1$, which in turn causes $Y=1$,\n 2. $\\lambda^K_{10}=0$ or $\\lambda^Y_{10}=0$: $X=1$ can only cause $Y=1$ by first causing $K=1$, and so seeing $K=0$ would be sure evidence that $X$ did not cause $Y$; and \n 3. $\\lambda^Y_{11}=0$ and $\\lambda^K_{11}=0$:  It is impossible that $K$ would be 1 no matter what the value of $X$, or that $Y$ would be 1 no matter what the value of $K$\n \nUnder these starting beliefs, observing the clue would lead us believe with certainly that $X$ had a positive effect on $Y$; and not finding the clue when we look for it would lead us to believe with certainty that $X$ had no effect. \nAt the other extreme, the clue would be completely uninformaive about $X$'s causal effect if we began with these beliefs:\n\n1. $\\lambda_{10}^{K}\\lambda_{10}^{Y} = \\lambda_{01}^{K}\\lambda_{01}^{Y}$: a positive $X \\rightarrow Y$ effect is equally likely to operate via linked positive effects as it is via linked negative effects, and\n2. $\\lambda_{00}^{K}\\lambda_{10}^{Y} = \\lambda_{11}^{K}\\lambda_{01}^{Y}$: In those situations in which $X$ does not affect $K$, but $K$ produces $Y=1$, $K$ is just as likely to be fixed at 0 as it is to be fixed at 1.\n\nUnder these starting beliefs, our beliefs about $X$'s causal effect will not shift, whether we find the clue or not.\n\n<!-- Under these beliefs, before observing $K$, our degree of confidence that the higher-level type is $t_{01}^{higher}$---that is that the case is a $b$ type---is:   -->\n\n<!--  $$P(t_i^{higher} = t^{higher}_{01}| X=Y=1 ) = \\frac{\\lambda_{01}^{K}\\lambda_{01}^{Y} + \\lambda_{10}^{K}\\lambda_{10}^{Y}} -->\n<!--  {\\lambda_{01}^{K}\\lambda_{01}^{Y} + \\lambda_{10}^{K}\\lambda_{10}^{Y}  +\\lambda_{11}^{Y} + \\lambda_{00}^{K}\\lambda_{10}^{Y} + \\lambda_{11}^{K}\\lambda_{01}^{Y}}$$  -->\n\n<!--  Then, if we observe $K=1$, the posterior is: -->\n\n<!--  $$P(t_i^{high}= t^{high}_{01} | X=Y=1, K=1 ) = \\frac{\\lambda_{01}^{K}\\lambda_{01}^{Y} } -->\n<!--  {\\lambda_{01}^{K}\\lambda_{01}^{Y}  +\\lambda_{11}^{Y} +  \\lambda_{11}^{K}\\lambda_{01}^{Y}}$$  -->\n\n<!--  After observing $K=0$ the posterior is: -->\n\n<!--  $$P(t_i^{high}= t^{high}_{01} | X=Y=1, K=0 ) = \\frac{\\lambda_{10}^{K}\\lambda_{10}^{Y} } -->\n<!--  {\\lambda_{10}^{K}\\lambda_{10}^{Y}  +\\lambda_{11}^{Y} +  \\lambda_{00}^{K}\\lambda_{10}^{Y}}$$  -->\n\n\n<!-- Thus updating is possible. For this strategy to work however, $K$ must vary independently of $X$---that is, $u_K$ has to matter. To see why, imagine that we knew in advance that $X$ fully determined $K$. In this situation, the clue has no probative value since it takes on the same value whether there was a causal effect of $X$ on $Y$ or not; observing $X$ would itself already provide full information on $K$. Put differently, all variation in causal effects across cases would derive from  $U_Y^{lower}$, and once $X$ has been observed, $K$ would provide no further information about $U_Y$.  Graphically, the region in which $X=1, K=1$ would be orthogonal to the types region. -->\n\nWe are providing here an account of process tracing focused on studying links in the causal chain between cause and effect. It is sometimes argued that studying a causal effect by breaking it down into component steps merely shifts the challenge of causal inference to a lower level of aggregation, and in fact multiplies it, by forcing us to identify causal effects at each link in the posited causal chain. The understanding of mediation-based process tracing that we are developing here makes clear why this view is misguided, at least insofar as we are willing to make our inferences conditional on background knowledge. When we invoke a more detailed lower-level model to investigate a higher-level effect, we are putting our prior beliefs to work to allow for greater learning than might otherwise occur. As we discussed in Chapter X, a lower-level model of mediation can be thought of as partitioning an unobservable quantity---the higher-level response node, $U_Y^{higher}$---into a potentially observable quantity $K$ (the mediator clue) and an unobservable quantity, $u_Y^{lower}$ ($Y$'s lower-level response type). Rather than addressing a query about $Y$'s response to $X$, our task is now to address two queries: one about $K$'s response to $X$ and another about $Y$'s response to $K$. Importantly, this move will not always be helpful. But it will be fruitful is when we have *stronger prior beliefs about smaller, intermediate links in a causal chain than about the $X \\rightarrow Y$ relationship taken as a whole.* We are arguably often in this situation. To return to our running example involving government replacement, we may not know much about the frequency with which a free press makes government replacement more likely. However, we may have some prior knowledge indicating that a free press increases reports of government corruption more often than it has no effect; and that greater reports of corruption are more likely to reduce governments' survival in office than to leave their survival prospects untouched. It is precisely those differential weights that we are able to put on causal effects at the lower-level---and not for the higher-level claim of interest---that allow the theory of mediation, and the empirical strategy that it suggests, to be informative. \n\nMore formally, we can use the concept of gains to theory, discussed in Chapter X, to asks: what is the value-added of the lower-level model of mediation, relative to the higher model? How much do we expect to learn from the research design---go and observe ($K$)---that the lower-level model suggests to us? \n\nSupposing that we have a theory of mediation with a single mediator, we can calculate the expected error without information on the clue, $P(t_{01}^{higher})(1-P(t_{01}^{higher}))$, as:\n\n $$\\text{Prior variance} = \\frac{\\left(\\lambda_{01}^{K}\\lambda_{01}^{Y} + \\lambda_{10}^{K}\\lambda_{10}^{Y}\\right)\\left(\\lambda_{11}^{Y} + \\lambda_{00}^{K}\\lambda_{10}^{Y} + \\lambda_{11}^{K}\\lambda_{01}^{Y}\\right)}\n {\\left(\\lambda_{01}^{K}\\lambda_{01}^{Y} + \\lambda_{10}^{K}\\lambda_{10}^{Y}  +\\lambda_{11}^{Y} + \\lambda_{00}^{K}\\lambda_{10}^{Y} + \\lambda_{11}^{K}\\lambda_{01}^{Y}\\right)^2}$$ \n\nWe can then calculate the expected error after with information on $K$, over the possible values of $K$ that we might see, as:\n\n<!-- $$\\left(\\lambda_{01}^{K}+\\lambda_{11}^{K}\\right) \\left(\\frac{\\lambda_{01}^{K}\\lambda_{01}^{Y} } -->\n<!--  {\\lambda_{01}^{K}\\lambda_{01}^{Y}  +\\lambda_{11}^{Y} +  \\lambda_{11}^{K}\\lambda_{01}^{Y}}\\right) -->\n<!--  \\left(1-\\frac{\\lambda_{01}^{K}\\lambda_{01}^{Y} } -->\n<!--  {\\lambda_{01}^{K}\\lambda_{01}^{Y}  +\\lambda_{11}^{Y} +  \\lambda_{11}^{K}\\lambda_{01}^{Y}}\\right) -->\n<!--  + -->\n<!--  \\left(1-\\lambda_{01}^{K}-\\lambda_{11}^{K}\\right) \\left(\\frac{\\lambda_{10}^{K}\\lambda_{10}^{Y} } -->\n<!--  {\\lambda_{10}^{K}\\lambda_{10}^{Y}  +\\lambda_{11}^{Y} +  \\lambda_{00}^{K}\\lambda_{10}^{Y}}\\right) -->\n<!-- \\left( 1- \\frac{\\lambda_{10}^{K}\\lambda_{10}^{Y} } -->\n<!--  {\\lambda_{10}^{K}\\lambda_{10}^{Y}  +\\lambda_{11}^{Y} +  \\lambda_{00}^{K}\\lambda_{10}^{Y}}\\right) $$  -->\n\n $$\\text{Expected Posterior Var} = \\frac{\\left(\\lambda_{01}^{K}+\\lambda_{11}^{K}\\right) \\lambda_{01}^{K}\\lambda_{01}^{Y}\\left(\\lambda_{11}^{Y}+\\lambda_{11}^{K}\\lambda_{01}^{Y} \\right) }\n {\\left(\\lambda_{01}^{K}\\lambda_{01}^{Y}  +\\lambda_{11}^{Y} +  \\lambda_{11}^{K}\\lambda_{01}^{Y}\\right)^2}+\n \\frac{\\left(1-\\lambda_{01}^{K}-\\lambda_{11}^{K}\\right)\\lambda_{10}^{K}\\lambda_{10}^{Y}\\left(\\lambda_{11}^{Y} +  \\lambda_{00}^{K}\\lambda_{10}^{Y}\\right) }\n {\\left(\\lambda_{10}^{K}\\lambda_{10}^{Y}  +\\lambda_{11}^{Y} +  \\lambda_{00}^{K}\\lambda_{10}^{Y}\\right)^2}$$ \n \nTo assess the gains provided by the lower-level theory, we can simply compare the first of these to the second: the expected error assuming no search for $K$ with the expected error assuming a search for $K$. The greater the reduction in expected error from searching for $K$, the more useful the lower-level theory has been. As can be seen from the above expressions, how much our expected error is reduced by $K$---that is, how much we expect to learn from this empirical strategy---will depend upon the $\\lambda$ values: that is, on the prior probabilities that we place on the lower-level response nodes, ($U_K$ and $U_Y^{lower}$. \n\nProcess-tracing research design, thus, can flow directly from our models of the world. This can also be seen clearly if we write down the probability of observing $K=1$ conditional on causal type and $X$, using the $\\phi$ notation from Humphreys and Jacobs (2015) and introduced in Chapter X. Here $\\phi_{jx}$ refers to the probability of observing a clue in a case of type $j$ when $X=x$. Starting with our prior distribution over the lower-level causal types (the $\\lambda$'s), we can derive, for an $X=1$ case, the probability of seeing the clue if the case is of type $b$ (positive effect) or of type $d$ (no effect, $Y$ always $1$):\n\n\\begin{eqnarray*}\n\\phi_{b1} &=& \\frac{\\lambda_{01}^{K}\\lambda_{01}^{Y}}{\\lambda_{01}^{K}\\lambda_{01}^{Y}+\\lambda_{10}^{K}\\lambda_{10}^{Y}}\\\\ \n\\phi_{d1} &=& \\frac{\\lambda_{11}^{Y}(\\lambda_{01}^{K}+\\lambda_{11}^{K})+\\lambda_{11}^{K}\\lambda_{01}^{Y}}{\\lambda_{11}^{Y} + \\lambda_{00}^{K}\\lambda_{10}^{Y} + \\lambda_{11}^{K}\\lambda_{01}^{Y}}\n\\end{eqnarray*}\n\nThese quantities allow for easy mapping between our prior beliefs about our causal query---as expressed in the lower level model---and the classic process-tracing tests in @Van-Evera:1997. Figure \\ref{fig:phis} illustrates. In each panel, we manipulate a prior for one or more of the lower-level causal effects, keeping all other priors flat, and we see how probative value changes. As the curves for $\\phi_b$ and $\\phi_d$ diverge, probative value is increasing since there is an increasing difference between the probability of seeing the clue if $X$ has a positive effect on $Y$ and the probability of seeing the lcue if $X$ has no effect. \n\nIn the left panel, we see that as we place a lower prior probability on $K$'s being negatively affected by $X$,^[For a given value of $\\lambda^K_{01}$, we hold the other $\\lambda^K$ values equal by assigning a value of $(1-\\lambda^K_{01})/3$ to each.] seeking $K=1$ increasingly takes on the quality of a hoop test for $X$'s having a positive effect on $Y$. The clue, that is, increasingly becomes something we must see if $X$ positively affects $Y$, with the clue remaining moderately probable if there is no effect. Why? The less likely we believe it is that $K=0$ was caused by $X=1$, the less consistent the observation of $K=0$ is with $X$ having a positive causal effect on $Y$ via $K$ (since, to have such an effect, if $X=1$ and $K=0$, would precisely have to mean that $X=1$ *caused* $K=0$). \nIn the second graph, we simultaneously change the prior probabilities of zero effects at both stages in the sequence: of $K$ and $Y$ being $1$ regardless of the values of $X$ and $K$, respectively.^[For a given value of $\\lambda^K_{11}$, we hold the other $\\lambda^K$'s equal by assigning a value of $(1-\\lambda^K_{11})/3$ to each; likewise for $\\lambda^Y_{11}$ and the other $\\lambda^Y$ values.] We see here that, as the probabilities of zero effects jointly diminish, seeking $K=1$ increasingly becomes a smoking-gun test for a positive effect of $X$ on $Y$: the probability of seeing the clue if the case is a $d$ type diminishes. The reason is that, as zero effects at the lower level become less likely, it becomes increasingly unlikely that $K=1$ could have occurred without a positive effect of $X$ on $K$, and that $Y=1$ could have occurred (given that we have seen $K=1$) without a posiitve effect of $K$ on $Y$.\n\n<!-- This example also helps clarify the kind of theoretical knowledge required for drawing inferences from clues. As we have emphasized, the structural equations comprising a causal model can be fully non-parametric. As the example illustrates, $U_Y$ can be a type variable that determines different the equation for an endogenous variable in a causal model can  can take the form of beliefs about the proportions of  -->\n\n\n```{r, echo = FALSE,  fig.align=\"center\", out.width='.85\\\\textwidth', fig.width = 9, fig.height = 4, fig.cap = \"\\\\label{fig:phis} The probability of observing $K$ given causal type for different beliefs on lower-level causal effects. In the left figure, priors on all lower-level causal effects are flat except for the probability that $X$ has a negative effect on $K$. If we believe that it is unlikely that $X$ has a negative effect on $K$, $K$ becomes a `hoop' test for the proposition that a case is of type $b$. The righthand figure considers simultaneous changes in $\\\\lambda_{11}^K$ and  $\\\\lambda_{11}^Y$---the probabilities that $K=1$ regardless of $X$, and that $Y=1$  regardless of $K$, with flat distributions on all other lower-level effects. With $\\\\lambda_{11}^K$, $\\\\lambda_{11}^Y$ both close to 0, $K$ becomes a 'smoking gun' test for the proposition that $X$ has a positive effect on $Y$ ($b$ type).\", errors = FALSE, warning = FALSE, message = FALSE, comment = FALSE}\n\n# sim_data <- function(sims=100, pX = 1, K_eventprobs = c(.25,.25,.25,.25), Y_eventprobs = c(.25,.25,.25,.25)){\n#   X <- rmultinom(sims, 1, c(1-pX, pX))\n#   X <- t(X) %*% (0:1)  \n#   uK <- rmultinom(sims, 1, K_eventprobs) \n#   uK <- t(uK) %*% (1:4)  \n#   uY <- rmultinom(sims, 1, Y_eventprobs) \n#   uY <- t(uY) %*% (1:4)  \n#   K <- (uK==1)*(X==0) +  (uK==2)*(X==1) +  (uK==4)\n#   Y <- (uY==1)*(K==0) +  (uY==2)*(K==1) +  (uY==4)\n#   \n#   a_higher <- (uK==2)*(uY==1) + (uK==1)*(uY==2)\n#   b_higher <- (uK==2)*(uY==2) + (uK==1)*(uY==1)\n#   c_higher <- (uY == 3) + (uK==3)*(uY==2) + (uK==4)*(uY==1)\n#   d_higher <- (uY == 4) + (uK==3)*(uY==1) + (uK==4)*(uY==2)\n#   \n#   data.frame(X, K, Y, a_higher, b_higher, c_higher, d_higher)\n#  }\n# \n# \n# pv <- function(sims=100, K_eventprobs = c(.25,.25,.25,.25), Y_eventprobs = c(.25,.25,.25,.25)){\n#   D <- sim_data(sims = sims, K_eventprobs = K_eventprobs, Y_eventprobs = Y_eventprobs)  \n#   c(mean(D$K[D$X==1 & D$Y==1 & D$b_higher ==1]), mean(D$K[D$X==1 & D$Y==1 & D$d_higher ==1]))\n# }\n\npv_analytic_b <- function(K_eventprobs = c(.25,.25,.25,.25), Y_eventprobs = c(.25,.25,.25,.25))\n  {K_eventprobs[2]*Y_eventprobs[2]}/{K_eventprobs[2]*Y_eventprobs[2]+K_eventprobs[1]*Y_eventprobs[1]}\n\npv_analytic_d <- function(K_eventprobs = c(.25,.25,.25,.25), Y_eventprobs = c(.25,.25,.25,.25))\n  {Y_eventprobs[4]*(K_eventprobs[2]+K_eventprobs[4])+K_eventprobs[4]*Y_eventprobs[2]}/{Y_eventprobs[4]+K_eventprobs[3]*Y_eventprobs[1]+K_eventprobs[4]*Y_eventprobs[2]}\n\npar(mfrow = c(1,2))\n\nplot(seq(0, .25, .01), sapply(seq(0, .25, .01), function(i) pv_analytic_b(K_eventprobs = c(i,(1-i)/3,(1-i)/3,(1-i)/3), Y_eventprobs = c(1/4,1/4,1/4,1/4))), type = \"l\", xlab = expression(paste(lambda[10]^{K})), ylab = \"\", main = \"A Hoop Test\", ylim = c(0,1))\n\npoints(seq(0, .25, .01), sapply(seq(0, .25, .01), function(i) pv_analytic_d(K_eventprobs = c(i,(1-i)/3,(1-i)/3,(1-i)/3), Y_eventprobs = c(1/4,1/4,1/4,1/4))), type = \"l\", lty = 2)\ntext(.1,.8, expression(phi[b]))\ntext(.1,.5, expression(phi[d]))\n\n\nplot(seq(0, .25, .01), sapply(seq(0, .25, .01), function(i) pv_analytic_b(K_eventprobs = c((1-i)/3,(1-i)/3,(1-i)/3, i), Y_eventprobs = c((1-i)/3,(1-i)/3,(1-i)/3, i))), ylim = c(0,1), type = \"l\", xlab = expression(paste(lambda[11]^{K},\" and \", lambda[11]^{Y})), ylab = \"\", main = \"A Smoking Gun Test\")\n\npoints(seq(0, .25, .01), sapply(seq(0, .25, .01), function(i) pv_analytic_d(K_eventprobs = c((1-i)/3,(1-i)/3,(1-i)/3, i), Y_eventprobs = c((1-i)/3,(1-i)/3,(1-i)/3, i))),  type = \"l\", lty = 2)\ntext(.1,.55, expression(phi[b]))\ntext(.1,.25, expression(phi[d]))\n\n```\n\n<!-- Strikingyly, the probative value  of the mediator clue is higher the lower the certainty with which $X$ causes the mediator or the mediator causes $Y$.  -->\n\n\n### Inference from a lower level model of moderating effects\nReturn now to the moderation example from Chapter 4, graphed here in Figure \\ref{fig:modinf}. Here $Y$ is directly affected by both $X$ and a second variable, $K$, which could be a potential clue. As we discuss in Chapter 4, when we theorize moderation of an $X \\rightarrow Y$ effect, we are again partitioning the higher-level response node, $U_Y^{higher}$, into an unobservable $U_Y^{lower}$ and a potentially observable ($K$) component. Whereas $Y$'s response to $X$ in the higher-level model is represented by a non-substantive response-node, we have now framed $Y$'s response in part in terms of a substantive concept that we believe conditions $X$'s effect. \n\nAnother important difference between a higher-level $X \\rightarrow Y$ model and a lower-level model of moderation is that $U_Y^{lower}$, in the latter model, determines $Y$'s response to *both* $X$ and $K$ (whereas $U_Y^{higher}$ only determines $Y$'s response to $X$). We can think of $U_Y^{lower}$, thus, as fully defining the interaction between $X$ and $K$: how, if at all, does $X$'s effect depend on $K$; and how, if at all, does $K$'s effect depend on $X$? \n\nUsing the notation from Chapter 4, we write a value for $U_Y^{lower}$ in a moderation model in the form $t_{ij}^{gh}$, where the four sub- and superscripts provide $Y$'s potential outcomes for the four possible combinations of $X$ and $K$ values. We arrange the sub- and superscripts to give them an easy visual interpretation, with $X=0, K=0$ being in the lower lefthand corner ($i$); $X$ going to $1$ as we move to the right; and $K$ going to $1$ as we move up. In other words, a unit has outcome $i$ when $X=0, K=0$, $j$ when $X=1, K=0$, $g$ when $X=0, K=1$, $h$ when $X=1, K=1$. $U_Y^{lower}$ in this graph denotes a multinomial distribution over the sixteen possible values of $t_{ij}^{gh}$. We represent all 16 types for $U_Y^{lower}$ in Table \\ref{typesmoderation}. Parallel to our notation for mediation, we further represent the population-level shares of the response types as $\\lambda_{ij}^{gh}$.\n\n\n\\begin{table}[h!]\n  \\centering\n  \\def\\arraystretch{1.3}\n    \\begin{tabular}{ccccccc}\n    \\hline\n    \\textbf {} & \\textbf {Type} &  $(Y | X=0,$ & $(Y |X=0, $ & $(Y | X=1, $ & $(Y | X=1, $  & Prior\\\\\n         & & $K=0)$ & $K=1)$ & $K=0)$ & $K=1)$ & ($\\lambda_{ij}^{gh}$) \\\\  \\hline\n    1 & $t_{00}^{00}$ \t\t\t&  0     & 0     & 0     & 0  & 0 \\\\\n    2 & $t_{00}^{01}$ \t& 0     & 0     & 0     & 1 & 0.01 \\\\\n    3 & $t_{01}^{00}$ \t& 0     & 0     & 1     & 0 & 0.2 \\\\\n    4 & $t_{01}^{01}$ \t\t\t& 0     & 0     & 1     & 1 & 0.1 \\\\\n    5 & $t_{00}^{10}$ \t& 0     & 1     & 0     & 0 & 0 \\\\\n    6 & $t_{00}^{11}$ \t\t\t& 0     & 1     & 0     & 1 & 0.01 \\\\\n    7 & $t_{01}^{10}$ \t& 0     & 1     & 1     & 0 & 0.2 \\\\\n    8 & $t_{01}^{11}$ \t\t& 0     & 1     & 1     & 1 & 0.2 \\\\\n    9 & $t_{10}^{00}$\t\t\t& 1     & 0     & 0     & 0 & 0 \\\\\n    10 & $t_{10}^{01}$ \t& 1     & 0     & 0     & 1 & 0.01 \\\\\n    11 & $t_{11}^{00}$\t\t\t& 1     & 0     & 1     & 0 & 0.1 \\\\\n    12 & $t_{11}^{01}$\t\t& 1     & 0     & 1     & 1 & 0.01 \\\\\n    13 & $t_{10}^{10}$\t\t\t& 1     & 1     & 0     & 0 & 0 \\\\\n    14 & $t_{10}^{11}$\t\t& 1     & 1     & 0     & 1 & 0.01 \\\\\n    15 & $t_{11}^{10}$\t\t& 1     & 1     & 1     & 0 & 0.1 \\\\\n    16 & $t_{11}^{11}$\t\t\t& 1     & 1     & 1     & 1 & 0.05 \\\\\n    \\bottomrule\n    \\end{tabular}%\n   \\caption{The table defines the 16 values (response types) that $U_Y^{lower}$ can take on, given a binary $X$ and $K$ as parents of $Y$. The `Type' column lists each of the 16 values, while the four columns to its right define each value in terms of the potential outcomes that it implies. The prior represents a possible set of beliefs, before seeing $K$, about the probability that an $X=Y=1$ case is of each type. Rows with a prior of 0 are types that are logically impossible for an $X=Y=1$ case. Row numberings are provided for ease of reference in the text.}\n  \\label{typesmoderation}%\n\\end{table}%\n\n\n<!-- begin{table}[h!] -->\n<!--   \\centering -->\n<!--   \\def\\arraystretch{1.3} -->\n<!--     \\begin{tabular}{cccccccl} -->\n<!--     \\hline -->\n<!--     \\textbf {} & \\textbf {Type} &  $(Y | X=0,$ & $(Y |X=0, $ & $(Y | X=1, $ & $(Y | X=1, $  & Prior\\\\ -->\n<!--          & & $K=0)$ & $K=1)$ & $K=0)$ & $K=1)$ & (Pop. share, $\\lambda_{ij}^{gh}) \\\\  \\hline -->\n<!--     1 & $t_{00}^{00}$ \t\t\t&  0     & 0     & 0     & 0  & 0 & (Excluded by $X,Y$ observation)\\\\ -->\n<!--     2 & $t_{00}^{01}$ \t& 0     & 0     & 0     & 1 & 0.01 & FP $\\rightarrow$ removal if sensitive; sensitivity $\\rightarrow$ removal if FP\\\\ -->\n<!--     3 & $t_{01}^{00}$ \t& 0     & 0     & 1     & 0 & 0.2 & FP $\\rightarrow$ removal if insensitive; no effect if sensitive\\\\ -->\n<!--     4 & $t_{01}^{01}$ \t\t\t& 0     & 0     & 1     & 1 & 0.1 & FP always $\\rightarrow$ removal\\\\ -->\n<!--     5 & $t_{00}^{10}$ \t& 0     & 1     & 0     & 0 & 0 & (Excluded by $X,Y$ observation)\\\\ -->\n<!--     6 & $t_{00}^{11}$ \t\t\t& 0     & 1     & 0     & 1 & 0.01 & Never FP effect; sensitivity always $\\rightarrow$ removal\\\\ -->\n<!--     7 & $t_{01}^{10}$ \t& 0     & 1     & 1     & 0 & 0.2 & FP $\\rightarrow$ removal if insensitive; FP prevents removal if sensitive\\\\ -->\n<!--     8 & $t_{01}^{11}$ \t\t& 0     & 1     & 1     & 1 & 0.2 & {}\\\\ -->\n<!--     9 & $t_{10}^{00}$\t\t\t& 1     & 0     & 0     & 0 & 0 & {}\\\\ -->\n<!--     10 & $t_{10}^{01}$ \t& 1     & 0     & 0     & 1 & 0.01 & {}\\\\ -->\n<!--     11 & $t_{11}^{00}$\t\t\t& 1     & 0     & 1     & 0 & 0.1 & {}\\\\ -->\n<!--     12 & $t_{11}^{01}$\t\t& 1     & 0     & 1     & 1 & 0.01 & {}\\\\ -->\n<!--     13 & $t_{10}^{10}$\t\t\t& 1     & 1     & 0     & 0 & 0 & {}\\\\ -->\n<!--     14 & $t_{10}^{11}$\t\t& 1     & 1     & 0     & 1 & 0.01 & {}\\\\ -->\n<!--     15 & $t_{11}^{10}$\t\t& 1     & 1     & 1     & 0 & 0.1 & {}\\\\ -->\n<!--     16 & $t_{11}^{11}$\t\t\t& 1     & 1     & 1     & 1 & 0.05 & {}\\\\ -->\n<!--     \\bottomrule -->\n<!--     \\end{tabular}% -->\n<!--    \\caption{The table defines the 16 values (response types) that $U_Y^{lower}$ can take on, given a binary $X$ and $K$ as parents of $Y$. The `Type' column lists each of the 16 values, while the four columns to its right define each value in terms of the potential outcomes that it implies. The prior represents a possible set of beliefs, before seeing $K$, about the probability that an $X=Y=1$ case is of each type. Rows with a prior of 0 are types that are logically impossible for an $X=Y=1$ case. Row numberings are provided for ease of reference in the text.} -->\n<!--   \\label{typesmoderation}% -->\n<!-- \\end{table}% -->\n\nSuppose, again, that we have observed $X=Y=1$ in a case and want to know whether $X=1$ caused $Y=1$. How can observing $K$, the moderator, help? In this setup, we can think of learning about $K$ as shaping inferences through two channels. When we observe a moderator clue, we learn (1) about the laws governing the case and (2) about the case being governed by those laws. Or, mapped onto Figure \\ref{typesmoderation}, we learn from a moderator clue both about (1) which row we are in and (2) which pair of columns we are in. \n \nFirst, observing the moderator clue together with $X$ provides information about the value of $u_Y^{lower}$---that is, which row in Figure \\ref{typesmoderation} the case belongs to. It tells us something, in other words, about how the case *would* respond in different contingencies. In fact, before we even observe $K$, we can eliminate rows---values of $U_Y^{lower}$---based on having observed $X$, given that we know $Y$. (This stands to reason, given that $X$ is $d$-connected to $U_Y^{lower}$ by $Y$.) Specifically, observing $X=1$ with $Y=1$ allows us to eliminate any types in which $Y$ is always $0$ when $X=1$: specifically, the types in rows 1, 5, 9, and 13. \n\nThen, observing $K$ allows us to go further. If we observe, $K=1$ (having already observed $X=Y=1$), then we are now able to rule out additional types: specifically, those in which $Y=0$ if $X=K=1$. These are the types in rows 3, 7, 11, and 15. (Alternatively, we if observe $K=0$, we eliminate any type for which $Y=0$ when $X=1$ and $K=0$, namely the types in rows 2, 6, 10, and 14.)\n\nThus, the case's possible response types upon observing $X=Y=K=1$ are those in rows 2, 4, 6, 8, 10, 12, 14, and 16. These types describe the possible ``laws'' governing the case: each indicates $X$'s effect in different contexts, as defined by $K$. The next step is to use $K$ once more, this time to tell us which context we are in. Since $K=1$, we know we are in the context represented by the second and fourth columns of potential outcomes.\n\nWe can thus ask for which of the 8 possible types does $X$ have a positive causal effect when $K=1$ (i.e., restricting to the second and fourth columns of potential outcomes). These are the types in rows 2, 4, 10, and 12: $t_{00}^{01}$, $t_{01}^{01}$, $t_{10}^{01}$, and $t_{11}^{01}$. In rows 6 ($t_{00}^{11}$), 8 ($t_{01}^{11}$), 14 ($t_{10}^{11}$), and 16 ($t_{11}^{11}$), $X$ has zero effect. Thus, for half of the types consistent with the data, $X$ has a positive effect on $Y$, and for half $Y$ would have been $1$ regardless of $X$'s value.\n\nAs for a mediator clue, we need to draw on prior beliefs over the causal possibilities in order to learn from the moderator clue. To help us think through how such beliefs might operate in this setting, let us return to our running example. Suppose that $X$ is the presence of a free press, $K$ is a measure of government sensitivity to public opinion, and $Y$ is government removal. Let us suppose that we do not know the structural equations connecting the variables (i.e., we bracket those presented in Chapter 2). However, based on previous research, we have beliefs over causal possibilities. \n\nIn the final column of Table \\ref{typesmoderation}, we provide a set of priors representing a possible set of such beliefs. Broadly, the priors we use for this illustration represent a few intuitive beliefs about the causal relationships in play: that the free press will represent a threat to governments that are insensitive to public opinion; that a free press could plausibly reinforce the legitimacy and thus prevent the removal of a government sensitive to public opinion (i.e., could have the opposite effect for a sensitive and an insensitive government); that a free press rarely prevents government removal when the government is insensitive to public opinion; that sensitivity of the government to public opinion rarely causes government removal; that it is not unlikely that the free press has no effect, regardless of sensitivity; and that it is somewhat likely that *neither* a free press nor sensitivity has no effect, regardless of the other. Types excluded by the $X, Y$ data alone are given a prior weight of 0 since we know that they are impossible before we observe the clue.\n\nGiven these priors, we can readily calculate our prior confidence---based just on the $X=Y=1$ observation---that the free press caused government removal (i.e., that the higher-level type is $t_{01}$. This is the population share represented by those types for which $X$ has a positive effect on $Y$ at some value of $K$, with each type weighted in the calculation by the probability that $K$ in fact takes on that value under which that type displays a positive effect of $X$ on $Y$.^[$$P(i \\in t_{01} | X=Y=1) = \\frac{\n\\pi^K\\left(\\lambda_{00}^{01}+\\lambda_{10}^{01}+\\lambda_{01}^{01}+\\lambda_{11}^{01}\\right)\n+\n(1-\\pi^K)\\left(\\lambda_{01}^{00}+\\lambda_{01}^{10}+\\lambda_{01}^{01}+\\lambda_{01}^{11}\\right)\n}{\n\\sum_{i = 0}^1\\left(\\pi^K\\left(\\lambda_{00}^{i1}+\\lambda_{10}^{i1}+\\lambda_{01}^{i1}+\\lambda_{11}^{i1}\\right)\n+\n(1-\\pi^K)\\left(\\lambda_{i1}^{00}+\\lambda_{i1}^{10}+\\lambda_{i1}^{01}+\\lambda_{i1}^{11}\\right)\n\\right)}$$] Thus, to calculate our prior on the causal effect of $X$, we need to also set a prior on how commonly we think governments (in cases with a free press and government removal) are sensitive to public opinion ($\\pi^K$ from our original causal model). Here we set the prior probability of $K=1$ to 0.5.\n\nUsing our specified prior beliefs, our prior confidence that the free press caused government removal is 0.61. Now, what happens if we go and collect our clue, measuring government sensitivity? If we look for the clue and find it ($K=1$), then our posterior probability becomes the population share of those types consistent with $X=Y=K=1$ for which $X$ has a positive effect on $Y$ when $K$ in fact is $1$.^[$P(i \\in t_{01} | X=Y=1, K=1) = \\frac{\\left(\\lambda_{00}^{01}+\\lambda_{10}^{01}+\\lambda_{01}^{01}+\\lambda_{11}^{01}\\right)}{\\sum_{i = 0}^1\\left(\\lambda_{00}^{i1}+\\lambda_{10}^{i1}+\\lambda_{01}^{i1}+\\lambda_{11}^{i1}\\right)}$] (We can see in this statement the learning both about which the laws governing the case's responses to context and about the context that the case is in.) This share works out to 0.325. Thus, if we observe a sensitive government, we become much less confident that $X$ caused $Y$ in this case. On the other hand, if we measure the government's sensitivity and find that it is insensitive ($K=0$), our confidence that $X$ caused $Y$ goes *up* somewhat to 0.73.^[$P(i \\in t_{01} | X=Y=1, K=0) &=& \\frac{\\left(\\lambda_{01}^{00}+\\lambda_{01}^{10}+\\lambda_{01}^{01}+\\lambda_{01}^{11}\\right}{\\sum_{i = 0}^1\\left(\\lambda_{i1}^{00}+\\lambda_{i1}^{10}+\\lambda_{i1}^{01}+\\lambda_{i1}^{11}\\right)}$] Thus, the search for governmental insensitivity functions as a relatively easy hoop test for the proposition that the free press caused the government's removal.\n\n\n\n<!-- Suppose that one first believed that the type was either  -->\n<!--  $t_{01}^{10}$ or  $t_{01}^{01}$. -->\n<!-- That is, a type for which  $Y=1$ if either $K=1$ or $X=1$ *but not both*, or it is a case where $X$ causes $Y$ no matter what the value of $K$. Having observed $X=Y=1$, seeing $K=1$ can rule out the possibility that the unit is of type  $t_{01}^{10}$. We  conclude it is of type $t_{01}^{01}$, and so $X$ caused $Y$---and *would have caused $Y$ even if $K$ were 0*. -->\n\n\n<!-- Second, observing a moderator clue identifies *which* contingencies the case is facing. The clue thus aids inference about $X$'s effect *even if $u_Y^{lower}$ is known*. For instance, suppose it is known that the  unit is of type $t_{01}^{11}$. For this  unit $Y=1$ if either $X=1$ of $K=1$. In this case, as $X$ is 1, whether $X$ caused $Y$ or not depends on $K$. If $K=0$ then $X$ indeed caused $Y$ but not if $K=1$.  -->\n\nUsing the expressions above, the prior before observing $K$ is $P(i \\in t_{01} | X=Y=1) = \\frac{\\pi^K\\lambda_{01}^{11}}{\\pi^K\\lambda_{01}^{11}+(1-\\pi^K)\\lambda_{01}^{11}} = \\pi^K$, and the posterior after observing $K=1$ (say) is   $P(i \\in t_{01} | X=Y=1, K=0) = \\frac{\\lambda_{01}^{11}}{\\lambda_{01}^{11}} = 1$.\n\nWe have shown for this example that the probability of type $t_{01}$ (positive case-level causal effect of $X$ on $Y$), knowing that $X=Y=1$, can be written in terms of the parameters of the lower-level graph: \n\n$$P(i \\in t_{01} | X=Y=1) = \\frac{\n\\pi^K\\left(\\lambda_{00}^{01}+\\lambda_{10}^{01}+\\lambda_{01}^{01}+\\lambda_{11}^{01}\\right)\n+\n(1-\\pi^K)\\left(\\lambda_{01}^{00}+\\lambda_{01}^{10}+\\lambda_{01}^{01}+\\lambda_{01}^{11}\\right)\n}{\n\\sum_{i = 0}^1\\left(\\pi^K\\left(\\lambda_{00}^{i1}+\\lambda_{10}^{i1}+\\lambda_{01}^{i1}+\\lambda_{11}^{i1}\\right)\n+\n(1-\\pi^K)\\left(\\lambda_{i1}^{00}+\\lambda_{i1}^{10}+\\lambda_{i1}^{01}+\\lambda_{i1}^{11}\\right)\n\\right)}$$\n\n```{r, echo = FALSE, fig.width = 7, fig.height = 5, fig.align=\"center\", out.width='.7\\\\textwidth', fig.cap = \"\\\\label{fig:modinf} A model with one explanatory  variable (top left), two lower level models that can imply it, and one model that does not.\"}\n\nhj_dag(x = c(1,1,2,2, 1.5, 1.5),\n       y = c(1,2,1,2, 1.5, 2),\n       names = c(\n         expression(paste(X)),\n         expression(paste(U[X])),  \n         expression(paste(\"Y\")),  \n         expression(paste(U[Y]^{lower})),\n         expression(paste(K)),\n         expression(paste(U[K])) \n         ),\n       arcs = cbind( c(2,1, 4, 6, 5),\n                     c(1,3, 3, 5, 3)),\n       title = \"Lower level graph:\\nModeration (Orthogonal Second Cause)\",\n       add_functions = 0, \n       contraction = .16, \n       padding = .2\n)\n\n\n```\n\n\nWe can now see that the posterior probabilities after observing $K$ can be written:\n\n\\begin{align}\nP(i \\in t_{01} | X=Y=1, K=0) &=& \\frac{\n\\left(\\lambda_{00}^{01}+\\lambda_{10}^{01}+\\lambda_{01}^{01}+\\lambda_{11}^{01}\\right)\n}{\n\\sum_{i = 0}^1\\left(\\lambda_{00}^{i1}+\\lambda_{10}^{i1}+\\lambda_{01}^{i1}+\\lambda_{11}^{i1}\\right)\n} \\\\\nP(i \\in t_{01} | X=Y=1, K=1) &=& \\frac{\n\\left(\\lambda_{01}^{00}+\\lambda_{01}^{10}+\\lambda_{01}^{01}+\\lambda_{01}^{11}\\right)\n}{\n\\sum_{i = 0}^1\\left(\\lambda_{i1}^{00}+\\lambda_{i1}^{10}+\\lambda_{i1}^{01}+\\lambda_{i1}^{11}\\right)\n}\n\\end{align}\n\nThese posterior probabilities can also be derived by calculating $\\phi_{t_{01}1}$ and $\\phi_{t_{11}1}$ as used in Humphreys and Jacobs (2015), that is, the probability of observing clue $K$ given causal type. By assumption, $K$ is as-if randomly assigned: in particular (without conditioning on $Y$), $K$ is orthogonal to $u_Y^{lower}$---that is, to $Y$'s responsiveness to $K$. While the probability of observing $K$ is thus the same for all lower-level $Y$ types, that probability is---critically---not the same for all the types in the *higher*-level theory. The higher-level types---those of interest for determining $X$'s effect on $Y$---are themselves related to the probability of $K$. Moreover, $K$ is also correlated with the *lower*-level type variable, $u_Y^{lower}$, *conditional* on $Y$. In particular, given $Y=1$, we have:\n\n$$\\phi_{t_{j1}1} = \\frac{\\pi^K\\left(\\lambda_{00}^{j1}+\\lambda_{10}^{j1}+\\lambda_{01}^{j1}+\\lambda_{11}^{j1}\\right)}{\\pi^K\\left(\\lambda_{00}^{j1}+\\lambda_{10}^{j1}+\\lambda_{01}^{j1}+\\lambda_{11}^{j1}\\right)\n+\n(1-\\pi^K)\\left(\\lambda_{j1}^{00}+\\lambda_{j1}^{10}+\\lambda_{j1}^{01}+\\lambda_{j1}^{11}\\right)}$$\n\n\n\n\n\n\n## Qualitative inferences strategies in the running example\nReturning to the running example involving government replacement in Figure \\ref{fig:running}, we can identify a host of causal estimands of interest and associated strategies of inference. We have shown above how one can use the structural equations in this model to provide a set of conditional causal graphs that let one see easily what caused what at different values of the root nodes $S$ and $X$. Each of these plots graphs a particular context. We can thus readily see which collection of root nodes constitutes a given query, or estimand.  With larger graphs, continuous variables, and more stochastic components, it may not be feasible to graph every possible context; but the strategy for inference remains the same. \n\nFor example, suppose one can see that $X=0$ and  $Y=0$ but does not know the causal effect of $X$ on $Y$  This is equivalent to saying that we know that we are in either panel $A$ or $B$ but we do not know which one. Defining the query in terms of root nodes, the question becomes  $S \\stackrel{?}{=} 1$, or $P(S=1|X=0,Y=0)$; the difference between the contexts in the two panels is that $S=0$ when, and only when, $X=0$ causes $Y=0$ . Given the structural equation for $S$, $P(S|X=0,Y=0) = P(S|X=0)$, and given independence of $X$ and $S$, $P(S=1|X=0)= \\pi^S$.  Figuring out $S$ fully answers the query: that is, $S$ is doubly decisive for the proposition. \n\nGraphically what is important is that $S$ is informative not because it is $d-$connected with $Y$, but because it is $d-$connected to the query variable---here, simply, to itself. For example,  suppose that $C$ were already observed together with $X$ and $Y$. $C$ and $X$ $d-$separate $S$ from $Y$. Yet $S$ would continue to be informative about the causal effect of $X$ on $Y$. We can, again, test this by comparing panel $A$ to panel $B$: the values of $C$, $X$, and $Y$ are the same in both graphs; it is only the value of $S$ that distinguishes between the two contexts. This highlights the importance of stating the estimands of interest in terms of root nodes. \n\nWe can also see how existing data can make clues uninformative. Say one wanted to know if $X$ causes $C$ in a case. As we can see from inspection of the panels, this query is equivalent to asking whether $S=1$ (as $X$ causes $C$ only in those two panels, $B$ and $D$, where $S=1$. $R$ is unconditionally informative about this query as $R$ is not $d-$separated from $S$. For example, $R=1$ implies $S=0$. However, if $C$ and $X$ are already known, then $R$ is no longer informative because $C$ and $X$ together *d*-separate $R$ from $S$. We can come to the same conclusion by reasoning with the graphs: if $X=0$ and $C=1$, we know we are in subfigure $A$ or $B$, and $X$ causes $C$ only in panel $B$. However, $R$ is of no help to us in distinguishing between the two contexts as it takes the same value in both graphs.\n\nMore generally, we can now see in this example how different types of clues can be informative, sometimes conditional on other data. Each of these types corresponds to a set of relations highlighted in Figure \\ref{fig:34graphs}. We can also read each of these results off of the subpanels in Figure \\ref{fig:running}.\n\n1. **Informative spouses** Spouses---parents of the same child---can inform on one another. As we have seen in other examples, when an outcome has multiple causes, knowing the value of one of those causes helps assess the effect(s) of the other(s). For example, here, $S$ and $X$ are both parents of $C$; $S$ is thus informative for assessing whether $X$ causes $C$. Indeed this query, written in terms of roots, is simply $P(S)$:  $X$ causes $C$ if and only if $S=1$. Likewise, $S$ causes $C$ (negatively) if and only if $X=1$.\n \n2. **Pre-treatment clues.** Did the absence of media reports on corruption ($R=0$) cause government survival ($Y=0$)? Look to the pre-treatment clue, $X$: $X=0$ is a smoking gun establishing that the absence of a report produced government survival. Or, substantively, if there were a free press, then a missing report would never be a cause of survival since it would occur only in the absence of corruption, which would itself be sufficient for survival. More broadly, this example illustrates how knowledge of selection into treatment can be informative about treatment effects.\n\n3. **Post-outcome clues.** Suppose we observe the presence of a free press ($X=1$) and want to know if it caused a lack of corruption ($C=0$), but cannot observe the level of corruption directly. Observing $Y$---which occurs after the outcome---is informative here: if $X=1$, then $X$ causes $C$ (negatively) if and only if $Y=0$. When an outcome is not observed, a consequence of that outcome can be informative about its value and, thus, about the effect of an observed suspected cause.\n\n4. **Mediators as clues**: We see a politically sensitive government ($S=1$) and its survival ($S=0$). Did the government survive because of its sensitivity to public opinion? Here, the mediation clue $C$ is helpful: a lack of corruption, $C=0$, is evidence of $S$'s negative effect on $Y$.\n\nWhile the above examples focused on case level causal effects, we can also how clues are informative for different types of estimand:\n\n\n1. **Average casual effects.** Analysis of a single case is informative about average causal effects if there is uncertainty about the distributions of root nodes. Recall that the values of $X$ and $S$, in the model, are determined by the parameters $\\pi^X$ and $\\pi^S$ (not pictured in the graph). Recall, further, that average causal effects are functions of these parameters: in a model in which $\\pi^X$ is explicitly included as a root, the query \"what is the average causal effect of $S$ on $Y$\" is $P(\\pi^X)$. Simple observation of $Y$ is informative about the value of $X$ and, in turn, about $P(\\pi^X)$. If we start with a Beta prior with parameters $\\alpha, \\beta$ over $\\pi^X$ and we observe $Y=1$, say, then the posterior is:  $$P(\\pi^X | Y=1) = \\frac{P(Y=1 | \\pi^X)P(\\pi^X)}{P(Y=1)} = \\frac{P(X=1, S=1 | \\pi^X)P(\\pi^X)}{P(X=1, S=1)} =  \\frac{P(X=1| \\pi^X)P(\\pi^X)}{P(X=1)}$$\nNote that $S$ drops out of the expression because of the assumed independence of $X$ and $S$.  We are left with the problem of updating a belief about a proportion, $\\pi^X$,  in a population given a positive draw from the population: ($P(\\pi^X | X=1)$) .  In the case of the Beta prior, the posterior would be  $Beta(\\alpha+1, \\beta)$. Note also that, using the $d-$separation criterion, learning $Y$ would be uninformative if we already knew $X$ or if we already knew $C$ and $R$, as each of these sets of nodes blocks $Y$ from $\\pi^X$, a parent of $X$.\n\n2. **Actual cause, notable cause.**  Consider now the query: is $X=0$ an *actual cause* of $Y=0$? The definition of actual causes, together with our structural model, would require that that $X=0$ and $Y=0$, and given $X=0$ the condition for $X$ to make a difference is $C=1$.  So our query is $C \\stackrel{?}{=} 1$ which in terms of roots is  $P((X=0) \\& ((X=0) \\text{or} (X=1 \\& S=0)))$, which is simply equal to $P(X=0)$. This means that in this example, whenever $X=0$, $X$ is an actual cause of $Y=0$. $X$ itself is decisive about this query.  The likelihood that $X=0$ will actually cause $Y=0$ is $1-\\pi^X$. The query whether $X=0$ is a *notable* cause of $Y=0$ in a case is a query about both $\\pi^X$ and $u_X$ (as  $u_X$ causes $X$ given $\\pi^X$). \n\n3. **Path estimands.** Consider now a causal path as an estimand; for example \"does $X$ cause $R$ which in turn causes $Y$\"; this path arises in  panel A and panel C only and so in terms of roots the query is $S \\stackrel{?}{=} 0$. $S$ is doubly decisive for this proposition. For such paths note that transitivity does not necessarily follow: for example $S=1, X=1$ is evidence for the path \"Free press prevents corruption and the absence of corruption causes the government to survive,\" but here the free press does not cause survival.\n\n",
    "created" : 1491337909503.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1682567282",
    "id" : "D5D6DB0B",
    "lastKnownWriteTime" : 1491938682,
    "last_content_update" : 1491938682,
    "path" : "~/Dropbox/ProcessTracing/8 Book/ii/05-dag_inference.Rmd",
    "project_path" : "05-dag_inference.Rmd",
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}
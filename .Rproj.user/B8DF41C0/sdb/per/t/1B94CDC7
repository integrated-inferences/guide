{
    "collab_server" : "",
    "contents" : "# (PART) Model Based Mixed Methods   {-} \n\n# Integrated inferences\n**\\color{blue}Key points: We argue that mixed methods can be thought of as the analysis of single cases with vector valued variables. Reconceptualizing as large n is useful prmarily for computation reasons and often comes with hidden independence assumptions. We illustrate the single case approach and provide a set of models for the many case approach.**\n\n<!-- Lots of this likely to change with integration with DAGs. -->\n\nThis chapter proceeds in three parts. First we show how the exact structure introduced in Chapter 6 for single case analysis can be used for multi-case analysis. In this case the nodes are vector-valued, and we focus on sample average causal effects which are just a particular summary of the vector-valued case level causal effects. But the fundamental inferential process is identical. In this sense the conceptual work has been done already and \"mixing methods\" is no different from purely qualitative inference. \n\nIncreasing observations does produce considerable computational complexity, however, and it also provides leverage to seek estimation on population quantities and not simple sample quantities. In the second part of the chapter, we introduce the baseline model developed in Humphreys and Jacobs (2015). This model has an advantage of simplicity but it has the disadvantage that it is only informally connected to a causal model. The probative value of clues is assumed, rather than derived from a causal structure. In the third part, we address this by working from the \"ground up,\" deriving probative value from a probabilistic causal model. We show how we can do this for two causal structures that we have been examining throughout: moderation and mediation. \n\n## There's only ever one case\nIn Humphreys and Jacobs (2015) we described an approach to the unification of inferences from within-case (process-tracing) and cross-case (correlational)  data. The basic idea is that, because a Bayesian approach can be used to update beliefs about causal effects using either process-based or correlational data, it's a small step to allow updating when both data types are present.\n\n<!-- For specific problems the different types of data make distinct contributions. For example for causal effects, observations of $X$ and $Y$ values  discriminating information about the type of that case---in our binary setup, narrowing it down to one of two types (for instance, $X=1, Y=1$ can only be a $b$ or a $d$.)  Additional within-case \\textit{clue} ($K$) information provides further discriminating power. Put another way, since the causal type affects the likelihood of observing patterns over $X$, $Y$, and $K$,  information about all three of these quantities allows us to update over the causal types.  -->\n\nWe have organized this book in a way that reflects the conventional distinction between within- and cross-case inference, with Part II focused on within-case inference and Part III focussing on inference from both within- and cross-case data.\n\nHowever, one thing that troubles us about this categorization is that it obscures a larger point that emerges from inference from causal models: conceptualized correctly, there is no difference at all between the data types or the inference used in within-case and cross-case inference. The reason is not, as @king1994designing suggest, that all causal inference is fundamentally correlational, even in seemingly single case studies. Nor is the point that, looked at carefully, single \"case studies\" can be disaggregated into many cases. The intuition runs in the opposite direction: fundamentally, model-based inference always involves comparing *a* pattern of data with the logic of the model. Looked at carefully, studies with multiple cases can be conceptualized of as single-case studies: the drawing of inferences from a single collection of clues.\n\nThe last point is critical, and seeing it clarifies that the most important conceptual work of Part III of this book was already completed in Part II. The key insight is that, when we move from a causal model with one observation to a causal model with multiple observations, all that we are doing is replacing nodes with a single value (i.e., scalars) with nodes containing multiple values (i.e., vectors). \n\nTo illustrate the idea that multi-case studies are really single-case studies with vector valued variables, consider the following situation. There are two units studied drawn from some population, a binary treatment $X$ is assigned independently with probability .5 to each case; an outcome $Y$ along with clue variable $K$ are observable.  We suppose  $X$ can affect $Y$ and in addition there is a background, unobserved, variable $T$ (causal type) that takes on values in $\\{a,b,c,d\\}$, that affects both $K$ and $Y$.  We will assume that $T$ is not independently assigned and that the two units are more likely to have the same values of $T$ than different values. For simplicity, we will suppose that for any given case $K=1$ whenever $X$ causes $Y$, and $K=1$ with a 50% probability otherwise. Thus, $K$ is informative about a unit's causal type.\n\nNote that we have described the problem at the unit level. We can redescribe it at the population level however as a situation in which a treatment vector $X$ can take on one of four values, $(0,0), (0,1), (1,0), (1,1)$ with equal probability (or more strictly: as determined by $U_X$). $T$ is also a vector with two elements that can take on one of 16 values $(a,a), (a,b),\\dots (d,d)$ as determined by $U_T$. In this case we will assume that the 16 possibilities are not equally likely, which captures the failure of independence in the unit level assignments.  $Y$ is a vector that reflects the elements of $T$ and $X$ in the obvious way (e.g $X=(0,0), T=(a,b)$ generates outcomes $Y=(1,0)$; though it is immediately obvious that representing nodes in vector forms allows for more general vector-level mappings to allow for SUTVA violations. $K$ has the same domain as $X$ and $Y$, and element $K[j]=1$ if $T[j]=b$.\n\nNote that to describe the estimand, the Sample Average Treatment Effect, we also need to consider operations and queries defined at the vector level. In practice we consider three operations, one in which both units have $X$ forced to 0 and two in which one nit has $X$ set to 0 and the other has $X$ set to 1. Thus we are interested in the average effect of changing one unit to treatment while the other is held in control. Note also that before our estimands were binary---of the form: is it a $b$ type?--and our answer was a probability; now our estimand is categorical and our answer is a distribution (what is the probability the SATE is 0, what is the probability the SATE is .5, etc...)\n\nRepresented in this way we can use the tools of Chapters 6 and 7 to fully examine this seemingly multi-case study. In the below we examine a situation in which we consider the value of observing $K$ on one case --- in this set up this is equivalent to observing part of the vector $K$ and making inferences on the full vector $T$.\n\nThe code box below shows the setup using the same tools as in Chapter 6. The specification of the vectors is not as simple, but the logic and analysis is the same. In all cases we simply specify the full distribution and then calculate the conditional distributions, given existing data and new clue data.\n\n```{r model, echo = TRUE, warning = FALSE, message = FALSE}\nlibrary(biqq)\n\n# Define a model\nmodel <- biqq_model(\n    var_names = c(\"T\", \"X\", \"K\", \"Y\"),\n    var_functions = list( \n          f_T = function(U,V) c(ceiling(U[1]/4), 1+((U[1]-1)%%4)),\n          f_X = function(U,V) c(ifelse(U[2] %in% c(3,4), 1, 0),  \n                                ifelse(U[2] %in% c(2,4), 1, 0)),\n          f_K = function(U,V) c(max(U[3] < 0,   V[[1]][1]==2), \n                                max(U[3] < .5,  V[[1]][2]==2)),\n          f_Y = function(U,V) 1*c((V[[1]][1]==2)*(V[[2]][1]) + \n                                  (V[[1]][1]==1)*(1-V[[2]][1]) + \n                                  (V[[1]][1]==4),\n                                  (V[[1]][2]==2)*(V[[2]][2]) + \n                                  (V[[1]][2]==1)*(1-V[[2]][2]) + \n                                  (V[[1]][2]==4))),\n    P = function() c( \n                    sample(c(1:16, seq(1,16,5), seq(1,16,5)),1),\n                    sample(4,1), rnorm(2)))\n\n# Define a query\nmy_operations <- list(list(NA, c(0,0), NA, NA, NA), \n                      list(NA, c(1,0), NA, NA, NA),\n                      list(NA, c(0,1), NA, NA, NA))\n\nmy_query      <- function(x, effect = 0) {\n                             (x[[2]][[4]][1] - x[[1]][[4]][1]  +\n                              x[[3]][[4]][2] - x[[1]][[4]][2] )/2 == effect} \n```\n\nWe are now in a position to calculate and plot the conditional probabilities---that is, the posterior distribution on the Sample Average Treatment Effect.\n\n```{r}\n# Calculate conditional probabilities on the query\nposterior <- sapply(c(-1, -.5, 0, .5, 1), function(effect) {\n  z <- biqq_which( model,\n                   operations = my_operations,\n                   query = function(x) my_query(x, effect = effect), \n                   sims = 20000, unlistit = FALSE)\n  c(mean(z$A), \n    mean(z$A[z$V$K1==0]), \n    mean(z$A[z$V$K1==1]))\n  })\n```\n\n```{r, fig.cap = \"Posterior on SATE from observation of one clue (red line indicates clue observed, blue line, not observed, black line is the prior. Here there are just two cases.\", echo = FALSE}\nplot((-2:2)/2, posterior[1,], type = \"l\", ylim = c(0, .5), xlab = \"Sample average treatment effect\", ylab = \"posterior probability\")\npoints((-2:2)/2, posterior[2,], type = \"l\", col = \"blue\")\npoints((-2:2)/2, posterior[3,], type = \"l\", col = \"red\")\n```\n\n\n\n\n\n## The Baseline Model {#baseline}\n\nThe approach given above captures the key conceptual issues involved in mixed methods inference for general processes. But for large problems with many cases, the approach is impractical. \n\nIn the remainder of this section we provide a complete description of a simple but complete baseline model that is likely to cover a range of applications of interest and that can be easily extended to many cases. We then show how this basic model can be extended to more complex research situations.  \n\n\nTurning now to the formalization of the baseline model, we describe a complete BIQQ model with respect to (a.) parameters, (b.) priors, (c.) likelihood, and (d.) inference. \n\\subsubsection{Parameters}\nIn the baseline model, we have three sets of parameters:\n\n\\begin{enumerate}\n\\item The population distribution of causal types\n\\item The probabilities with which types are assigned to treatment \n\\item The probabilities with which clues are associated with types\n\n\\end{enumerate}\n\nWe discuss these in turn.\n\n\\textbf{Distribution of types.} As above, we are interested in the proportion of $a$, $b$, $c$, or $d$ types in the population, denoted by $\\lambda=(\\lambda_a , \\lambda_b , \\lambda_c ,  \\lambda_d)$ (Table \\ref{tabPO}). Implicit in this representation of causal types is a SUTVA assumption  | i.e., that potential outcomes for a given case depend only on that case's treatment status. Our setup with four types also implies just a single explanatory variable. Though embedded in the baseline model, both assumptions can be relaxed (see section \\ref{extensions}).\n\n\\textbf{Treatment assignment.} Let $\\pi=(\\pi_a,\\pi_b,\\pi_c,\\pi_d)$ denote the collection of assignment probabilities: that is, the probability of receiving treatment ($X$=1) for cases of each causal type. We assume in the baseline model that cases are independently assigned to treatment. Assignment propensities could in principle, however, be modeled as correlated or as dependent on covariates. \n\n\\textbf{{Clues.} }  Let $\\phi=(\\phi_{jx})$ denote the collection of probabilities that a case of type $j$ will exhibit clue $K=1$ (when it is sought), when $X=x$. Thus, $\\phi_{b1}$ is the probability of observing the clue for a treated case of $b$ type, while $\\phi_{b0}$ is the probability of observing the clue for an untreated $b$ type. Note that we allow clue probabilities to be conditional on a case's treatment status. The rationale is that, for many process-based clues, their likelihood of being observed will depend on the case's causal condition. %In principle, one could also make these probabilities conditional on case outcomes. \nWe again assume that the realization of clues is independent across cases. For simplicity, we also assume in the baseline model that only one clue is examined per case, though nothing rides on this.\n  \n\n\nIn total, in the baseline model the parameter vector $\\theta$ has 16 elements grouped into three families: $\\theta = (\\lambda, \\pi, \\phi)$. \n\n\\subsubsection{Priors}{\\label{BIQQPriors}\nOur uncertainty before seeing the data is represented by a prior probability distribution over the parameters of interest, given by $p(\\theta)$. The model accords great flexibility to researchers in specifying these prior beliefs, a point we return to in the concluding section. In our baseline model, we employ priors formed from the product of priors over the component elements of $\\theta$; that is, we assume that priors over parameters are independent. For the $\\lambda$ parameters, we employ a Dirichlet distribution in most applications below; for both $\\pi$ and $\\phi$ parameters, we employ a collection of Beta distributions.\n\nIn some situations, researchers may have uncertainty over some parameters but know with certainty the value of others. %In practice $\\rho$ and $\\kappa$ may be under the control of researchers and we assume in all analyses below that these are known. \nIn experimental work, for instance, assignment probabilities may be known with certainty. Parameters with known values can be removed from $\\theta$, entering into the likelihood as fixed values. We model $\\phi$ as fixed in some applications below, though in others allow for high uncertainty over assignment. In general, we expect that $\\lambda$ and $\\phi$ will be uncertain. \n\nIn general, sharper (lower-uncertainty) priors on one set of parameters will tend to generate more learning about the others. For instance, we learn more about causal effects and clue probabilities when we have sharper priors on assignment propensities.  While BIQQ can also employ uninformative priors, the consequences of flat priors differ across our parameter families. We can learn from correlational and process data with flat priors on both types and assignment propensities; a pattern of $X,Y$ (and clue) data can narrow down type possibilities even with uninformative priors about assignment.\\footnote{For instance, observing a very large share of $X=0, Y=0$ cases in our sample will (regardless of assignment propensities or priors on types) tend to reduce posterior estimates of the population share of $a$ and $d$ types.} However, we can draw no causal inferences from within-case information with a flat prior on $\\phi$:  a clue is uninterpretable without a belief about its differential relationship to alternative types. As discussed above, informative priors on $\\phi$ will typically reflect a theory (or theories) of the mechanism through which a causal effect operates together with a belief about what we should observe if that mechanism is operating. As highlighted in the conclusion, however, we ultimately require \\textit{empirical} justification for our priors on $\\phi$ if we wish to ground empirical conclusions on qualitative data. \n\n\\subsubsection{Likelihood}\nIn the baseline model, we assume that $X$, $Y$ data is observed for all cases under study, but that $K$ data may be sought for only a subset of these. Thus, each case displays one of twelve possible data realizations: formed by all combinations of $X\\in\\{0,1\\}$, $Y\\in\\{0,1\\}$, and $K \\in \\{0,1,*\\}$ (using the wildcard symbol ``$*$'' to denote that a clue has not been sought in a case). \n\nWe can then define two vectors registering the probabilities of the twelve possible case-level data realizations: \n\n\\begin{eqnarray*}\nw_{XY*} & = & {\\left( \n\\begin{array}{c}\nw_{00*} \\\\ w_{01*} \\\\  w_{10*} \\\\ w_{11*}\n\\end{array} \n\\right)= \n\\left( \\begin{array}{c}\n\\lambda_b(1-\\pi_b)\n+\n\\lambda_c(1-\\pi_c)\\\\\n\n\\lambda_a(1-\\pi_a)\n+\n\\lambda_d(1-\\pi_d)\\\\\n\n\\lambda_a \\pi_a\n+\n\\lambda_c\\pi_c)\n\n \\\\\n\\lambda_b\\pi_b\n+\n\\lambda_d\\pi_d\n\n\\end{array} \\right)}\n\\\\ \nw_{XYK}&=&{\\left( \\begin{array}{c}\nw_{000} \\\\ w_{001} \\\\  \\vdots \\\\ w_{111}\n\\end{array} \\right)= \n\\left( \\begin{array}{c}\n\\lambda_b(1-\\pi_b)(1-\\phi_{b0})\n+\n\\lambda_c(1-\\pi_c)(1-\\phi_{c0})\\\\\n\n\n\\lambda_b(1-\\pi_b)\\phi_{b0}\n+\n\\lambda_c(1-\\pi_c)\\phi_{c0}\\\\\n\n\\vdots \\\\\n\\lambda_b\\pi_b\\phi_{b1}\n+\n\\lambda_d\\pi_d\\phi_{d1}\n\n\\end{array} \\right)}\n\\end{eqnarray*}\n\nWe next let $n_{XYK}$ denote an 8-element vector recording the number of cases in a sample displaying each possible combination of $X,Y,K$ data, where $K\\in\\{0,1\\}$, thus: $n_{XYK}=(n_{000},n_{001},n_{100},\\dots ,n_{111})$. Similarly, we let $n_{XY*}$ denote a four-element vector recording the data pattern for cases in which no clue evidence is gathered: $n_{XY*}=(n_{00*},n_{01*},n_{10*} ,n_{11*})$. Finally, assuming that data are independently and identically distributed, the likelihood is:\n\\begin{equation*}\n\\Pr(\\mathcal{D}|\\theta) = \\begin{cases}\n  \\text{Multinom}(n_{XY*}|w_{XY*}) \\times \\text{Multinom}(n_{XYK}|w_{XYK}) & \\text{if } \\sum n_{XYK} = k \\\\\n  0 & \\text{otherwise}.\n\\end{cases}\n\\end{equation*}\n\nThis likelihood simply records the product of the probability that $X,Y$ data would look as they do in those cases in which only $X,Y$ data are gathered, and the probability that the $X,Y,K$ data would look as they do in those cases in which within-case data are also gathered (while fixing the size of this second set of cases at $k$). \n\n\nAs before we highlight that the likelihood contains information on data gathering; in particular it contains information on qualitative and quantitative case selection. In general, researchers do not necessarily seek clue information for all cases under study. In the baseline model, clue evidence is sought in a randomly selected $k$ of $n$ cases. Again, one could model more complex qualitative case selection processes, which for instance might be independent (if for example a clue is sought in each case with a fixed probability), dependent on $X$ or $Y$ values, or dependent on potential outcomes.  We discuss some of these possibilities in our discussion of case selection in Chapter 11.\n\nAlso implicit in the likelihood are assumptions regarding the process for selecting the overall set of $n$ cases under study. In the baseline model, we assume that the number of studied cases is fixed at $n$ and that each case is selected for study with equal probability. Alternatively, one could model the probability of selection of cases as independent (and thus the number of cases, $n$,  as stochastic) or as dependent on potential outcomes or other features (such as the values of $X$ or $Y$). \n\n<!-- Some of these possibilities are addressed in the Supplementary Materials (\\S\\ref{AppExtensions}). Our application below to the natural resources and civil war literature also illustrates how known non-random case selection processes can sometimes be treated as random conditional on a clue. -->\n\nThe assumption of random sampling in the baseline model, both for the selection of study cases (from the population) and for the selection of a subsample of these for further, within-case data collection, justifies treating cases as ``exchangeable,'' which renders the likelihood function informative \\citep{ericson1969subjective}. More generally, even where researchers choose to sample on some observable (e.g., $X$ or $Y$ values), selecting randomly (conditional on that observable) will generally be necessary to defend an assumption of exchangeability.\n\n\\subsubsection{Inference}\nWith these elements in hand, inference is done by applying Bayes' rule:\n\n\n\\begin{equation} \\label{Bayes2}\np(\\theta |\\mathcal{D}) = \\frac{\\Pr(\\mathcal{D} | \\theta)p(\\theta)}{\\int {\\Pr(\\mathcal{D} | \\theta')p(\\theta') d\\theta'}}\n\\end{equation}\n\n\nThere are many methods for estimating this posterior probability, though in most cases we use Monte Carlo Markov Chain sampling implemented via RStan \\citep{gelman2013bayesian, rstan-software:2014}. \n\n<!-- In the Supplementary Materials (\\S \\ref{AppSim}), we show how to carry out BIQQ inference ``by hand.'' In Section (\\S\\ref{code}) we provide code for implementing the baseline BIQQ model via RStan, given user-defined priors and arbitrary $X$, $Y$, $K$, data.  -->\n\nThe resulting posterior probability distribution reflects a shift in weight toward those parameter values that are more consistent with the evidence and provides updated beliefs about all of the parameters of interest. Of greatest interest in many situations will be the posterior distribution over types in the population, $(\\lambda_a,\\lambda_b,\\lambda_c,\\lambda_d)$. From these, a marginal distribution of the posterior on treatment effects $(\\lambda_b-\\lambda_a)$ can be readily computed. \n\nEqually important, the posterior provides updated beliefs about the other primitives in the analysis, including the process assigning cases to treatment ($\\pi_j$) and the probative value of clues $(\\phi_{jx})$. That is to say, the framework captures the effect that observing evidence should have on the very beliefs that condition our interpretation of correlational or process-tracing evidence. This updating can occur because of the integration of independent streams of evidence: in effect, BIQQ employs \\textit{clue}-independent $X,Y$ information about types to test clue predictions, and \\textit{correlation}-independent clues to types to test beliefs about how types are assigned to treatment.\n\n\n<!-- ## Integrated Inference Step by Step {#AppSim} -->\n\n<!-- Do wth conditional probability tables -->\n\n\n<!-- Table \\ref{IllusPriors}, describes a prior distribution that includes only three possible values for $\\theta$  states of the world. Here the parameters include $\\lambda$, $\\pi$ and $\\phi$ as in our baseline model. %They exclude $\\kappa$ and $\\rho$, which is appropriate here if $\\rho$ is believed to be constant and if $\\kappa$ is 1.  -->\n\n<!-- In addition we include a new pair of parameters $\\eta$, where $\\eta^L$ and $\\eta^M$ are indicators that one of two rival causal accounts, $L$ or $M$, is correct.  -->\n\n\n<!-- \\begin{table}[h!] -->\n<!--   \\centering -->\n\n<!--     \\begin{tabular}{lc|ccc|c} -->\n<!-- \\hline -->\n<!--           & \\multicolumn{1}{c|}{} &\\multicolumn{3}{c|}{State of the World:} & Prior \\\\ -->\n<!-- &Parameter&   $\\theta_1$ & $\\theta_2$ & $\\theta_3$  &expectation\\\\ -->\n<!-- \\hline  -->\n<!--     \\multicolumn{1}{l}{\\textbf{Prior on $\\theta_j$:}} & {} & 0.33  & 0.33  & 0.33  &  \\\\ \\hline \\hline -->\n<!--     \\multicolumn{1}{l}{\\multirow{4}[2]{*}{Types}} & {$\\lambda_a$} & 0     & 0.1   & 0.2   & 0.1 \\\\ -->\n<!--     \\multicolumn{1}{l}{} & {$\\lambda_b$} & 0.8   & 0.1   & 0     & 0.3 \\\\ -->\n<!--     \\multicolumn{1}{l}{} & {$\\lambda_c$} & 0.1   & 0.4   & 0.4   & 0.3 \\\\ -->\n<!--     \\multicolumn{1}{l}{} & {$\\lambda_d$} & 0.1   & 0.4   & 0.4   & 0.3 \\\\ \\hline -->\n<!--     \\multicolumn{1}{l}{\\multirow{4}[2]{*}{Assignment Propensities}} & {$\\pi_a$} & 0.5   & 0.5   & 0.5   & 0.5 \\\\ -->\n<!--     \\multicolumn{1}{l}{} & {$\\pi_b$} & 0.5   & 0.5   & 0.5   & 0.5 \\\\ -->\n<!--     \\multicolumn{1}{l}{} & {$\\pi_c$} & 0.5   & 0.1   & 0.4   & 0.33 \\\\ -->\n<!--     \\multicolumn{1}{l}{} & {$\\pi_d$} & 0.5   & 0.9   & 0.6   & 0.67 \\\\ \\hline -->\n<!--     \\multicolumn{1}{l}{\\multirow{8}[2]{*}{Clues}} & {$\\phi_{a0}$} & 0.5   & 0.9   & 0.6   & {0.67} \\\\ -->\n<!--     \\multicolumn{1}{l}{} & {$\\phi_{a1}$} & 0.5   & 0.1   & 0.4   & {0.33} \\\\ -->\n<!--     \\multicolumn{1}{l}{} & {$\\phi_{b0}$} & 0.5   & 0.9   & 0.6   & {0.67} \\\\ -->\n<!--     \\multicolumn{1}{l}{} & {$\\phi_{b1}$} & 0.5   & 0.1   & 0.4   & {0.33} \\\\ -->\n<!--     \\multicolumn{1}{l}{} & {$\\phi_{c0}$} & 0.5   & 0.1   & 0.4   & {0.33} \\\\ -->\n<!--     \\multicolumn{1}{l}{} & {$\\phi_{c1}$} & 0.5   & 0.9   & 0.6   & {0.67} \\\\ -->\n<!--     \\multicolumn{1}{l}{} & {$\\phi_{d0}$} & 0.5   & 0.1   & 0.4   & {0.33} \\\\ -->\n<!--     \\multicolumn{1}{l}{} & {$\\phi_{d1}$} & 0.5   & 0.9   & 0.6   & {0.67} \\\\ \\hline  -->\n<!--         \\multicolumn{1}{l}{\\multirow{2}[2]{*}{Theory}} & {$\\eta^L$} & 1     & 0     & 0     & 0.33 \\\\ -->\n<!--         \\multicolumn{1}{l}{} & {$\\eta^M$} & 0     & 1     & 1     & 0.67 \\\\ \\hline -->\n<!--     \\hline -->\n<!--     \\multicolumn{1}{l}{\\textbf{Values implied by priors:}} & {} &       &       &       &  \\\\ \\hline -->\n<!--     \\multicolumn{1}{l}{Population incidence of clue} & {} & 0.5   & 0.5   & 0.5   & 0.5 \\\\ -->\n<!--     \\multicolumn{1}{l}{Population incidence of treatment} & {} & 0.5   & 0.5   & 0.5   & 0.5 \\\\ -->\n<!--     \\multicolumn{1}{l}{Population incidence of outcome} & {} & 0.5   & 0.5   & 0.5   & 0.5 \\\\ -->\n<!--     \\multicolumn{1}{l}{ATE} &       & 0.8   & 0     & -0.2  & 0.2 \\\\ -->\n<!-- \\hline -->\n<!--     \\end{tabular}% -->\n<!--   \\caption{Illustration (Part 1 of 3): Illustration of prior beliefs over potential outcomes, assignment, and theoretical validity.  In this illustration it is assumed that the researcher starts out uncertain over only three combinations of parameters.}  \\label{ill:priors}% -->\n<!-- \\label{IllusPriors} -->\n<!-- \\end{table}% -->\n\n\n\n\n<!-- In this example, the researcher considers two theories, $L$ and $M$. In one state of the world, $\\theta_1$, $L$ is true. In $\\theta_1$, the treatment also has strong causal effects (ATE = 0.8), all cases receive treatment with equal probability (0.5 each), and the clue is equally likely to be observed for all types and treatment states (clues have no probative value). We can also read the table as indicating that theory $L$ implies no uncertainty about the other parameters, reflected in the fact that $L$ is associated with only one $\\theta$. A prior probability of one-third is placed on theory $L$ and all associated parameter values. -->\n\n<!-- Theory $M$ is believed to be true in $\\theta_2$ and $\\theta_3$, each of which is also given a prior probability of 0.33. Yet theory $M$ is associated with some uncertainty about the other parameters: across $\\theta_2$ and $\\theta_3$, expected treatment effects vary between weak and negative, and beliefs about the probative value of clues vary.\\footnote{Note that we do not post an index to explicitly associate theories with different values of $\\phi$, though the association can nevertheless be derived from the probability distribution.}  -->\n\n\n<!-- Note that each admissible state for this example is consistent with a belief that the population incidence of clue $K$ is 0.5, the population incidence of the treatment condition $X=1$ is 0.5, and the population incidence of outcome $Y=1$ is 0.5. Uncertainty over three constrained combinations nonetheless allows for relatively complex priors over potential outcomes, assignment processes, theories, and clue predictions as well as correlations among all of these.  -->\n\n<!-- Given the priors described in Table \\ref{IllusPriors}, researchers can use Bayes' rule to form posteriors on all the parameters listed in Table \\ref{IllusPriors}, for any realization of the data. -->\n\n<!-- Table \\ref{IllusPosteriors} shows the possible posteriors for a design in which a researcher collects data on a \\textit{single} case randomly drawn from the population, and observes the value taken by $X$, $Y$, and $K$.  -->\n\n<!-- The table shows the posterior distribution of weights that we would then place on each state of the world $\\theta_j$. Each row represents one possible set of observed $X$, $Y$, and $K$ values for the selected case. Within each row, the first sub-row indicates the prior probability, under each $\\theta_j$, that this set of values would be observed. The second sub-row provides the \\textit{posterior} weight we then place on each $\\theta_j$ if that set of observations is in fact made.  -->\n\n\n<!-- \\begin{table}[h!] -->\n<!--   \\centering -->\n<!--     \\begin{tabular}{llcccc} -->\n<!--     \\toprule -->\n<!--  &&$\\theta_1$& $\\theta_2$ & $\\theta_3$& Total \\\\ \\hline -->\n<!-- Prior on $\\theta_j$ & & 0.33 & 0.33 & 0.33 & 1\\\\ \\hline -->\n<!--     Prob $X=1, Y=1, K=1$, for each $\\theta_j$ (type $b$ or $d$) &       & 0.23  & 0.33  & 0.14  & 0.23 \\\\  -->\n<!--     Posterior on $\\theta_j$ &       & 0.32  & 0.47  & 0.21  & 1 \\\\  \\hline -->\n<!--     Prob  $X=0, Y=1, K=1$, for each $\\theta_j$ (type $a$ or $d$): &       & \\textbf{0.03} & 0.05  & 0.12  & 0.07 \\\\ -->\n<!--     Posterior on $\\theta_j$ &       & 0.13  & 0.25  & 0.63  & 1 \\\\ \\hline -->\n<!--     Prob  $X=1, Y=0, K=1$, for each $\\theta_j$ (type $a$ or $c$) &       & 0.03  & 0.04  & 0.14  & 0.07 \\\\ -->\n<!--     Posterior on $\\theta_j$ &       & 0.12  & 0.2   & 0.67  & 1 \\\\ \\hline -->\n<!--     Prob  $X=0, Y=0, K=1$, for each $\\theta_j$ (type $b$ or $c$): &       & 0.23  & 0.08  & 0.1   & 0.13 \\\\ -->\n<!--     Posterior on $\\theta_j$ &       & 0.56  & 0.2   & 0.24  & 1 \\\\ \\hline -->\n<!--     Prob  $X=1, Y=1, K=0$, for each $\\theta_j$ (type $b$ or $d$): &       & 0.23  & 0.08  & 0.1   & 0.13 \\\\ -->\n<!--     Posterior on $\\theta_j$ &       & 0.56  & 0.2   & 0.24  & 1 \\\\ \\hline -->\n<!--     Prob  $X=0, Y=1, K=0$, for each $\\theta_j$ (type $a$ or $d$): &       & 0.03  & 0.04  & 0.14  & 0.07 \\\\ -->\n<!--     Posterior on $\\theta_j$ &       & 0.12  & 0.2   & 0.67  & 1 \\\\ \\hline -->\n<!--     Prob  $X=1, Y=0, K=0$, for each $\\theta_j$ (type $a$ or $c$): &       & 0.03  & 0.05  & 0.12  & 0.07 \\\\ -->\n<!--     Posterior on $\\theta_j$ &       & 0.13  & 0.25  & 0.63  & 1 \\\\ \\hline -->\n<!--     Prob  $X=0, Y=0, K=0$, for each $\\theta_j$ (type $b$ or $c$): &       & 0.23  & 0.33  & 0.14  & 0.23 \\\\ -->\n<!--     Posterior on $\\theta_j$ &       & 0.32  & 0.47  & 0.21  & 1 \\\\ \\hline -->\n\n<!--     \\end{tabular}% -->\n<!--   \\caption{Illustration (Part 2 of 3). Given the prior information provided in Table \\ref{IllusPriors}, the probability of each combination of $X$, $Y$, and $K$ observations can be calculated for each possible state of the world ($\\theta_j$). This in turn allows for the calculation of the posterior probability of each state of the world for each possible pattern of data using Bayes' rule. } -->\n<!--   \\label{IllusPosteriors}% -->\n<!-- \\end{table}% -->\n\n\n<!-- Table \\ref{ill3_posteriors} provides our posteriors on all parameters, with each column representing one possible realization of the data for a single case.  -->\n\n<!-- A few implications of this simple exercise are apparent from Table \\ref{ill3_posteriors}. \\textit{First, the example demonstrates that observing data---even for a single case---allows for updating on all parameters of interest:}  causal effects, the merits of different theoretical accounts, assignment propensities, and the probative value of clues. Note, for instance, how our belief about the ATE falls dramatically when we observe a single $X=1$ case in which $Y=0$ and the clue is present. This shift results from the fact that this pattern of evidence was much more likely under $\\theta_3$ than under $\\theta_1$ or $\\theta_2$. With the shift in confidence toward $\\theta_3$, we in turn place more weight on that vector's constituent beliefs in a relatively high $\\lambda_a$ and a low $\\lambda_b$, and this updating brings down our estimate of the ATE ($\\lambda_b-\\lambda_a$). -->\n\n<!-- The exercise demonstrates the sometimes counter-intuitive nature of Bayesian updating. In particular, we see that \\textit{a belief can gain support from data that are \\textit{unlikely} under that belief---as long as those data are even \\textit{more unlikely} under the alternatives.} Consider, for instance, what happens when we observe $X=Y=1$ and $K=0$. This observation pushes beliefs in theory $L$ above 50\\%, even though such an observation was expected under $\\theta_1$, in which $L$ was believed to be true, with only a 0.23 probability. Yet this observation was even \\textit{more} unexpected under theory $M$ ($\\theta_2$ and $\\theta_3$).  -->\n\n<!-- An important implication for case selection also follows. Theory $L$ might seem to have had only a modest stake in a case with $X=Y=1$ and $K=0$, which it predicted to be somewhat but not extremely unlikely. However, even when a belief makes no strong prediction about a particular configuration of $X,Y$, or $K$ values, \\textit{such a case will have large implications for the belief as long as the \\textit{alternative} implies a sharp and divergent prediction about the case's likelihood of occurring.} -->\n\n<!-- Further, the example shows that, \\textit{in some situations, the most significant updating occurs over analytical assumptions rather than substantive causal effects.} For instance, where $X=Y=K=0$ or $X=Y=K=1$, there is a small loss in confidence in theory $L$ relative to theory $M$ and a small increase in the expected treatment effect. But there is a larger gain in confidence in the probative value of clues. This latter updating occurs because of a substantial shift from $\\theta_3$ to $\\theta_2$, which both contain theory $M$, and which yields a shift in support between the alternative clue probabilities that we believed might be associated with $M$.  -->\n\n\n\n<!-- \\begin{table}[h!]\\small -->\n<!--   \\centering -->\n<!--     \\begin{tabular}{llcccccccc} -->\n<!--     \\toprule -->\n<!--     Event &       & \\begin{sideways}X=1, Y=1, K=1\\end{sideways} & \\begin{sideways}X=0, Y=1, K=1\\end{sideways} & \\begin{sideways}X=1, Y=0, K=1\\end{sideways} & \\begin{sideways}X=0, Y=0, K=1\\end{sideways} & \\begin{sideways}X=1, Y=1, K=0\\end{sideways} & \\begin{sideways}X=0, Y=1, K=0\\end{sideways} & \\begin{sideways}X=1, Y=0, K=0\\end{sideways} & \\begin{sideways}X=0, Y=0, K=0\\end{sideways} \\\\ -->\n<!--     \\hline -->\n<!--     Probability of event: &       & 0.233 & 0.066 & 0.067 & 0.134 & 0.134 & 0.067 & 0.066 & 0.233 \\\\ \\hline -->\n<!--      & \\multicolumn{1}{c}{{$\\lambda_a$}} & 0.09  & 0.15  & 0.15  & 0.07  & 0.07  & 0.15  & 0.15  & 0.09 \\\\ -->\n<!--      & \\multicolumn{1}{c}{{$\\lambda_b$}} & 0.31  & 0.13  & 0.12  & 0.47  & 0.47  & 0.12  & 0.13  & 0.31 \\\\ -->\n<!--      & \\multicolumn{1}{c}{{$\\lambda_c$}} & 0.30  & 0.36  & 0.36  & 0.23  & 0.23  & 0.36  & 0.36  & 0.30 \\\\ -->\n<!--      & \\multicolumn{1}{c}{{$\\lambda_d$}} & 0.30  & 0.36  & 0.36  & 0.23  & 0.23  & 0.36  & 0.36  & 0.30 \\\\ -->\n<!--      & \\multicolumn{1}{c}{{$\\pi_a$}} & 0.50  & 0.50  & 0.50  & 0.50  & 0.50  & 0.50  & 0.50  & 0.50 \\\\ -->\n<!--      & \\multicolumn{1}{c}{{$\\pi_b$}} & 0.50  & 0.50  & 0.50  & 0.50  & 0.50  & 0.50  & 0.50  & 0.50 \\\\ -->\n<!--      & \\multicolumn{1}{c}{{$\\pi_c$}} & 0.29  & 0.34  & 0.35  & 0.40  & 0.40  & 0.35  & 0.34  & 0.29 \\\\ -->\n<!--     Posterior on:  & \\multicolumn{1}{c}{{$\\pi_d$}} & 0.71  & 0.66  & 0.65  & 0.60  & 0.60  & 0.65  & 0.66  & 0.71 \\\\ -->\n<!--      & \\multicolumn{1}{c}{{$\\phi_{a0}$}} & 0.71  & 0.66  & 0.65  & 0.60  & 0.60  & 0.65  & 0.66  & 0.71 \\\\ -->\n<!--      & \\multicolumn{1}{c}{{$\\phi_{a1}$}} & 0.29  & 0.34  & 0.35  & 0.40  & 0.40  & 0.35  & 0.34  & 0.29 \\\\ -->\n<!--      & \\multicolumn{1}{c}{{$\\phi_{b0}$}} & 0.71  & 0.66  & 0.65  & 0.60  & 0.60  & 0.65  & 0.66  & 0.71 \\\\ -->\n<!--      & \\multicolumn{1}{c}{{$\\phi_{b1}$}} & 0.29  & 0.34  & 0.35  & 0.40  & 0.40  & 0.35  & 0.34  & 0.29 \\\\ -->\n<!--      & \\multicolumn{1}{c}{{$\\phi_{c0}$}} & 0.29  & 0.34  & 0.35  & 0.40  & 0.40  & 0.35  & 0.34  & 0.29 \\\\ -->\n<!--      & \\multicolumn{1}{c}{{$\\phi_{c1}$}} & 0.71  & 0.66  & 0.65  & 0.60  & 0.60  & 0.65  & 0.66  & 0.71 \\\\ -->\n<!--      & \\multicolumn{1}{c}{{$\\phi_{d0}$}} & 0.29  & 0.34  & 0.35  & 0.40  & 0.40  & 0.35  & 0.34  & 0.29 \\\\ -->\n<!--      & \\multicolumn{1}{c}{{$\\phi_{d1}$}} & 0.71  & 0.66  & 0.65  & 0.60  & 0.60  & 0.65  & 0.66  & 0.71 \\\\ \\hline -->\n<!--      & \\multicolumn{1}{c}{{$\\eta^L$}} & 0.32  & 0.13  & 0.12  & 0.56  & 0.56  & 0.12  & 0.13  & 0.32 \\\\ -->\n<!--      & \\multicolumn{1}{c}{{$\\eta^M$}} & 0.68  & 0.87  & 0.88  & 0.44  & 0.44  & 0.88  & 0.87  & 0.68 \\\\ -->\n<!-- Implied posterior on: & ATE    & 0.22  & -0.02 & -0.04 & 0.40  & 0.40  & -0.04 & -0.02 & 0.22 \\\\ -->\n<!--     \\bottomrule -->\n<!--     \\end{tabular}% -->\n<!--   \\caption{Illustration (Part 3 of 3)  Posteriors on states of the world, ($\\theta_j$), as calculated in Table \\ref{IllusPosteriors} given data ($X$, $Y$, and $K$), imply posteriors over all quantities of interest.} -->\n<!--   \\label{ill3_posteriors}% -->\n<!-- \\end{table}% -->\n\n<!-- \\clearpage -->\n<!-- \\newpage -->\n\n\n<!-- % WE ARE CUTTING GOOD STUFF THAT COULD BE USEFUL IN AN EXPANDED VERSION -->\n<!-- % Where $\\Omega$ includes a distribution across alternative theories of mechanism, the procedure also produces updated beliefs about which theory is correct. Thus, for example, $\\Omega$ may be structured such that that any constituent vector $\\omega$ that includes element $z^L=1$ also includes $(\\phi_{b1}=0.9,\\phi_{d1}=0.1)$, while any $\\omega$ containing $z^M=1$ includes the distinct clue predictions $(\\phi_{b1}=0.4,\\phi_{d1}=0.4)$. Then, any evidence and updating that results in increased confidence in the first set of clue predictions relative to the second set will also lead to an increase in the weight placed on theory $L$ relative to theory $M$ (and vice versa).  -->\n\n<!-- % NEXT PARAGRAPH, WITH FOOTNOTE, A CANDIDATE FOR CUTTING OR MOVING TO AN ONLINE APPENDIX. 345 WORDS. JUST SAYIN'. -->\n\n\n\n\n<!-- %Do we need this next point? If so, can the point be clarified? How is this different from just mobilizing our posterior clue probabilities and applying them to a case of interest? -->\n<!-- % % IT IS different because you use clue information ON THIS CASE to make an inference about this case -->\n\n\n<!-- % THIS TO APPEAR ELSEWHERE  -->\n<!-- %The basic inferential step provided in Equation \\ref{Bayes} can be extended in additional ways. For instance, it can be used to create updated beliefs about case-specific causal effects. Here, our vector of priors $\\omega$ could include a prior about the set to which a given case belongs. For example, we can let $\\omega$ include the subvector $(\\tau_A,\\tau_B,\\tau_C,\\tau_D)=(0,1,0,0)$, indicating that under $\\omega$, the case in question is of type $b$. -->\n\n### Extensions {#extensions}\n\nThe baseline model simplifies certain features of real-world research situations: treatments and outcomes are binary; neither measurement error nor spillovers occur; there is only one treatment of interest and one clue to examine; and our focus is on a single causal estimand (population-level treatment effects). \n\nImportantly, however, the basic logic of integration underlying the BIQQ framework does not depend on any of these assumptions. In what follows, we suggestively indicate how BIQQ can be extended to capture a wider range of research situations and objectives.\n\n\\subsubsection{Multiple explanatory variables and interaction effects}\\label{ManyX}\nSuppose that, instead of a single explanatory variable, we have $m$ explanatory (or control) variables $X_1,X_2,\\dots, X_m$. As these $m$ variables can take on $2^m$ possible combinations of values, we now have $2^{2^m}$ types, rather than four. Assuming that clue probabilities are conditional on type and the values of the explanatory variables, $\\phi$ would contain $2^{2^m}\\times 2^m = 2^{2^m+m}$ rather than the eight values in our baseline model.   Below, we describe the case with two explanatory variables in more detail and show how this setup allows researchers to examine both interaction effects and equifinality.\n\n\n\\subsubsection{Continuous data} \nWe can similarly shift from binary to continuous variable values through an expansion of the causal types. Suppose that $Y$ can take on $m$ possible values. With $k$ explanatory variables, each taking on $r$ possible values, we then have $m^{r^k}$ causal types and, correspondingly, very many more elements in $\\phi$. Naturally, in such situations, researchers might want to reduce complexity by placing structure onto the possible patterns of causal effects and clue probabilities, such as assuming a monotonic function linking effect sizes and clue probabilities.\n\n\n\\subsubsection{Measurement error}\\label{measurement} %We have assumed no measurement error; in applications there could be considerable interest in measurement error. On one hand clue information may contain information about possible mismeasurement on $X$ and $Y$; on the other hand there might interest in whether measured clues adequately capture those features of a causal process that is thought to be measureable.  \nThe probability of different types of measurement error can be included among the set of parameters of interest, with likelihood functions adjusted accordingly. Suppose, for instance, that with probability $\\epsilon$ a $Y=0$ case is recorded as a $Y=1$ case (and vice versa). Then the event probability of observing an $X=1$,$Y=1$ case, for example, is $\\epsilon \\lambda_a \\pi_a + (1-\\epsilon) \\lambda_b \\pi_b + \\epsilon \\lambda_c \\pi_c + (1-\\epsilon) \\lambda_d \\pi_d$. %If instead there were measurement error on $X$ but not on $Y$, then the event probability would be: $\\epsilon \\lambda_a (1-\\pi_a) + (1-\\epsilon) \\lambda_b \\pi_b + \\epsilon \\lambda_d (1-\\pi_d) + (1-\\epsilon) \\lambda_d \\pi_d$. \nSimilar expressions can be derived for measurement error on $X$ or $K$. Specifying the problem in this way allows us both to take account of measurement error and learn about it.\n\n\\subsubsection{Spillovers}\\label{spillovers} \nSpillovers may also be addressed through an appropriate definition of causal types. For example a unit $i$ that is affected either by receiving treatment or via the treatment of a neighbor, $j$, might have potential outcomes $Y_i(X_i,X_j)=\\max(X_i,X_j)$ while another type that is not influenced by neighbor treatment status has  $Y_i(X_i,X_j)=\\max(X_i)$. With such a set-up, relevant clue information might discriminate between units affected by spillovers and those unaffected.   \n\n\\subsubsection{Complex qualitative data}\\label{ManyK}\nWhile the baseline model assumes a single, binary within-case observation, we can readily incorporate more complex qualitative data. Instead of assuming $K\\in\\{0,1\\}$, we could assume qualitative $K\\in \\mathbb{R}^m$, a multi-dimensional space capturing the possibility of multiple clues, each taking on any number of possible values. Let $\\phi_j(K|X)$ denote a probability density function over $\\mathbb{R}^m$. Probative value then comes from the differences in the $\\phi_j$ densities associated with different causal types, $j$. \n\nMultiple clues can also be correlated in arbitrary ways, allowing for additional learning to emerge from their joint observation. To illustrate, suppose that two binary clues are examined and that these have a joint distribution as given in Table \\ref{twoclues}. In this case, information on $K_1$ or $K_2$ alone provides little information (not even a ``straw-in-the-wind'' test). The combination of clues, however, provides a stronger test. For example, examining whether {both} $K_1$ and $K_2$ are present provides a smoking gun test for a $d$ type. \n\n\\begin{table}[h!]\n  \\centering\n    \\begin{tabular}{c|ccccc}\n    \\hline\n         Type & $K_1=K_2=0$ & $K_1=1,K_2=0$     & $K_1=0,K_2=1$      & $K_1=K_2=1$ & Total \\\\\n    \\hline\n    $b$     & 0     & 1/2  & 1/2  & 0  & 1 \\\\\n    $d$     & 1/4 & 1/4 & 1/4 & 1/4 & 1 \\\\\n    \\hline\n    \\end{tabular}%\n  \\caption{Clue distribution for types $b$ and $d$ (given $X=1$). Cells show the probability of a given clue combination for each causal type.}\n  \\label{twoclues}%\n\\end{table}%\n\n\n### Multiple Causes\nHere we provide more intuition for how the baseline model  can handle multiple causal variables characterized by either equifinality (multiple potential causes of the same outcome) or interaction effects. The core approach is to expand the number of types to take into account the more complex combinations of causal conditions for which potential outcomes must now be defined. Table \\ref{types2X} displays the set of potential outcomes for two binary causal variables. With two causes, $X_1$ and $X_2$, we now have 16 types as defined by the potential outcomes under alternative combinations of causal conditions. \n\n\n\\begin{table}[h!]\n  \\centering\n    \\begin{tabular}{clcccc}\n    \\hline\n    \\textbf{Type} & Label &  $(Y | X_1=0,$ & $(Y |X_1=1, $ & $(Y | X_1=0, $ & $(Y | X_1=1, $ \\\\\n    &       & $X_2=0)$ & $X_2=0)$ & $X_2=1)$ & $X_2=1)$ \\\\  \\hline\n        1&chronic \t\t\t&  0     & 0     & 0     & 0 \\\\\n    2&jointly-beneficial \t& 0     & 0     & 0     & 1 \\\\\n    3&2-alone-beneficial \t& 0     & 0     & 1     & 0 \\\\\n    4&2-beneficial \t\t\t& 0     & 0     & 1     & 1 \\\\\n    5&1-alone-beneficial \t& 0     & 1     & 0     & 0 \\\\\n    6&1-beneficial \t\t\t& 0     & 1     & 0     & 1 \\\\\n    7&any-alone-beneficial \t& 0     & 1     & 1     & 0 \\\\\n    8&any-beneficial \t\t& 0     & 1     & 1     & 1 \\\\\n    9&any-adverse \t\t\t& 1     & 0     & 0     & 0 \\\\\n    10&any-alone-adverse \t& 1     & 0     & 0     & 1 \\\\\n    11&1-adverse \t\t\t& 1     & 0     & 1     & 0 \\\\\n    12&1-alone-adverse \t\t& 1     & 0     & 1     & 1 \\\\\n    13&2-adverse \t\t\t& 1     & 1     & 0     & 0 \\\\\n    14&2-alone-adverse \t\t& 1     & 1     & 0     & 1 \\\\\n    15&jointly-adverse \t\t& 1     & 1     & 1     & 0 \\\\\n    16&destined \t\t\t& 1     & 1     & 1     & 1 \\\\\n    \\bottomrule\n    \\end{tabular}%\n   \\caption{Types given two treatments (or one treatment and one covariate)}\n  \\label{types2X}%\n\\end{table}%\n\n\nTaking interaction effects first, Type 3 (2-alone-beneficial), for instance, is a type in which $X_2=1$ causes $Y=1$ only when $X_1=0$, and not when $X_1=1$. The hypothesis of no interaction effects is the hypothesis that all cases are of type 1, 4, 6, 11, 13, or 16 (that is chronic, destined, 1-beneficial, 2-beneficial, 1-adverse, or 2-adverse). Note that the binary outcome framework excludes possibilities, such as two countervailing or two additive effects of $X_1$ and $X_2.$ % there is no notion of ``1 beneficial and 2 adverse,'' nor can both be unconditionally beneficial. \n\nTurning now to equifinality: In the simple typological setup in the main paper, the difference between a $b$ and a $d$ type already implies equifinality: for a $b$ type, the positive outcome was caused by treatment; for a $d$ type, the same outcome is caused by some other (unspecified) cause. Table \\ref{types2X}, however, explicitly builds multiple causes into the framework. Suppose, for instance, that we have two cases, one of Type 4 and one of Type 6, and that $X_1=X_2=1$ in both cases. For both cases, we will observe $Y=1$. However, for the Type 4 case, the outcome was caused by $X_2$ (in the sense that it would not have occurred if $X_2$ had been 0, but would have even if $X_1$ was 0) whereas the outcome in the Type 6 case was caused by $X_1$, but not by $X_2$.\n\nThe parameters in the model would now be defined in terms of these 16 types and two causal variables. \n\nFor estimating population-level causal effects, we would state priors about the population proportions of these 16 types. \n\nAssignment probabilities would be expressed, separately for each independent variable, as the probability that that each type is assigned to the value 1 on that variable, yielding 32 $\\pi$ values in total. \n\nClue probabilities, finally, would be supplied for each type. In principle, these $\\phi$ values could be made conditional on the combination of $X_1$ and $X_2$ values, potentially yielding 64 $\\phi$ values. In practice, greater structure might facilitate analysis. For example, if a given clue's likelihood depends only on the value of one of the independent variables, rather than that of both, this greatly reduces the required number of $\\phi$ priors.\n\n\n## A grounded model\n\nThe shortcoming of the baseline model is that it is not fully connected to any causal model. The probative values of clues are assumed, rather than deriving from a more fundamental set of beliefs about causal processes.\n\nWe have, however, the elements in hand to do this derivation.\n\nConsider a generalization of the models introduced in Chapter 4 in which a treatment $X$ is a cause of both $K$ and $Y$, and outcome $Y$ is a product of both $X$ and $K$. Though $K$ is both a mediator and a moderator for the effect of $X$. We will let $U^Y$ denote the causal type with respect to $Y$, which can take on one of 16 values (written in the form $t_{pq}^{rs}$), and $u^K$ the type with respect to $K$ which takes on four (capturing the mapping from $X$ to $K$ and written in the form $t_{w}^{z}$).\n\nTo allow for the possibility of non-random selection of $X$ we will assume that the assignment probability for $X$ depends on $U^Y$. This is a feature shared also in the baseline model when we specify $\\pi$ as a function of types $a$,$b$,$c$,$d$.\n\nWe let $\\lambda^j$ denote the probability of the type vector $t^j$.\n\nIn this case the specification of our \"primitives\" requires a specification over the following quantities:\n\n1. A distribution over the 15-dimensional simplex representing possible values of $\\lambda^Y$--which in turn determine types $u^Y$.\n2. A distribution over the 3-dimensional vector representing possible values of $\\lambda^K$,  which in turn determine types $u^K$.\n3. 16 distributions over $[0,1]$ representing assignment propensities for $X$ given $t^Y$. \n\nNote that this model is not fully general. Obviously all elements are binary and a causal order is assumed. Less obviously we implicitly assume that $K$ is independent of $t^Y$ conditional on $X$.\n\nWith these elements in hand, however, all we need now is to provide a mapping from these fundamental parameters to the parameters used in the baseline model to form the likelihood. With this in hand the problem reduces to the baseline problem, albeit with a different procedure for generating priors. Estimation produces estimated both for the fundamental parameters described here and the \"transformed parameters\" used by the baseline model.  \n\nThe key transformation is the identification of causal types resulting from the 64 combinations of $\\lambda^Y$ and $\\lambda^K$. These are shown below.\n```{r, include = FALSE} \nlibrary(biqq)\n\n# Indices on lambda_Y, lambda_K\nlambda_Y <- matrix(perm(rep(2,4)), 16, 4)\nlambda_K <- matrix(perm(rep(2,2)), 4, 2)\n\n# Function to Figure out a causal type given indices on lambda_Y and lambda_K\ncausal_type <- function(iK, iY){\n  Y0 <- lambda_Y[iY, 1 + lambda_K[iK,1]]     #X=0, K= K(0)\n  Y1 <- lambda_Y[iY, 3 + lambda_K[iK,2]]\n  type <- 1 + (Y0 + 2*Y1)\n  c(\"c\", \"a\", \"b\", \"d\")[type]\n  }\n\n# Generate matrix of types\ntypes <- t(sapply(1:16, function(iY){sapply(1:4, function(iK) {causal_type(iK, iY)})} ))\n\n# Function to figure out indices in which K is seen given type and X value\n\nphi_01_given_type <- function(type = \"a\"){\n  t_indices <- which(types==type, arr.ind = TRUE)\n  K_given_t_X_0 <- t_indices[t_indices[,2] %in% (1:4)[lambda_K[,2]==0],] \n  K_given_t_X_1 <- t_indices[t_indices[,2] %in% (1:4)[lambda_K[,2]==1],] \n  list(t = t_indices, phi0 = K_given_t_X_0, phi1 = K_given_t_X_1)}\nabcd <- letters[1:4]\nout  <- sapply(abcd, phi_01_given_type)\n\n\n# translate into a set of text lines\nmake_string <- function(ij){\n  x <- sapply(1:nrow(ij), function(j)  {paste0(ij[j,],  collapse = \"*lambda_K_\")})\n  paste0(\"lambda_Y_\", x, collapse = \"+\")\n  }\n\nour_type_transformations <- sapply(abcd, function(t) {\n  deno  <- make_string( as.matrix(out[\"t\",t][[1]]) )\n  paste0(\"lambda_t_\", t, \" = (\", deno, \")\")\n  })\n\nour_phi_transformations <- sapply(abcd, function(t) {\n  deno  <- make_string( as.matrix(out[\"t\",t][[1]]) )\n  num0  <- make_string( as.matrix(out[\"phi0\", t][[1]]) )\n  num1  <- make_string( as.matrix(out[\"phi1\", t][[1]]) )\n  c(paste0(\"phi_\", t, \"_0 = (\", num0, \")/(\", deno, \")\"),\n    paste0(\"phi_\", t, \"_1 = (\", num1, \")/(\", deno, \")\"))\n  })\n```\n\nConsider the following matrices of values for $u_Y$ and $u_K$, where $\\lambda_{pq}^{rs}$ is the probability that $u^Y = t_{pq}^{rs}$, meaning that $Y$ would take the value $p$ when $X=0, K=0$,  $q$ when $X=0, K=1$,  $r$ when $X=1, K=0$,  $s$ when $X=1, K=1$. Similarly $\\lambda_{w}^{z}$ is hte probability that $u^K$ takes value  $\\t_{w}^{z}$  meaning that $K$ takes the value $w$ when $X=0$ and $z$ when $X=1$.\n\n```{r, include = FALSE}\nK0 <- sapply(1:16, function(i) paste0(lambda_Y[i,1:2], collapse = \"\"))\nK1 <- sapply(1:16, function(i) paste0(lambda_Y[i,3:4], collapse = \"\"))\nrn <- paste0(\"$\\\\lambda_{\", K0, \"}^{\", K1, \"}$\")\n# rn <- paste0(\"\\$\\lambda_{\", K0, \"}^{\", K1, \"}\\$\")\n\na <- cbind(paste(1:16, \":\", rn), types)\n\n```\n\n\nConsider the following matrix of causal types:\n\n\n```{r, echo = FALSE, results = \"asis\", warning = FALSE}\nlibrary(knitr)\nlibrary(xtable)\n# kable(a, col.names = c(\"$u_Y \\\\downarrow u_K \\\\rightarrow$\",\"$1:  \\\\lambda_0^0$\", \"$2: \\\\lambda_0^1$\", \"$3: \\\\lambda_1^0$\", \"$4: \\\\lambda_1^1$\"),  caption = \"\\\\label{tab:types} Row labels indicate $Y$ type defined as value of Y for each combination of $X$ and $K$; columns show $K$ type defined as outcomes for each value of $X$.\", align = c(\"ccccc\"))\n\ncolnames(a) <-  c(\"$u_Y \\\\downarrow u_K \\\\rightarrow$\",\"$1:  \\\\lambda_0^0$\", \"$2: \\\\lambda_0^1$\", \"$3: \\\\lambda_1^0$\", \"$4: \\\\lambda_1^1$\")\nprint(xtable(a,  caption = \"\\\\label{tab:types} Cell entries denote transformed type. Row labels indicate $Y$ type, $t^Y$ (which arises with prior probability $\\\\lambda^Y$)  for each combination of $X$ and $K$; columns show $K$ type, $t^K$, (arising with prior probability $\\\\lambda^K$)  defined as outcomes for each value of $X$.\", align = c(\"c\",\"c\",\"c\",\"c\",\"c\", \"c\")), comment = FALSE, sanitize.text.function = force, include.rownames = FALSE)\n\n```\n\n```{r, comment = NA, include = FALSE, eval = FALSE}\n#We want to define \n\nprint(our_type_transformations)\n\nprint(our_phi_transformations)\n```\n\nThese types are the *transformed parameters*; the probability of a type is just the sum of the probabilities of the fundamental types that compose it, formed by taking the product of the $\\lambda^Y$ and $\\lambda^K$ values marked in the rows and columns of  table \\ref{tab:types}. \n\nSimilarly $\\phi_{tx}$ can be constructed as the probability of observing $K$ conditional on this type (again, sums of products of probabilities associated with cells in table  \\ref{tab:types}). For instance, using the row and column indices in exponents (GIVE FULL LABELS) from table \\ref{tab:types}:\n\n$$\\phi_{b1}=\\frac{\\lambda_K^2(\\lambda_Y^2+\\lambda_Y^4+\\lambda_Y^6+\\lambda_Y^8)+\\lambda_K^4(\\lambda_Y^2+\\lambda_Y^4+\\lambda_Y^{10}+\\lambda_Y^{12})}{\n\\lambda_K^1(\\lambda_Y^3+\\lambda_Y^4+\\lambda_Y^7+\\lambda_Y^8)+\\lambda_K^2(\\lambda_Y^2+\\lambda_Y^4+\\lambda_Y^6+\\lambda_Y^8)+\\lambda_K^3(\\lambda_Y^3+\\lambda_Y^4+\\lambda_Y^11+\\lambda_Y^{12})+\\lambda_K^4(\\lambda_Y^2+\\lambda_Y^4+\\lambda_Y^{10}+\\lambda_Y^{12})}$$\n\n\n\nWith these transformed parameters in hand, the likelihood is exactly the same as that specified in the baseline model.\n\n## Conclusion\n\n**TO COME**s\n",
    "created" : 1500910645356.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2952118657",
    "id" : "1B94CDC7",
    "lastKnownWriteTime" : 1498913793,
    "last_content_update" : 1498913793,
    "path" : "C:/Dropbox/ProcessTracing/6 Book/ii/08-mixing-methods.Rmd",
    "project_path" : "08-mixing-methods.Rmd",
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}
{
    "collab_server" : "",
    "contents" : "# (PART) Foundations  {-} \n\n# Causes and Causal Estimands {#methods}\n\n**\\color{blue}Goal: We introduce the counterfactual causal model and use it to motivate a series of causal estimands of interest. We connect the structural models approach  to the logic of process tracing. We propose defining estimands as claims about nodes on causal graphs and provide illustrations.**\n\n```{r, include = FALSE}\nsource(\"hj_dag.R\")\n```\n\nWhile social scientific methods can be addressed to many sorts of questions, matters of causation have long been central to theoretical and empirical work in political science, economics, sociology, and psychology. Causality is also the chief focus of this book. Causal questions, however, can come in many forms. Before we address how causal inferences can be drawn, we must first define what we mean by causation and unpack and define the different kinds of causal estimands in which we might be interested.\n\n<!-- We start with a description of the causal estimands of interest. We first define a causal effect and then introduce causal graphs and estimands. We give an example of a simple causal model that we return to to illustrate key ideas throughout the paper. -->\n\n## The counterfactual model\n\nThe counterfactual model is the dominant model of causal relations in the social sciences. The basic idea,  sometimes attributed to David Hume^[Hume's writing contains ideas both about causality as regularity and causality as counterfactual. On the latter the key idea is \"if the first object had not been, the second never had existed\" [@hume2000enquiry, Section VIII].] and more recently associated with @splawa1990application and @lewis1973counterfactuals^[See also  @lewis1986causation.], conceptualizes causal relations as relations of \"difference-making.\" In the counterfactual view, $X$ caused $Y$ means: had $X$ been different, $Y$ would have been different. Importantly, the antecedent, \"had $X$ been different,\"  imagines a *controlled* change in $X$, rather than a naturally arising difference in $X$. The counterfactual claim, then, is not that $Y$ is different in those cases in which $X$ is different; it is, rather, that if one could have *made* $X$ different, $Y$ would have been different. \n\n<!-- Causal effects are thus unmeasurable quantities. They are not differences between possible observations in the world, but differences between outcomes in the world and counterfactual outcomes that need to be inferred rather than measured.       -->\n\nIn political science, the potential outcomes framework is commonly employed to describe counterfactual causal relations.  Let $Y(x)$ denote the \"potential\" outcome (the value $Y$ would take on) when $X=x$. Then, if $X$ is a binary variable, the effect of $X$ on $Y$ is simply defined as $Y(1) -  Y(0)$. \n\nConsider, for instance, a situation in which some individuals in a diseased population are observed to have received a treatment while others have not ($X$). Assume that, subsequently, a researcher observes which individuals become healthy and which do not ($Y$). Let us further assume that each individual belongs to one of four unobserved response ``types,'' defined by the potential effect of treatment on the individual:\\footnote{We implicitly invoke a SUTVA. See also \\citet{HerronQuinn} for a similar classification of types.}\n\n\\begin{itemize}\n    \\item \\textbf{a}dverse: Those who would get better if and only if they do not receive the treatment\n    \\item \\textbf{b}eneficial: Those who would get better if and only if they do receive the treatment\n    \\item \\textbf{c}hronic: Those who will remain sick whether or not they receive treatment\n    \\item \\textbf{d}estined: Those who will get better whether or not they receive treatment\n\\end{itemize}\n\nThroughout, we will use the letters, $a,b,c,d$, to denote these causal types and $\\lambda_a,\\lambda_b,\\lambda_c,\\lambda_d$, to denote the relative share of these types in the population. \n\nThese types differ in their ``potential outcomes'' | that is on what outcomes, $Y$,  they \\textit{would} have depending on their treatment condition $X$ \\citep{Rubin1974}. More formally, we let $Y(x)$ denote a case or type's potential outcome  when $X=x$. Thus, the potential outcomes are $Y(0)=1, Y(1)=0$ for type $a$; $Y(0)=0, Y(1)=1$ for type $b$; $Y(0)=0, Y(1)=0$ for type $c$; and $Y(0)=1, Y(1)=1$ for type $d$.  These potential outcomes are illustrated in Table \\@ref(tab:PO).\n\n\n<!-- \\begin{table}[h!]\n\\begin{tabular}{l|cccc} \\small\n\n& Type a & Type b & Type c & Type d \\\\\n\n& \\textbf{a}dverse effects & \\textbf{b}eneficial Effects & \\textbf{c}hronic cases & \\textbf{d}estined cases \\\\\n\\hline\nNot treated &    Healthy &       Sick &       Sick &    Healthy \\\\\nTreated &       Sick &    Healthy &       Sick &    Healthy \\\\\n\\end{tabular}  \n\\caption{Potential Outcomes: What would happen to each of four possible types of case if they were or were not treated.}\n\\label{tabPO}\n\\end{table} -->\n\n\n| \\small      |          Type a          |            Type b           |         Type c         |          Type d         |\n|-------------|:------------------------:|:---------------------------:|:----------------------:|:-----------------------:|\n|             | \\textbf{a}dverse effects | \\textbf{b}eneficial Effects | \\textbf{c}hronic cases | \\textbf{d}estined cases |\n| Not treated |          Healthy         |             Sick            |          Sick          |         Healthy         |\n| Treated     |           Sick           |           Healthy           |          Sick          |         Healthy         |\n\nTable: (\\#tab:PO)Potential Outcomes: What would happen to each of four possible types of case if they were or were not treated.\n\n\n\nFor any given response type, the causal effect of $X$ on $Y$ is $Y(1) -  Y(0)$. Thus, the causal effect is $-1$ for $a$ types, $1$ for $b$ types, and $0$ for both $c$ and $d$ types.\n\nShifting to an illustration from the social world, consider the effect of economic crisis on the collapse of an authoritarian regime (note here, ``collapse'' is the positive outcome, $Y=1$). An $a$ case is one in which crisis, if it occurs, \\textit{prevents} an authoritarian regime from collapsing (effect $= -1$); in a $b$ case, economic crisis, if it occurs, generates authoritarian collapse in a country that would otherwise have remained authoritarian (effect $= 1$); a $c$ case is a regime that will not collapse with or without economic crisis (effect $= 0$); and a $d$ case is one that will collapse with or without crisis (effect $= 0$).\n\nWe can also use potential-outcomes notation and reasoning for more complex causal relations. For example, let $Y(x_1, x_2)$ denote the outcome when $X_1=x_1$ and  $X_2=x_2$. Then the quantity $\\left(Y(1, 1) - Y(0, 1)\\right) - \\left(Y(1, 0) - Y(0, 0)\\right)$ describes the interactive effect of two treatments: it captures how the effect of $X_1$ changing from $0$ to $1$ is different between those situations in which $X_2=1$ and those situations in which $X_2=0$.\n\n\n## Counter-intuitive implications of the counterfactual model\n\nAlthough the counterfactual framework is now widely employed, it contains a set of implications that might sit uncomfortably with common conceptions of how causal inference operates.\n\n\nSecond, it is often intuitive to think of causal processes as sets of transitive relations: if $A$ causes $B$ and  $B$ causes $C$, then we might think that $A$ causes $C$. Yet, in the counterfactual model, causal relations are *not* transitive. In a classic illustration, imagine a boulder that rolls down a hill, causing you to duck, and that ducking in turn saves your life. As a counterfactual matter, the boulder clearly caused the ducking and the ducking your survival. But the boulder rolling down the hill did not save your life. To consider an example from the social realm, think about situations in which action begets reaction. For instance, a rebellion causes a military crackdown, and the military crackdown causes the regime to survive; yet the rebellion did not cause the regime to survive. (For discussions see @hall2004two and @paul2013causation.) The insight has implications both for process tracing and for correlational approaches to establishing causation. Finding in a causal link in a case from $A$ to $B$ and a causal link from $B$ to $C$ is not equivalent to finding that $A$ caused $C$. Likewise, identifying a population-level causal effect of $X$ on some suspected mediator $M$, and another effect of $M$ on $Y$, does not establish an $X\\rightarrow Y$ causal relationship.\n\n<!-- I do not find the next point convincing. -->\n\nThird, notions of causality going back at least to Hume [@hume2000enquiry] treat causal relations as characterized by spatio-temporal contiguity between cause and effect, or at least of the intermediate steps between them. Yet in the counterfactural model, there is no requirement that causes be temporally or spatially connected to their effects. For instance, *potentially* intervening events that did *not* occur can have causal effects, even though they make no spatio-temporal contact with the observable events that seem to lie along the path from $X$ to $Y$. The plague that put Friar John into quarantine meant that he did not deliver the letter to Romeo to inform him that Juliet was not dead, which in turn led to Romeo's death. There is a *causal* path from the plague to Romeo's death, but no *spatio-temporal* one. \n\nFourth, hypothesis-testing at the case level sometimes proceeds as though competing explanations amount to rival causes, where $A$ caused $B$ implies that $C$ did not. But in the counterfactual model, causal relations are neither rival nor decomposable. If two out of three people vote for an outcome under majority rule, for example, then both of the two supporters caused the outcome: the outcome would not have occurred if *either* supporter's vote were different. The causes are not rival. Now, imagine that all three of three voters supported an outcome. Then the three votes jointly caused the outcome. However, this joint cause is not decomposable into its component parts: none of the individual votes had *any* effect on the outcome. A change in any one vote would have made no difference.\n\n<!-- Does the latter example above  satisfy minimality? Is a graph minimal that points from all three yesses to the outcome? -->\n\nThus, there appear to be some tensions between the counterfactual model and some common notions of causality. These tensions largely disappear, however, once we properly specify causal models as systems of causal relations. For this work, Directed Acyclic Graphs provide a powerful tool.   \n\n## Causal Models and Directed Acyclic Graphs\nIn principle, highly complex causal relations can be fully expressed in potential-outcomes notation. However, for structures involving multiple variables, it can be useful to generate a visual representation of causal relations.  In this section, we show how causal models and directed acyclic graphs (DAGs)  can represent substantive beliefs about how the world works. The key ideas in this section can be found in many texts (see, e.g., Halpern and Pearl (2005) and Galles and Pearl (1998)). \n\nWe illustrate the components of causal models and DAGs with a simple model of government survival. Suppose that we believe that governments' survival in office is affected by the independence of the press. We might theorize the presence of a free press as affecting the likelihood of government turnover through the two pathways: a direct path and an indirect path running through the presence of media reports of official corruption. This simple model is represented in Figure \\ref{fig:simpleDAG}. We will speak to this simple model the present section, and then elaborate the model and graph later in this chapter.\n\n```{r, echo = FALSE, fig.width = 5, fig.height = 3,  fig.align=\"center\", out.width='.5\\\\textwidth', fig.cap = \"\\\\label{fig:simpleDAG} $X$, $R$, and $Y$ are endogeneous variables. $U_X$, $U_R$, and $U_Y$ are exogeneous. The arrows show relations of causal dependence between variables, from which relations of conditional independence can be deduced.  This DAG could represent a simple causal model in which the presence of a free press ($X$) affects the likelihood of government turnover ($Y$) via media reports of offical corruption ($R$), with each variable also subject to a random disturbance ($U_i$). Not shown on the graph are the ranges of the variables or the functional relations between them.\"}\npar(mar=c(1,1,1,1))\nhj_dag(x = c(0, 0, 1, 1, 2, 2),\n       y = c(2, 3, 2, 3, 2, 3),\n       names = c(\n         \"X\", \n         expression(paste(U[X])),\n         \"R\",\n         expression(paste(U[R])), \n         \"Y\", \n         expression(paste(U[Y])) \n         ),\n       arcs = cbind( c(2, 4, 6, 1, 1, 3),\n                     c(1, 3, 5, 7, 3, 5)),\n       title = \"A Simple DAG\",\n       padding = .4, contraction = .15) \n\n```\n\nWe consider causal  models  formed out of three components: variables, functions, and distributions.\n\n\\textbf{The variables.} The first component of a causal model are the variables, which are of two kinds: exogenous and endogenous. Exogenous variables are those that influence other variables in the model but themselves have no causes specified in the model. We can think of exogenous variables as random disturbances or shocks to the system described in the model. Endogenous variables are those that are functions of exogenous variables and possibly also of other endogenous variables.^[Conventionally, we denote the set of exogenous variables as \\(\\mathcal{U}\\) and the set of endogenous variables as \\(\\mathcal{V}\\).] In our simple model, graphed in Figure \\ref{fig:simpleDAG}, the endogenous variables are $X$ (presence of a free press), $R$ (Media reports of corruption), and $Y$  (Government removal) since  each of these is influenced by one or more other variables in the model. The exogenous variables include a set of unspecified factors that give rise to a free press ($U_X$) and a set of unspecified exogenous factors (other than a free press) that affect the emergence of media reports ($U_R$) and government removal ($U_Y$). \n\nIn identifying the variables, we also need to specify the \\emph{ranges} across which they can potentially vary. We might specify, for instance, that all variables in the model are binary, taking on the values 0 or 1.\n\nIf we let \\(\\mathcal{R}\\) denote a set of ranges for all variables in the model, we can indicate $X$'s range, for instance, by writing \\(\\mathcal{R}(X)=\\{0,1\\}\\). The variables in a causal model together with their ranges---the triple \\((\\mathcal{U}, \\mathcal{V}, \\mathcal{R})\\)---are sometimes called a \\emph{signature}, \\(\\mathcal{S}\\). \n\n\n**The functions.** Next, we need to specify our beliefs about the causal relations among the variables. For each endogenous variable, we encode information about its direct causal determinants in a causal function.^[The collection of all causal functions in the model can be denoted as $\\mathcal{F}$.] For each endogenous variable, we can specify two kinds of variables as direct causes:  (i) other endogenous variables, which we call the variable's *parents*;^[For variable $V_i$, we write its parents as $PA_i$.] and (ii) an exogenous variable. Thus, for instance, the variable $Y$ in Figure \\ref{fig:simpleDAG} has as its direct causes the variable $R$, its parent (an endogenous variable itself) and the random-disturbance, $U_Y$.^[Any variable with no parents in $\\mathcal V$ must be a function of a member of $\\mathcal U$; otherwise, we could not consider it endogenous. A variable that is a function of one or more members of $\\mathcal V$, however, can be modeled without a $U_i$ term if we believe that it is fully determined by variables specified in $\\mathcal V$.] \n\nSpecifying a function means writing down whatever general or theoretical knowledge we have about the direct causal relations between variables. A function needs to specify how the value that one variable takes on is determined by the values that other variables---its direct causes---take on. That mapping must be such that every combination of values of causal variables generates only one value for the outcome. \n\nWe can specify this relationship in a vast variety of ways. \n\n* If, for instance, an outcome $B=1$ always occurs whenever the cause is present ($A=1$), we can write down the function: $B=A$\n\n* If we believe that the outcome occurs only when *two* conditions are present---say, $A=1$ and $C=1$---this can be easily expressed as: $B=AC$. \n\n* We can similarly express set-theoretic relations of any kind among any number of factors.  If $B$ increases in a linear fashion with $A$, then we can write: $B=\\beta A$, where $\\beta$ is a parameter that we may not know the value of at the outset of a study (and may learn about from the data).\n\n* We can, further, express uncertainty that we have about causal relations. If we believe $B$ to be linearly affected by $A$ but also subject to forces that we do not yet understand and have not yet specified in our theory, then we can write: $B=\\beta A+U_B$,  where $U_B$ represents an exogenous, random disturbance. \n\n* To take this a step further, we may even be uncertain about whether or in what direction $A$ affects $B$, or believe that this relationship varies across cases for reasons that we do not yet understand. We can capture this type of uncertainty as an interaction such as: $B=AU_B$. Here, the value of our random-disturbance term does not merely represent noise around an $A \\rightarrow B$ relationship, but conditions the strength, sign, or existence of the relationship itself.^[Which of these $U_B$ conditions will depend on the $U_B$'s specified range. For instance, if we allow $U_B$ to take on the range [0,1], then it can condition the strength or existence of a positive causal effect of $A$ on $B$, but not the sign of the effect.] We can also express more fundamental uncertainty about causal relations, such as whether they take linear or exponential form.^[Consider, for instance, $B=\\beta A^{U_A}$.] \n\nThe larger point is that models can be constructed to be quite specific or extremely general, depending on the state of prior knowledge about the phenomenon under investigation. More particularly, the use of a structural model *does not require precise knowledge of specific causal relations*, that is, of functional forms. In fact, we can include factors as another variable's \"parents\" even if we are unsure that those factors matter. Including variables on the righthand side in a functional equation allows for the possibility that those variables matter, and in turn sets us up to investigate their possible effect empirically.^[For instance, in $B=AU_B+C$, $A$ will only affect $B$ if $U_B$ takes on a non-zero value. As we discuss below and in later chapters, our investigation might then center on drawing inferences about $U_B$ in order to assess $A$'s causal effect.]\n\nIt is worth underlining a few important aspects of the functions in causal models. First, unlike regression equations and other equations describing data patterns, these functions are intended to express *causal* beliefs. When we write $B=\\beta A$, we do not just mean that the values of $B$ and $A$ are expected to be linearly related. We mean that the value of $A$ *determines* the value of $B$ through this linear function. Functions are, in this sense, meant as *directional* statements, with causes on the righthand side and the effect on the left.\n\nSecond, in a structural-model context, all causal relations are conceptualized as deterministic: a variable's value is fully *determined* by the values of the righthand side terms in its functional equation. We express uncertainty about the value that a variable will take on, given the value of its parents, as random disturbances, the $U$ terms. We note that this understanding of causality---as ontologically deterministic, but empirically imperfectly understood---is compatible both with the potential-outcomes framework, with views of causation commonly employed by qualitative researchers (see, e.g., @mahoney2008toward), and with understandings of causal determinism going back at least to @laplace1901philosophical. \n\nThird, to specify functions is to unpack a potentially complex web of causal relations into its constituent causal links. For each variable, we need only specify how we believe it to be affected by its *direct* causes -- that is to say, its parents and any $U$ term pointing directly into it. Our outcome of interest, $Y$, may be a shaped by multiple, long chains of causality. To theorize how $Y$ is generated, however, we do not need to express $Y$ as a function of all of the variables in the model (i.e., of all of its ancestors). Rather, we write down how $Y$ is shaped by its immediate causes; how those immediate causes are shaped by their immediate causes; and so on. Thus, for instance, in any model compatible with the graph in Figure \\ref{fig:simpleDAG}, we would express $Y$ as a function of $R$ (its parent) and $U_Y$; $R$ as a function of $X$ and $U_R$; and $X$ as a function of $U_X$. A variable's function must include as arguments (righthand-side terms) all, and only, those variables that point directly affect that variable.^[Put differently, the set of a variable's parents is required to be minimal in the sense that a variable is not included among the parents if, given the other parents, the child does not depend on it in any state that arises with positive probability.]\n\n<!-- ]  Variables that have no parents are called *roots*.^[Thus in our usage all elements of $\\mathcal{U}$ are roots, but so are variables in $\\mathcal{V}$ that depend on variables in $\\mathcal{U}$ only.]  We will say that $\\mathcal{F}$ is a set of *ordered structural equations* if no variable is its own descendant and if no element in $\\mathcal{U}$ is parent to more than one element of \\(\\mathcal{V}\\).^[This last condition can be achieved by shifting any parent of multiple children in $\\mathcal{U}$ to $\\mathcal{V}$.] -->\n\n<!-- Do we need to define roots? -->\n\nWe refer to the endogenous direct causes of a variable as its \"parents\", and we use familial terms more generally to describe relations in a causal model. For instance, we say that $R$ is a *child* of $X$. We can also say that $X$ is an \"ancestor\" of $Y$ (a node upstream from $Y$'s parent) and that $Y$ is a descendant of $X$ (a node downstream from $X$'s child).\n\n<!-- For notational simplicity we generally write functional equations in the form $c(a,b)$ rather than $f_c(a,b)$. -->\n\n**The distributions**. So far, we have specified the variables in the model and their causal relations, possibly with uncertainty. As should be clear from the foregoing discussion, a key feature of a properly specified causal model is that *the values of the exogenous variables*---those with no arrows pointing in to them---*are sufficient to determine the values of all other variables in the model.* For instance, in Figure \\ref{fig:simpleDAG}, knowing the values of $U_X$, $U_R$, and $U_Y$ would tell us the values of $X$, $R$, and $Y$. We thus refer to a given set of values for all $U$ terms in a model as a *context*.\n\nIn general, $U$ terms---capturing unspecified disturbances---represent features of the world that we are *not* able to directly observe. In some cases, we may not even have a specific conceptualization of the phenomena in the world to which a $U$ term corresponds. Nonetheless, we may be able to make *inferences* about the value of a $U$ term from observed data. Indeed, given the role of the exogenous terms in a model in determining the operation of the world that the model describes, learning about a case's context becomes central to model-based causal inquiry and, as we shall see, lies at the heart of the framework that we are elaborating.\n\n**Structural Causal Model.** Putting these components together gives a *structural causal model.*  More formally, a **structural causal model** *over* signature $\\mathcal{S}=<\\mathcal{U},\\mathcal{V},\\mathcal{R}>$ is a pair $<\\mathcal{S}, \\mathcal{F}>$, where $\\mathcal{F}$ is a set of ordered structural equations containing a function  $f_i$  for each element $Y\\in \\mathcal{V}$.^[We say that $\\mathcal{F}$ is a set of ordered structural equations if no variable is its own descendant and if no element in $\\mathcal{U}$ is parent to more than one element of \\(\\mathcal{V}\\). This last condition can be achieved by shifting any parent of multiple children in $\\mathcal{U}$ to $\\mathcal{V}$. This definition thus includes an assumption of acyclicity, which is not found in all definitions in the literature.] However, we have not yet inscribed into the model any beliefs about how *likely* or *common* different kinds of contexts might be. Thus, for instance, a structural causal model consistent with Figure \\ref{fig:simpleDAG} may stipulate that $X$ has an effect on $Y$ but will say nothing in itself about the distribution of $X$---beyond limitations on its domain.^[Thus $P(y|x, u_Y)$ would defined by thus structural model (as a degenerate distribution), but $P(x)$, $P(u_Y)$ and $P(x,y, u_Y)$ would not be.] This is, in particular, because the structural model *says nothing about the distribution of $U_X$*, the exogenous force that determines $X$'s value. \n\n**Probabilistic causal models.**  Once we introduce beliefs about the values of the exogenous terms in a model---the context---we have a \"probabilistic causal model.\" A probabilistic causal model entails not just claims about how the world works under different conditions, but also beliefs about what conditions we are likely to face. In general, we can express these beliefs about context as a joint probability distribution over the $U$ terms.^[We assume that the elements of $\\mathcal{U}$ are generated independently of one another. While this is not without loss of generality, it is not as constraining as it might at first appear: any graph in which two $U$ variables are not independent can be replaced by a graph in which these $U$ terms are listed as (possibly unobserved) nodes in $\\mathcal{V}$, themselves generated by a third variable in $\\mathcal{V}$ with, possibly, a parent in $\\mathcal{U}$. Note also that one could envision \"incomplete probabilistic causal models\" in which researchers claim knowledge regarding distributions over *subsets* of $\\mathcal{U}$.] Thus, a probabilistic causal model is a structural causal model coupled with a probability distribution over the model's exogenous variables. A structural causal model might support a claim of the form: \"$X$ causes $Y$ if and only if condition $C$ holds.\" A corresponding probabilistic model, however, might support a stronger claim of the form: \"Condition $C$ arises with frequency $\\pi^C$, and so $X$ causes $Y$ with probability $\\pi^C$.\"\n\n<!-- Am trying to render all the indented paragraphs below as a single, multi-paragraph footnote. Have read up and tried all sorts of things, no luck yet. -->\n\n**TO BECOME A LONG FOOTNOTE....**\n  The assumptions that no variable is its own descendant and that the $U$ terms are generated independently make the model *Markovian*, and the parents of a given variable are Markovian parents. Knowing the set of Markovian parents allows one to write relatively simple factorizations of a joint probability distribution, exploiting the fact (\"the Markov condition\")  that all nodes are *conditionally independent* of their nondescendants, conditional on their parents. Variables $A$ and $B$ are \"conditionally independent\" given $C$ if $P(a|b,c) = P(a|c)$ for all values of $a, b$ and $c$.  \n  To see how this Markovian property allows for simple factorization of $P$ for Figure \\ref{fig:simpleDAG}, note that $P(X, R, Y)$ can always be written as: \n  $$P(X, R, Y) = P(X)P(R|X)P(Y|R, X)$$ \n  If we believe, as in the figure, that $X$ causes $Y$ only through $R$ then we have the slightly simpler factorization:\n  $$P(X, R, Y) = P(X)P(R|X)P(Y|R)$$ \n  Or, more generally:\n\n\\begin{equation} \nP(v_1,v_2,\\dots v_n) = \\prod P(v_i|pa_i)\n(\\#eq:markov)\n\\end{equation}     \n\n  The distribution $P$ on $\\mathcal{U}$ induces a  joint probability distribution on $\\mathcal{V}$ that captures  not just information about how likely different states are to arise but also the relations of conditional independence between variables that are implied by the underlying causal process. For example, if we thought that $X$ caused $Y$ via $R$ (and only via $R$), we would then hold that $P(Y | R) = P(Y | X, R)$: in other words if $X$ matters for $Y$ only via $R$ then, conditional on $R$, $X$ should not be informative about $Y$.   \n  In this way, a probability distribution $P$ over a set of variables can be consistent with some causal models but not others. This does not, however, mean that a specific causal model can be extracted from $P$. To demonstrate with a simple example for two variables, any probability distribution on $(X,Y)$ with $P(x)\\neq P(x|y)$ is consistent both with a model in which $X$ is a parent of $Y$ and with a model in which $Y$ is a parent of $X$.\n**LONG FOOTNOTE ENDING HERE.**  \n  \nWe need not say much more, for the moment, about the probabilistic components of causal models. But to foreshadow the argument to come, our beliefs about the likelihoods of different contexts play a central role in the framework that we are developing. We will both use beliefs about context to support causal inferences and, as we observe data, update those beliefs on the way to drawing inferences about causal effects.\n\n\n### Features of causal graphs\n\nIn visually illustrating the components of causal models, we have already employed what is called a causal graph in Figure \\ref{fig:simpleDAG}. We now explicitly point out a number of the features of causal graphs, as we will be using them throughout this book. \n\nWe can represent elements of a causal model as a set of vertices (or nodes), representing variables, connected by a collection of directed edges (single-headed arrows).  We add a directed edge from node $A$ to node $B$ if and only if $A$ has a direct effect on $B$. The resulting diagram is  a *directed acyclic* graph (DAG) if there are no paths along directed edges that lead from any node back to itself (no cycles). \n\nThe drawing of causal graphs follows two rules. First, the *absence* of an arrow between $A$ and $B$ means that $A$ is not a direct cause of $B$.^[By \"direct\" we mean that the effect is not fully mediated by one or more other variables in the model.] Here lies an important asymmetry: drawing such an arrow does not mean that we know that $A$ *does* directly cause $B$; but omitting such an arrow implies that we know that $A$ does *not* directly cause $B$. \n\nSecond, any cause common to multiple variables must be represented on the graph: this rule ensures that we are capturing all potential correlations among variables suggested by our theory.^[More specifically, these two conditions define a ``causal DAG'' [@hernan2006instruments]. Further, the specification that $\\mathcal{F}$ is a set of ordered structural equations ensures that the graph is acyclic.] \n\nExplicitly adding $U$ terms is considered optional; in common practice, $U$'s are excluded from the visual representation, with the understanding that every variable on the graph has a $U$ term pointing into it. We will generally draw the $U$ terms where they are of particular theoretical or analytical interest but will otherwise omit them. Whether we include or omit $U$ terms, we will generally treat those nodes in a graph that have no arrows pointing into them as the exogenous variables that define the context.\n\nAs should be clear, a DAG does not represent all features of a causal model: it records which arguments enter into the structural equation for each variable---what may cause what---but contains no other information about the form of those relations. Thus, for instance, the DAG in Figure \\ref{fig:simpleDAG} tells us that $Y$ is function of both $X$ and $R$ (as well as of $U_Y$) but it does not tell us if that joint effect is additive, interactive, or anything else about the structure of causation (i.e., functional form).\n\n\n###Conditional independence from DAGs\n\nHowever, the rules of causal-graphical representation make DAGs informative in another critical way. In particular, they allow for an easy reading of *conditional independencies* between variables in the model. \n\nWe say that two variables, $A$ and $B$, are \"conditionally independent\" given a set of variables $\\mathcal C$ if, once we have knowledge of the values in $\\mathcal C$, knowledge of $A$ provides no information about $B$ and vice-versa. Taking $\\mathcal C$ into account thus \"breaks\" any relationship that might exist unconditionally between $A$ and $B$. For instance, suppose that war is a cause of both military casualties and inflation. Military casualties and inflation will then be (unconditionally) correlated with one another because of their shared cause. Thus, if I learn that there have been military casualties, this information will lead me to think it more likely that there is also inflation (or vice versa). However, assuming that war is their only common cause, we would say that military casualties and inflation are conditionally independent given war. Across cases where war is present (holding war constant), we should find no correlation between military casualties and war; likewise, for cases in which war is absent. Equivalently, if I already know that there has been a war, then learning that there have been military casualties provides no additional information about the likelihood that there has also been inflation. \n\nRelations of conditional independence are central to the strategy of statistical control, or adjustment, in correlation-based forms of causal inference, such as regression. In a regression framework, identifying the causal effect of an explanatory variable, $X$, on a dependent variable, $Y$, requires the assumption that $X$'s value is conditionally independent of $Y$'s potential outcomes (over values of $X$) given the model's covariates. To draw a causal inference from a regression coefficient, in other words, we have to believe that including the covariates in the model \"breaks\" any biasing correlation between the value of the causal variable and its unit-level effect.\n\nAs we will explore, however, relations of conditional independence are of much more general interest in that they tell us, given a model, *when information about one feature of the world may be informative about another feature of the world, given what we already know*. By identifying the possibilities for learning, relations of conditional independence are thus critical to research design.\n\n\n<!-- Some possibilities are excluded by the framework, however: for example, one cannot represent uncertainty regarding whether $A$ causes $B$ or $B$ causes $A$. -->\n\n<!-- It would be nice to make a less general statement than the above as it sounds like none of this has any relevance if we think there's reciprocal causation in the causal system of interest; and that sounds like it excludes A LOT of problems political scientists are interested in. Pearl has a bit of discussion of the fact that one can compute the effect of interventions for models with cyclic features as well. So can we put this point more narrowly? Have been looking into this a bit but not yet sure exactly how to do that. -->\n\n\n\n\n<!-- The above point is something I want to get more clarity around. Also, why we are defining roots as we are. -->\n\n<!-- In Figure \\ref{fig:simpleDAG} we show a simple DAG that represents a situation in which $X$ is a parent of $M$, and $M$ is a parent of $Y$. In this example, the three variables $U_X$, $U_M$, and $U_Y$ are all exogenous and thus elements of \\(\\mathcal{U}\\). $X$, $M$, and $Y$ are endogenous and members of  \\(\\mathcal{V}\\). If these three variables were binary, then there would be eight possible realizations of outcomes, i.e., of \\(\\mathcal{V}\\). In the underlying model,  $U_X$ is an ancestor of $X$, $M$, and $Y$ which are all descendants of $U_X$. The elements of $\\mathcal{U}$ are all roots, though $X$ is also  a root as it has no parent in $\\mathcal{V}$.   -->\n\n \n<!-- In addition we will usually omit variables from a graph only if they are single parents---this has the advantage of clarifying that all uncertainty is over the value of roots, and not over functional forms given roots; this is without loss of generality as parameters for functional equations can themselves be represented as roots.      -->\n\n<!-- As a very simple example one might imagine that $A$ an $B$ are independently generated binary variables; $C$ is an indicator for whether $A$ and $B$ have the same value. Then obviously if you know $C$, then knowing $A$ tells you everything about $B$. -->\n\nTo see how a DAG can reveal conditional independencies, it is useful first to note three pairs of features of the flow of information in causal graphs:\n\n(1a) Information can flow unconditionally along a path of arrows pointing in the same direction. If we have $X\\rightarrow Y \\rightarrow Z$, then learning only $X$ can provide information about $Z$ and vice-versa. \n\n(1b) Learning the value of a variable along a path of arrows pointing in the same direction *blocks* flows of information across that variable. If we have $X\\rightarrow Y \\rightarrow Z$, then knowing the value of $Y$ renders $X$ no longer informative about $Z$, and vice versa: anything that $X$ might tell us about $Z$ is already captured by the information about $Y$.\n\n(2a) Information can flow unconditionally across the branches of any forked path. If we have $X\\leftarrow Y\\rightarrow Z$, then learning only $X$ again can provide information about $Z$ and vice-versa.\n\n(2b) Learning the value of the variable at the forking point blocks *flows* of information across the branches of a forked path. If we have $X\\leftarrow Y\\rightarrow Z$, then learning $X$ provides no information about $Z$ if we already know the value of $Y$.^[Readers will recognize this statement as the logic of controlling for a confound that is a cause of both an explanatory variable and a dependent variable to achieve conditional independence.]\n\n(3a) When two or more arrowheads collide, generating an inverted fork, there is no unconditional flow of information between the incoming sequences of arrows. If we have $X\\rightarrow Y\\leftarrow Z$, then learning only $X$ provides no new information about $Z$, and vice-versa. In the language used by Pearl and others, $Y$ here is a \"collider\" for $X$ and $Y$, a child of two or more parents.\n\n(3b) Collisions can be sites of *conditional* flows of information. Although information does not flow unconditionally across colliding sequences, it does flow across incoming sequences *conditional* on knowing the value of the collider variable or any of its descendants. If we have $X\\rightarrow Y\\leftarrow Z$, then learning $X$ *does* provides new information about $Z$, and vice-versa, if we also know the value of $Y$ (or any of its descendants). \n\nThe last point is somewhat counter-intuitive. It is commonly understood that, for two variables that are correlated unconditionally, that correlation can be \"broken\" by controlling for a third variable. Less obviously, in the case of collision, two variables that are unconditionally independent are *correlated* conditional on a third variable, the collider. The reason is in fact quite simple: if an outcome is a joint function of two inputs, then if we know the outcome, information about one of the inputs can provide information about the other input. For example, if I know that you have brown eyes, then learning that your mother has blue eyes makes me more confident that your father has brown eyes. \n\nUsing these basic principles, conditional independencies can be read off a DAG in the following way. If we want to know whether two variables, $A$ and $B$, are conditionally independent given some set of variables, $\\mathcal C$, we need to find out whether any paths between $A$ and $B$ are \"active\" given $\\mathcal C$. Put slightly differently, the question is whether variables in $\\mathcal{C}$ block information flows from $A$ to $B$---or rather allow, or even create, such flows. This can be assessed as follows. For each possible path between $A$ and $B$, we check whether there are three connected variables $X, Y, Z$^[Where $X$ and/or $Y$ may be $A$ and/or $B$.] on that path that either:\n\n(a) form a \"chain\" $X\\rightarrow Y \\rightarrow Z$ (going either direction) or \"fork\" $X\\leftarrow Y\\rightarrow Z$, with $Y \\subset \\mathcal{C}$,^[For instance, under this criterion, if we have $A\\rightarrow W\\rightarrow Y \\rightarrow B$, and $Y$ is an element of the set $\\mathcal{C}$, then this path is not active given $\\mathcal{C}$. Similarly, if we have $A \\leftarrow X\\leftarrow Y\\rightarrow Z \\rightarrow B$, and $Y$ is in $\\mathcal{C}$, then the path is not active given $\\mathcal{C}$.] or \n\n(b) form an \"inverted fork\" $X \\rightarrow Y \\leftarrow Z$, with neither $Y$ nor its descendants in $\\mathcal C$.^[For instance, if we have $A \\rightarrow W \\rightarrow Y \\leftarrow Z \\leftarrow B$, and $Y$ is *not* in $\\mathcal{C}$, then the path is not active given $\\mathcal{C}$. In other words, \"inverted fork\" paths are not unconditionally active. Knowing only $A$, in this setup, tells us nothing about $B$, and vice-versa.] \n\nIf either of these conditions holds, then the path is not active given $\\mathcal C$. In the first case, any possible information flows along the path are *blocked* by a variable in $\\mathcal C$. In the second case, *no* variable in $\\mathcal C$ is *creating* an information flow that would not otherwise be present. If there are no active paths, then $A$ and $B$ are conditionally independent given $\\mathcal C$. In the graph-analytic language of Pearl and others, $A$ and $B$ are said to be \"$d$-separated\" by $\\mathcal{C}$.^[There are multiple techniques for establishing $d-$separation. Pearl's guide \"$d-$separation without tears\" appears in an appendix to @pearl2009causality.]  \n\nThus, in Figure \\ref{fig:simpleDAG}, we can readily see that $X$ and $Y$ are conditionally independent given $R$: $X$ and $Y$ are *d-*separated by $R$.  Conversely, $R$ and $U_Y$ are unconditionally independent. However, conditioning on $Y$ $d-$*connects* $R$ and $U_Y$, generating a dependency between them. If and only if we know the outcome, $Y$, then learning $R$ yields information about $U_Y$. To foreshadow a point that we develop further later in the book, this analysis reveals how we can, and cannot, learn empirically about elements of our models. For instance, if $U_Y$ is a variable of interest but not directly observable, then information on $R$ is unhelpful if we have not observed the outcome, $Y$, but *is* informative if we have.\n\n\n###Interventions in causal graphs\n\nA second advantage of causal graphs is that they provide a useful structure for thinking through causal effects. In a causal-model framework, we think of a variable's causal effect as being the effect of an imagined *intervention*: a manipulation of some variable, $X$, that provides $X$ with a value that is *not* determined by its parents.  In an intervention, it is as if the causal function for $X$ is replaced by the function $X=x$, with $x$ being the constant value to which we have set $X$. If we take $X$ to be binary, to keep things simple, then the causal effect of $X$ on $Y$ is the difference between the value $Y$ would take on under the intervention $X=0$ (which can be written as $do (X)=0$) and the value $Y$ would take on under the intervention $X=1$ ($do(X)=1$). \n\nAs this manipulative account of causation makes clear, analyzing the effect of $X$ requires us we to set aside the \"natural causes\" of $X$ itself in a particular way. We can readily see how this works graphically by considering a modified version of our free press/government survival model as displayed in Figure \\ref{fig:DAGdirect}. As compared with Figure \\ref {fig:simpleDAG}, this DAG represents model in which $X$ has effects on $Y$ both indirectly through $R$ and directly. We might imagine, for instance, that part of the effect of a free press ($X$) on government turnover ($Y$) runs via media reports of official corruption ($R$) while part of the effect runs through a deterrent effect of a free press that reduces graft and thus leaves greater public resources for investment in public goods (neither of which mediating variables are represented on the graph).\n\nSuppose that we want to estimate the effect of an increase in media reports of corruption, $R$, on government survival, $Y$. We thus want to know the difference between the value that $Y$ would take on if the number of media reports were set at a  high level and the value $Y$ would take on if media reports were set to low. As should be clear from the graph, we cannot estimate this difference by comparing the observed value of $Y$ when media report levels are high to the observed value of $Y$ when media report levels are low. The reason is that $R$ has an ancestor, $X$, that affects $Y$ via a path that does not run through $R$. Thus, the value that $Y$ takes on when $R$ is observed to be high (or low) will be determined not just by $R$ but also by $X$, which is itself systematically correlated with $R$ by being its ancestor. The difference between $Y$'s values at two values of $R$ will thus itself be a combination of $R$'s effect and of $X$'s effect.\n\nInstead, the query, \"What is the effect of $R$ on $Y$?\", must be conceived of in a way that separates out the effect of $X$ on $R$. We can represent the empirical conditions that we would need to estimate this effect in Figure \\@ref(fig:DAGdirectmut). We have \"mutilated\" the original DAG by removing all arrows pointing into our causal variable, $R$ (here, simply the arrow running from $X$ to $R$). This graph represents a world in which $R$ has been manipulated, rather than naturally caused. (Equivalently, it is a world in which $R$ is purely exogenous.) And what the mutilated graph tells us is that, in an empirical situation in which the  dependency of $R$ on $X$ could be removed, $R$'s causal effect on $Y$ *could* be estimated by comparing $Y$'s values at high and low levels of $R$.\n\n**TO BECOME A LONG FOOTNOTE....**\nIn the formulation used in  @pearl2009causality, an intervention involving an endogenous variable $V_i$ can be written as $do(V_i)=v_i'$, or for notational simplicity $\\hat{v}_i'$ (meaning $V_i$ is forced to take the particular value $v_i'$). The resulting distribution can be written as a modified version of Equation \\@ref(eq:markov):\n\n\\begin{equation} \nP(v_1,v_2,\\dots v_n|\\hat{v}_i) = \\prod_{-i}P(v_j|pa_j)\\mathbb{1}(v_i = v_i')(\\#eq:eqdo)\n\\end{equation}\n\nwhere $-i$ indicates that the product is formed over all variables $V_j$ other than $V_i$, and the indicator function ensures that probability mass is only placed on vectors (or worlds) with $v_i = v_i'$. This new distribution has a graphical interpretation, representing the probability distribution over a graph in which all arrows into $V_i$ are removed. \n\nThe difference between Equation \\@ref(eq:markov) and \\@ref(eq:eqdo) is the difference between an *observed* probability distribution and the effect of an *intervention*. The key differences is that, when we intervene to manipulate a variable, we break the link between the variable and its \"natural\" causes.\n**LONG FOOTNOTE ENDING HERE**\n\nIt is a separate question how such an empirical situation might be generated. But it is not hard to see from Figure \\ref{fig:DAGdirect} that the  dependency of $R$ on $X$ could be removed either via (i) directly manipulating $R$ in a manner orthogonal to $X$ or (ii) controlling for $X$, since any variation in $R$ conditional on $X$ would itself be independent of $X$. Equivalently, using graph-analytic logic, we can also see that conditioning on $X$ blocks the path between $R$ and $Y$ that generates the confound (called a \"backdoor\"), thus expunging the confounding effect from the observed correlation between $R$ and $Y$.\n\n\n```{r, echo = FALSE, fig.width = 5, fig.height = 3,  fig.align=\"center\", out.width='.5\\\\textwidth', fig.cap = \"\\\\label{fig:DAGdirect} As compared with Figure \\ref {fig:simpleDAG}, this DAG represents a model in which $X$ has effects on $Y$ both indirectly through $R$ and directly. We might imagine, for instance, that part of the effect of a free press ($X$) on government turnover ($Y$) runs via media reports of offical corruption ($R$) while part of the effect runs through a deterrent effect of a free press that reduces graft and thus leaves greater public resources for investment in public goods (neither of which mediating variables are represented on the graph).\"}\npar(mar=c(1,1,1,1))\nhj_dag(x = c(0, 1, 2),\n       y = c(2, 3, 2),\n       names = c(\"X\", \"R\", \"Y\"),\n       arcs = cbind( c(1, 2, 1),\n                     c(2, 3, 3)),\n       title = \"A DAG with Indirect and Direct Effects\",\n       padding = .4, contraction = .15) \n\n```\n\n\n```{r, echo = FALSE, fig.width = 5, fig.height = 3,  fig.align=\"center\", out.width='.5\\\\textwidth', fig.cap = \"\\\\label{fig:DAGdirectmut} This DAG represents a 'mutilated' version of the previous graph in which the causes of $R$ have been removed, and thus captures the empirical relations that would be required to hold to estimate the effect of $R$ on $Y$.\"}\npar(mar=c(1,1,1,1))\nhj_dag(x = c(0, 1, 2),\n       y = c(2, 3, 2),\n       names = c(\"X\", \"R\", \"Y\"),\n       arcs = cbind( c(2, 1),\n                     c(3, 3)),\n       title = \"A 'Mutilated' DAG for Estimating the Effect of R on Y\",\n       padding = .4, contraction = .15) \n\n```\n\n\n\n\n\n<!-- Suppose, for instance, that democracy ($D$) causes higher levels of private investment ($I$) and greater public-goods provision ($P$), and that both of these cause faster economic growth ($G$). If we want to know the joint distribution of public-goods provision and democracy, we would use Equation \\ref{eqmarkov}, conditioning on the values of the parents of these two variables. If, however, we want to know what level of growth would be produced by an increase public-goods provision---a causal question---we have to ask  -->\n\n\n\n<!-- ```{r, echo = FALSE, fig.width = 11, fig.height = 11.5, fig.align=\"center\", out.width='\\\\textwidth', fig.cap = \"\\\\label{fig:intervention} The main panel shows a simple causal model. $S$ and $X$ are stochastic, other variables determined by their parents, as shown in bottom right panel. Other panels show four possible histories that can arise depending on values taken by $S$ and $X$, along with causal relations in each case. The equations for $S$ and $X$ are written with indicator variables, which take a value of 1 whenever the $u$ value is less than the $\\\\pi$ value.\", fig.align=\"center\", warning = FALSE} -->\n\n<!-- hj_dag(x = c(1, 2, 2, 3), -->\n<!--        y = c(1, 2, 0, 1), -->\n<!--        names = c(\"D\",\"I\", \"P\", \"G\"), -->\n<!--        arcs = cbind( c(1, 1, 3, 2), -->\n<!--                      c(2, 3, 4, 4)), -->\n<!--        add_functions = 0, -->\n<!--        contraction = .2 -->\n<!--        ) -->\n<!-- title(\"A causal model\") -->\n<!-- ``` -->\n\n\n\n<!-- Not completely following the logic this next paragraph or sure if it is helpful here -->\n\n<!-- So far, although not completely general, the focus on causal DAGs is consistent with most approaches used in qualitative work on process tracing, in qualitative case analysis, and in econometric approaches. Some of these approaches commonly assume simple functional forms but these impositions are not implied by the general approach. For example econometric models often impose linear assumptions---for example in work on linear structural equations. Qualitative case analysis often assume all units are binary and that outcomes are deterministic. Under some representations the latter assumption implies conditional independencies that cannot be read from the graph, and thus violate stability conditions commonly assumed of the probability distributions that graphs are meant to represent (though it is still always that case that one can tell from the graph whenever two sets of variables are not conditionally independent given some other set). In our running example described below we give an example of such deterministic relations. -->\n\n\n\n\n\n### Causal queries\n\nIn studying causation, social scientists ask a wide range of causal questions. We are, variously, interested in:\n\n* Case-level causal effects: What is the effect of $X$ on $Y$ in a given case?\n\n* Causal attribution: Did the condition $X$, present in a case, cause the outcome, $Y$, that occurred in that case?\n\n* Actual causes: Which of the antecedent conditions present in the case either was a counterfactual cause of the outcome or *could* have been a counterfactual cause given the way in which events actually played out?\n\n* Average causal effects: What is the mean effect of $X$ on $Y$ across a population of cases?\n\n* Causal pathways: How did $X$ exert its effect on $Y$ in a case? How does $X$ affect $Y$ in a population of cases?\n\n<!-- Some causal questions involve realized values of variables only, some involve counterfactual statements, and some involve combinations of these.  -->\n\nIn this section, we sort through how various causal questions of interest to social scientists map onto, or can be described in terms of, causal models. Assessing many causal questions requires understanding multiple parts of a causal model. In what follows we advocate an approach in which causal questions --- which we term *queries* --- can be defined as questions about the *values of exogenous nodes on a causal graph*, including unobservable $U$ terms. ^[With some abuse of notation we use $Q$ generically to refer to the query itself and the the set of variables whose values determine the query. Thus a query may be written as the random variable $Q =\\mathbb{1}((u_X = 1) \\& (u_Y = 0))$, which takes on a value $q=1$ if both $u_X = 1$ and $u_Y = 0$ and 0 otherwise. Assessing this query requires understanding the values of particular roots, or query nodes, $\\{U_X, U_Y\\}$ which we also refer to as $Q$.]  Addressing a given causal question then involves using data on observed features of a graph to make inferences about those unobserved or unobservable features of the graph that define the query. These inferences are, of course, always conditional on the graph itself. \n\n<!-- (a) uncertainty about causal questions is represented as uncertainty about  and (b)  -->\n\nIn this framework, inferences about causation amount to inferences about the *context* that a case is in: that is, whether conditions in the case (the relevant exogenous-variable values) are such that a given causal effect, causal pathway, etc. would have been operating. We can translate questions about causation into questions about context because, in a structural causal model, the values of all exogenous variables are sufficient to determine the value of all endogenous nodes: context determines outcomes. This further implies that, for any manipulation of an exogenous or endogenous variable, there exist one or more exogenous nodes on the graph that suffice to determine the effect on all endogenous variables in the graph: context determines *effects*. Likewise, the settings on the model's exogenous variables determine the pathway(s) through which one variable in the model will affect another.\n\nIt is important to note a difference between this formulation and the conceptualization of causality typically employed in the potential outcomes framework. We characterize causal inference as learning about a unit *as it is*, conditional on a causal model, rather than learning about the unit as it is and as it could be. Suppose, for instance, that in a causal model a car will start if it has gas and if the key is turned.^[A version of this example is in @darwiche1994symbolic.] In a standard potential outcomes setup, the question \"Does turning the key cause the car to start?\" is equivalent to asking, \"Would the car start if the key is turned?\" and \"Would the car start if the key is not turned?\" In our model-based framework, the question of the key-turning's causal effect is somewhat differently framed as a question about an exogenous variable: \"Does the car have gas?\" In the model-based framework, then, our query becomes a question about the state of affairs in the case---about the case's *context*---rather than a pair of factual and counterfactual questions about outcomes with and without treatment. These two framings are fully consistent with one another, and counterfactual reasoning is no less important in the model-based framework; it has simply been displaced to the causal model, which encodes all counterfactual relations.\n<!-- moreover this can always be done formally, even if the causal model contains no additional assumptions about the causal process.      -->\n\n**Case-level Counterfactual Causes and Causal Attribution.** The simplest quantity of interest is the question of whether there is a case-level counterfactual causal effect. Does $X$ cause $Y$ in this case? The closely connected question of causal attribution [@yamamoto2012understanding] asks: did $X$ cause $Y$'s value in this case?\n\nConsider again our four causal types, above. In this setup, $X$'s causal effects on $Y$ can vary across cases. We can readily translate this setup, in which different cases have different causal effects, into a structural causal model. We can do so by letting $Y$ be a function both of $X$ and of a *response-type variable* that encodes potential outcomes, a variable that we will denote as $Q$. We represent this simple model graphically in Figure \\ref{fig:DAGtypes}. Here $Q$ can be thought of as variable that conditions the effect of $X$ on $Y$. \n\nWe then need to specify the values that $Q$ can take on, $Q$'s range. With a binary causal variable of interest, we can write down a value of $Q$ as $q_{ij}$. The pair of subscripts simply conveys the type's potential outcomes: $i$ represents the value that $Y$ takes on if $X=0$ while $j$ represents the value that $Y$ takes on if  $X=1$. Thus, in a binary framework, $Q$ can take on four values, corresponding to our original four types: $q_{00}$ (a $c$ type), $q_{10}$ (an $a$ type), $q_{01}$ (a $b$ type) and $q_{11}$ (a $d$ type). This setup also allows us to write down a simple, closed-form functional equation for $Y$ in terms of its parents, $X$ and $Q$: $Y(x,q_{ij}) =  i(1-x) + jx$.^[To generate this closed-form function, we decompose $q_{ij}$ into its component parts, $i$ and $j$. Note that there is no loss of generality in the functional form linking $X$ and $Q$ to $Y$. In a causal model framework, the structural equations, such as those linking $X$ and $Y$ conditional on another node, can be entirely non-parametric.]\n\n\n```{r, echo = FALSE, fig.width = 5, fig.height = 3,  fig.align=\"center\", out.width='.5\\\\textwidth', fig.cap = \"\\\\label{fig:DAGtypes} This DAG is a graphical representation of the simple causal setup in which the effect of $X$ on $Y$ in a given case depends on the case's response type, represented by $Q$. With a single binary causal variable of interest, we let $Q$ take on values $q_{ij}$, with $i$ representing the value $Y$ takes on if $X=0$ and $j$ representing the value $Y$ takes on if  $X=1$. With a binary framework outcome, $Q$ ranges over the four values: $q_{00}$, $q_{10}$, $q_{01}$ and $q_{11}$.\"}\npar(mar=c(1,1,1,1))\nhj_dag(x = c(0, 1, 1),\n       y = c(1, 1, 2),\n       names = c(\"X\", \"Y\", \"Q\"),\n       arcs = cbind( c(1, 3),\n                     c(2, 2)),\n       title = \"A DAG with Response Type, Q\",\n       padding = .4, contraction = .15) \n\n```\n\n\n\n<!-- Let $\\pi^Q$ denote a multinomial distribution over these four values and let t -->\n\nIn this setup, then, the query, \"What is $X$'s causal effect in this case?\" becomes a question about the value of the exogenous variable, $Q$. We leave for later the question of how we go about learning about $Q$. \n\nWe note that, in this discussion, we are employing a more generic property of causal graphs. In a graph of the general form $X \\rightarrow Y \\leftarrow Z$, the effect of $X$ on $Y$ in a case will depend on the value of $Z$ in that case. $Z$ in this structure might be a random disturbance term, $U_Y$, or a variable with a substantive interpretation. Either way, where a node has multiple parents, we should generally conceive of the parents as exerting their effects interactively. There are special situations in which $X$'s effect will not depend on the value of $Z$. For instance, if $Z$ operates only additively on $Y$ (say, $Y=X+Z$) and $Y$ is not bounded, then $Z$ is irrelevant to $X$'s causal effect, which will be homogeneous across cases and fixed by the model. But, in general, the causal effect of a parent on its child will depend on the value(s) of the other parent(s) (its spouse(s)).^[Nodes that share a child are spouses.] In this sense, for a given $X \\rightarrow Y$ relationship, any other parents of $Y$ can be thought of as response-type variables; these are the variables that define $X$'s case-level causal effect. Put differently, learning about the case-level effect of a causal variable on an outcome means learning about the outcome's other cause(s).\n\nNote also that, in the above illustration, the variable $Q$ is not specified in substantive terms; it is a carrier for causal information. However, social scientific theories commonly use substantive concepts as response-type variables. The effect of fiscal stimulus on economic growth is theorized to depend on the unemployment rate; the effect of public opinion on policy is held to depend on institutional arrangements; the effect of natural resources on civil war might depend on the level of economic development. Any time one variable moderates the influence of another, the two variables operate as response-type nodes for one another's effects on the outcome. Later in this chapter and in other chapters, we work with further examples in which the exogenous variables that define a query have a stronger substantive interpretation.   \n\nMore generally, work in graphical models defines the causal effect of $X$ on $Y$ in terms of the changes in $Y$ that arise from interventions on $X$. For example, using the notation for interventions given above we can describe the effect of a change in $X$ from $x'$ to $x''$ on the probability that $Y=1$ in unit $i$ as:\n\n\\begin{equation}\nP(y=1|\\hat{x}'_i) - P(y=1|\\hat{x}''_i) (\\#eq:ate)\n\\end{equation}\n\nwhere $P(y=1|\\hat{x}'_i)$ is calculated from the marginal distribution of $y$ given the post-intervention distribution described by Equation \\@ref(eq:eqdo) above. With $y$ expressed as a function of $x$, this quantity reduces to a statement about the probability of $Q$ taking on a given value (i.e., of the relation between $Y$ and $X$ taking a particular functional form).\n\nA query about causal attribution is related to, but different from, a query about a case-level causal effect. When asking about $X$'s case-level effect, we are asking, \"*Would* a change in $X$ cause a change in $Y$ in this case?\" The question of causal attribution is, \"*Did* $X$ cause $Y$ to take on the value it did in this case?\" We can define a causal-attribution query in terms of a larger set of nodes. The case-level causal effect is determined  by one or more response-type variables pointing into $Y$, as in the example above. Causal attribution, however, is defined both by the case's response type *and* by the value of $X$. To attribute $Y$'s value in a case to $X$, we need to know not only whether this is the kind of case in which $X$ could have an effect on $Y$ but also whether $X$ *in fact* took on a value that would have produced the realized value of $Y$, given the case's response type. Thus, in Figure \\ref{fig:DAGtypes}, a query about causal attribution would be defined in terms of two exogenous nodes, $X$ and $Q$.\n\n**Average Causal Effects.** A more general query would be to ask about an average causal effect in some population. This too can be conceptualized as learning about values of an exogenous variable in the model. We can do this by conceiving of each case as having been drawn from a population composed of different response-types. What we then need to learn about---what will define the average causal effect---is the *shares* of these response types in the population. \n\nMore formally and using the same notation as above and in @humphreys2015mixing, we assume that each case has been randomly selected from a population in which share $\\lambda_j$ are of type $j$. Thus, given our four response types above, $\\lambda_a$ is the share of cases in the population with negative effects; $\\lambda_b$ is the share of cases with positive effects; and so on. The population can thus be characterized by a multinomial distribution with probabilities $\\lambda = (\\lambda_a, \\lambda_b, \\lambda_c, \\lambda_d)$. Moreover, in a binary setup in which all effects are of 1-unit in size, the average causal effect can be simply characterized as the share of positive-effect cases less the share of negative-effect cases: $\\lambda_b - \\lambda_a$. \n\nGraphically, we can represent this setup by including $\\lambda$ in a more complex causal graph as in Figure \\ref{fig:DAGace}. As in our setup for case-level causal effects, $X$'s effect on $Y$ in a case depends on (and only on) the case's response type, $Q$. The key difference is that we now model the case's response type not as exogenously given, but as a function of two additional variables: the distribution of response types in a population and a random process through which the case's type is \"drawn\" from that distribution. We represent the type distribution as $\\lambda$ (a vector of values for the proportions $\\lambda_a, \\lambda_b, \\lambda_c, \\lambda_d$) and $U_Q$, respectively. We might stipulate, for instance, that $U_Q$ has a uniform distribution, between 0 and 1, and write down the structural equation for $Q$ as: \n\n$Q=$\n  \n  $q_{10}$ if $U_Q < \\lambda_a$\n  \n  $q_{01}$ if $\\lambda_a < U_Q < \\lambda_a + \\lambda_b$\n  \n  $q_{00}$ if $\\lambda_a + \\lambda_b < U_Q < \\lambda_a + \\lambda_b + \\lambda_c$\n  \n  $q_{11}$ if $\\lambda_a + \\lambda_b + \\lambda_c < U_Q < \\lambda_a + \\lambda_b + \\lambda_c + \\lambda_d$\n\n\n\\begin{equation} \n(\\#eq:Q)\n\\end{equation} \n\nIn this model, our causal query---$X$'s average causal effect---is thus defined by $\\lambda$, and specifically by its elements $\\lambda_a$ and $\\lambda_b$ (the shares of negative- and positive-causal-effect cases, respectively, in the population). Thus, again, we can represent our causal query as an exogenous variable in our causal model. Of course, like $Q$, $\\lambda$ is not directly observable. Thus, inference about average causal effects will necessarily involve using information about *observable* nodes to learn both about unobservables of interest. We might, for instance, use observations of $X$ and $Y$ to learn about a case's response type ($Q$) and, possibly repeating across many cases, about the share of different types in the population ($\\lambda$).^[Note also that $\\lambda$ can be thought of as itself drawn from a distribution, such as a Dirichlet. The hyperparameters of this underlying distribution of $\\lambda$ would then represent our uncertainty over $\\lambda$ and hence over average causal effects in the population.]\n\n<!-- **I have decided not to incorporate $U_\\lambda$ in the graph because it would actually require a node for the distribution's hyperparameters as well and I think would in any case cloud the point we want to make here.** -->\n\n<!-- Formally, this kind of average causal effect is also calculated using Equation \\ref{ate}, though for a model that is not conditional on the case at hand. -->\n\n\n\n```{r, echo = FALSE, fig.width = 5, fig.height = 3,  fig.align=\"center\", out.width='.5\\\\textwidth', fig.cap = \"\\\\label{fig:DAGace} This DAG is a graphical representation of a causal setup in which case's are drawn from a population composed of different response types. As before, $X$'s effect on $Y$ is a function of a response-type variable, $Q$. Yet here we explicitly model the case-level response type as a draw from a distribution of response types in a population. The variable $\\\\lambda$ is a vector representing the multinomial distribution of response types in the population while $U_Q$ is a random variable representing the draw of each case from the distribution defined by $\\\\lambda$. A case's response type, $Q$, is thus a joint function of $\\\\lambda$ and $U_Q$.\"}\npar(mar=c(1,1,1,1))\nhj_dag(x = c(0, 2, 2, 1, 3),\n       y = c(1, 1, 2, 3, 3),\n       names = c(\"X\", \"Y\", \"Q\", expression(lambda), expression(paste(U[Q]))),\n       arcs = cbind( c(1, 3, 4, 5),\n                     c(2, 2, 3, 3)),\n       title = \"A DAG with Response Type Drawn from a Population-level Distribution of Response Types\",\n       padding = .4, contraction = .15) \n\n```\n\n\n<!-- **ARE WE IN FACT GOING TO CARRY A DISCUSSION OF ACTUAL AND NOTABLE CAUSES THROUGH THE BOOK? IF NOT, WE SHOULD CUT THE NEXT TWO SUBSECTIONS. IT WILL TAKE PEOPLE A LONG TIME TO GET THE ACTUAL CAUSE IDEA, SO ONLY WORTH THE EFFORT IF THERE'S A PAYOFF.** -->\n\n**Actual Cause.** So far we have been dealing with causes in the standard counterfactual sense: antecedent conditions that are causes in the sense that a change in the condition would have produced a different outcome. Sometimes, however, we are interested in identifying antecedent conditions were not counterfactual difference-makers but that nonetheless generated or produced the outcome. Consider, for instance, a situation in which an outcome was overdetermined: multiple conditions were present, each of which on their own, could have generated the outcome. Then none of these conditions *caused* the outcome; yet one or more of them may have been distinctively important in *producing* the outcome. The concept of an *actual cause* may be useful in defining our causal question. \n\nLet us first approach the concept at an intuitive level. An antecedent condition, $A$, that played a role in generating an outcome might not be a counterfactual cause because, had it not occurred, some second chain of events set in motion by $B$ would have unfolded, generating the outcome anyway. In the standard counterfactual scenario, $A$ is not a counterfactual cause: take away $A$ and the outcome still happens because of the chain of events emanating from $B$. Yet let us imagine that the fact that $A$ did occur prevented part of $B$'s chain of events from unfolding and itself producing the outcome. Then let us imagine a tweaked counterfactual comparison in which we *fix* the observed fact that $B$'s causal sequence did not fully unfold. We can then ask: *conditional on $B$'s sequence not fully unfolding*, would $A$ have been a counterfactual cause of the outcome? If so, then $A$ is an actual cause of the outcome. We have, in a sense, identified $A$ as distinctively important in the production of the outcome, even if it was not a case-level cause in the usual sense.\n\nMore formally, and using the definition provided by [@halpern2015modification], building on [@halpern2005causesa] and others, we say that a condition ($X$ taking on some value $x$) was an *actual cause* of an outcome (of $Y$ taking on some value $y$), where $x$ and $y$ may be collections of events, if:\n\n1. $X=x$ and $Y=y$ both happened\n2. there is some set of variables, $\\mathcal W$, such that if they were fixed at the levels that they actually took in the case, and if $X$ were to be changed, then $Y$ would change (where $\\mathcal W$ can also be an empty set)\n3. no strict subset of $X$ satisfies 1 and 2\n\nThe definition refers to a condition that *would* have been a counterfactual cause of the outcome if we were to imagine holding constant some set of events that in fact occurred. \n\nA motivating example used in much of the literature on actual causes [e.g.  @hall2004two] imagines  two characters, Sally and Billy, simultaneously throwing stones at a bottle. Both are great shots and hit whatever they aim at. Sally's stone hits first, and so the bottle breaks. Billy's stone *would* have hit had Sally's not hit, and would have broken the bottle. Did Sally's throw cause the bottle to break? Did Billy's?\n\nBy the usual definition of causal effects, neither Sally's nor Billy's action had a causal effect: without either throw, the bottle would still have broken. We commonly encounter similar situations in the social world. We observe, for instance, the onset of an economic crisis and the breakout of war---either of which would be sufficient to cause the government's downfall---but with the economic crisis occurring first and toppling the government before the war could do so. Yet neither economic crisis nor war made a difference to the outcome.\n\nTo return to the bottle example, while neither Sally's nor Billy's throw is a counterfactual cause, there is an important sense in which Sally's action obviously broke the bottle, and Billy's did not. This intuition is confirmed by applying the definition above. Consider first the question: Did Sally's throw break the bottle? Conditions 1 and 3 are easily satisfied, since Sally \\emph{did} throw and the bottle \\emph{did} break (Condition 1), and \"Sally threw\" has no strict subsets (Condition 3). \n\nCondition 2 is met if Sally's throw made a difference, counterfactually speaking; and in determining this, we are permitted to condition on (to fix in the counterfactual comparison) any event or set of events that actually happened (or on on none at all). To see why Condition 2 is satisfied, we have to think of there being three steps in the process: Sally and Billy throw, Sally's or Billy's rock hits the bottle, and the bottle breaks. In actuality, Billy's stone did not hit the bottle. And conditioning on this actually occurring event (Billy's stone not hitting), the bottle would *not* have broken had Sally not thrown. From the perspective of counterfactual causation, it may seem odd to condition on Billy's stone not hitting the bottle when thinking about Sally not throwing the stone since Sally's throwing the stone was the very thing that prevented Billy from hitting the bottle. Yet Halpern argues that this is an acceptable thought experiment for establishing the importance of Sally's throw since conditioning is constrained to the actual facts of the case. Moreover, the same logic shows why Billy is not an actual cause. The reason is that Billy's throw is only a cause in those conditions in which Sally did not hit the bottle. But because Sally \\emph{did} actually hit the bottle, we are not permitted to condition on Sally not hitting the bottle in determining actual causation. We thus cannot---even through conditioning on actually occurring events---construct any counterfactual comparison in which Billy's throw is a counterfactual cause of the bottle's breaking.\n\nThe striking result here is that there can be grounds to claim that a condition was the actual cause of an outcome even though, under the counterfactual definition, the effect of that condition on the outcome is 0. (At the same time, all counterfactual causes are automatically actual causes; they meet Condition 2 by conditioning on nothing at all, an empty set $\\mathcal W$.) One immediate methodological implication follows: since actual causes need not be causes, there are risks in research designs that seek to understand causal effects by tracing back actual causes---i.e., the way things actually happened. If we traced back from the breaking of the bottle, we might be tempted to identify Sally's throw as the cause of the outcome. We would be right only in an actual-causal sense, but wrong in the standard, counterfactual causal sense. Chains of events that appear to \"generate\" an outcome are not always causes. ^[Perhaps more surprising, it is possible that the expected causal effect is negative but that $X$ is an actual cause in expectation. For instance, say that 10% of the time Sally's shot intercepted Billy's shot but without hitting the bottle. In that case the average causal effect of Sally's throw on bottle breaking is $-0.1$ yet 90% of the time Sally's throw is an actual cause of bottle breaking (and 10% of the time it is an actual cause of non-breaking). For related discussions see @menzies1989probabilistic.]\n\nAs with other causal queries, the question \"Was $X=x$ the actual cause of $Y=y$?\" can be redefined as a question about which values for exogenous nodes produce conditions under which $X$ could have made a difference. Similarly, the question of how *common* it is for a condition to be an actual cause can be expressed as values of nodes, possibly including nodes that record parameter values for the relevant root nodes. \n\n<!-- We should try to be more specific here and for notable causes about what the nodes we'd want to learn about are. -->\n\n**Notable cause** An extended notion [@halpern2016actual, p 81] of actual causes restricts the imagined counterfactual deviations to states that are more likely to arise (more \"normal\") than the factual state. We will call this notion a ``notable cause.'' Similarly, one cause, $A$, is more notable than another cause, $B$, if a deviation in $A$ from its realized state is more likely than a deviation in $B$ from its realized state. \n\nFor intuition, we might wonder why a Republican was elected to the presidency in a given election. In looking at some minimal winning coalition of states that voted Republican, we might distinguish between a set of states that *always* vote Republican and a set of states that usually go Democratic but voted Republican this time. If the coalition is minimal winning, then every state that voted Republican is a cause of the outcome in the standard sense. However, only the states that usually vote Democratic are notable causes since it is only for them that the counterfactual scenario (voting Democratic) was more likely to arise than the factual scenario. In a sense, we take the \"red\" states' votes for the Republican as given---placing it, as it were, in the causal background---and identify as \"notable\" those conditions that mattered and easily could have gone differently. By the same token, we can say that, among those states that voted Republican this time, those that more commonly vote Democratic are *more* notable causes than those that less commonly vote Democratic.\n\nAgain, whether something is a notable cause, or the likelihood in some population that a condition is a notable cause, can be expressed as a claim about the value of a set of root nodes.\n\n<!-- So there are 2 things being defined: notable vs. not notable, and more vs. less notable. -->\n\n<!-- The election example seems to be illustrating the first of these since it refers to the volatile states being notable.  -->\n\n<!-- But the reasoning doesn't line up with the definition of notable given here: we haven't said that these are states that usually vote non-Republican. We've only said that their voting non-R is more likely than the other states' voting non-R. -->\n\n<!-- Shall I fix by simply changing the volatile states to ones that usually vote D? -->\n\n<!-- Also, we are not saying what the nodes in Q are, just that there are some. Could we say that they're the same nodes as for a plain actual cause PLUS nodes going into X (a possible notable cause) representing parameters of the distribution of X? -->\n\n**Causal Paths.** To develop richer causal understandings, researchers often seek to describe the causal path or paths through which effects propagate. Consider the DAG in Figure \\ref{fig:DAGpaths}, and assume that all variables are binary, taking on values of $0$ or $1$. We have explicitly drawn in response-type variables, $Q^M$ and $Q^Y$, as parents of $M$ and $Y$, respectively, as they will be critical to thinking about causal paths.\n\nSuppose that we observe $X=1$ and $Y=1$ in a case. Suppose, further, that we have reasonable confidence that $X$ has had a positive effect on $Y$ in this case. However, we may be interested in knowing whether that effect ran *through* $M$. Importantly, a question about what the causal path was goes beyond assessing whether some mediating event along a causal path occurred. We cannot, for instance, establish that the top path was operative simply by determining the value of $M$ in this case---though that might be useful information. \n\nRather, the question of whether the top causal path is operative is the composite question of, first, whether $X$ had an effect on $M$ in this case and, second, whether that effect (the difference in $M$'s value caused by a change in $X$) in turn *caused* $Y$ to take on the value that it did. In other words, what we want to know is whether the effect of $X$ on $Y$ depended on---*would not have happened without*---the effect of $X$ on $M$.^[A very similar question is taken up in work on  mediation where the focus goes to understanding quantities such as the ``indirect effect'' of $X$ on $Y$ via $M$. Formally, the indirect effect would be $Y(X=1, M = M(X=1, Q^M), Q^Y) - Y(X = 1, M = M(X=0, Q^M), Q^Y))$, which captures the difference to $Y$ if $M$ were to change in the way that it would change due to a change in $X$, but without an actual change in $X$ [@pearl2009causality p 132, @imai2010general].] Framing the query in this way makes clear that asking whether a causal effect operated via a given path is in fact asking about a specific set of causal effects lying along that path.\n\n\n```{r, echo = FALSE, fig.width = 5, fig.height = 3,  fig.align=\"center\", out.width='.5\\\\textwidth', fig.cap = \"\\\\label{fig:DAGpaths} Here $X$ has effects on $Y$ both indirectly through $M$ and directly.\"}\npar(mar=c(1,1,1,1))\nhj_dag(x = c(0, 1, 2, 1, 2),\n       y = c(2, 3, 2, 4, 3),\n       names = c(\"X\", \"M\", \"Y\", expression(paste(Q^M)), expression(paste(Q^Y))),\n       arcs = cbind( c(1, 2, 1, 4, 5),\n                     c(2, 3, 3, 2, 3)),\n       title = \"A DAG with Two Causal Paths\",\n       padding = .4, contraction = .15) \n\n```\n\n\nAs we can show, we can also define a path query as a question about specific, exogenous nodes on a causal graph. In particular, just as we have defined other questions about causal effects in terms of response-type nodes, a causal path can also be defined in terms of the values of type nodes: specifically, in the present example, in terms of the nodes $Q^M$ and $Q^Y$. To see why, let us first note that there are two combinations of effects that would allow $X$'s positive effect on $Y$ to operate via $M$: (1) $X$ has a positive effect on $M$, which in turn has a positive effect on $Y$; or (2) $X$ has a negative effect on $M$, which has a negative effect on $Y$. \n\nThus, in establishing whether $X$ affected $Y$ through $M$, the first question is whether $X$ affected $M$ in this case. Whether or not it did is a question about the value of the response-type node, $Q^M$. Let us assume that $Q^M$ can take on four possible values corresponding to the four possible responses to $X$: $q^M_{10}, q^M_{01}, q^M_{00}, q^M_{11}$.^[In other words, $X$'s effect on $M$ could be adverse, beneficial, chronic, or destined, respectively.] For sequence (1) to operate, $Q^M$ must take on the value $q^M_{01}$, representing a positive effect of $X$ on $M$. For sequence (2) to operate, $Q^M$ must take on the value $q^M_{10}$, representing a negative effect of $X$ on $M$.\n\n$Q^Y$ is somewhat more complicated since it will define $Y$'s response to different combinations of two other variables, $X$ and $M$. Thus, $Q^Y$ must cover the full set of possible causal interactions between two binary causal variables. We provide notation for type variable values with two causes later in the book. However, we display the full 16 types---each a possible value of $Q^Y$---in Table \\@ref(tab:typespaths). As we see, $Y$'s response to $M$ can be characterized by any of our four response types when $X=0$, and by any of our four response types when $X=1$,^[This is precisely equivalent to noting that $X$'s effect on $Y$ can be of any of the four types when $M=0$ and of any of the four types when $M=1$.] generating 16 possible response patterns to combinations of $X$ and $M$ values. Thus, for instance, type 5 describes a response pattern in which $M$ has a positive effect on $Y$ when $X=0$ and no effect, with $Y$ stuck at $0$, when $X=1$; and in which $X$ has no effect when $M=0$ and a negative effect when $M=1$. In type 11, $M$ has a negative effect on $Y$ regardless of $X$'s value; and $X$ has no effect regardless of $M$'s value.\n\nWhat values of $Q^Y$, of those displayed in Table \\@ref(tab:typespaths), then are compatible with the operation of the $M$ causal path? Let us first consider this question with respect to sequence (1), in which $X$ has a positive effect on $M$, and that positive effect is necessary for $X$'s positive effect on $Y$ to occur. For this sequence to operate, $Q^M$ must take on the value of $q^M_{01}$. When it comes to $Q^Y$, then, what we need to look for are types in which $X$'s effect on $Y$ depends on $M$'s taking on the value it does as a result of $X$'s positive effect on $M$. \n\nWe are thus looking for response types that represent two kinds of counterfactual causal relations. First, $X$ must have a positive effect on $Y$ when $M$ changes as it should given $X$'s positive effect on $M$ ($Q^M=q^M_{01}$. Second, that change in $M$, generated by a change in $X$, must be *necessary* for $X$'s positive effect on $Y$ to operate. The thought experiment here thus imagines a situation in which $X$ changes from $0$ to its realized value, $1$,^[This is the natural thought experiment when explaining a case with realized value of $X=1$, in which the outcome can be thought of as having been generated by a change from $X=0$. The identification of types does hinge, however, on the direction in which we imagine types changing. In other situations, we might observe $X=Y=0$ and thus conceive of the outcome as having been generated by a change from $X=1$ to $X=0$ (again, assuming a positive effect of $X$ on $Y$). When we do this, query 2 below changes: we are now looking for types in which $Y=1$ when $X=0$ but $M=1$. (Does $Y$ stay at $1$ when $X$ moves to $0$ but $M$ doesn't?) The queries are then satisfied by types $6$ and $8$, rather than $2$ and $6$.] but $M$ does *not* change to the value that it should as a result of this change in $X$. We then inspect our types to see if $Y$ would change to $1$ in this situation. It is this counterfactual that isolates the causal significance of the path that runs through $M$. It is only if $Y$ would *not* change to $1$ in this situation that that we have a response-type for which this mediating path matters. \n\nAssuming a positive effect of $X$ on $M$ ($Q^M=q^M_{01}$), we thus need to apply three queries to the types in Table \\@ref(tab:typespaths):^[Using standard potential outcomes notation, we can express the overall query, conditioning on a positive effect of $X$ on $M$, via the inequality $Y(X=1, M(X=1)) - Y(X=0, M(X=0)) \\neq Y(X=1, M(X=0)) - Y(X=0, M(X=0))$. The three specific queries formulated below simply correspond to the three unique elements of this expression. (1a) do(X=0, M=M(X=0))...Y=0?; (1b)  do(X=1, M=M(X=1))...Y=1; (2) do(X=1, M=M(X=0))...Y=0? We can also readily map the path query that we are pursuing here---does the positive effect of $X$ on $Y$ depend on $X$'s effect on $M$---onto a query posed in terms of indirect effects. For instance, in our binary setup, conditioning our path query on a positive causal effect of $X$ on $Y$, a positive effect of $X$ on $M$, and an imagined change from $X=0$ to $X=1$ generates precisely the same result (identifies the same $Q^Y$ types) as asking which $Q^Y$ types are consistent with a positive indirect effect of $X$ on $Y$, conditioning on a positive total effect and $X=1$.]\n\n1. Is $X=1$ a counterfactual cause of $Y=1$? Establishing this positive effect of $X$ involves two queries:\n\n    a) Where $X=0$, does $Y=0$? As we are assuming $X$ has a positive effect on $M$, if $X=0$ then $M=0$ as well. We thus look down the $X=0, M=0$ column and eliminate those types in which we do not observe $Y=0$. This eliminates types $9$ through $16$.\n\n    b) Where $X=1$, does $Y=1$? Again, given $X$'s assumed positive effect on $M$, $M=1$ under this condition. Looking down the $X=1, M=1$ column, we eliminate those types where we do not see $Y=1$. We retain only types $2, 4, 6,$ and $8$.\n\n2. Is $X$'s effect on $M$ necessary for $X$'s positive effect on $Y$? That is, do we see $Y=1$ *only* if $M$ takes on the value generated by $X=1$ ($M=1$)? To determine this, we inspect the \"counterfactual\" condition in which $X=1$ yet $M=0$, and we ask: does $Y=0$? Of the four remaining types, only $2$ and $6$ pass this test. \n\n\n<!-- \\begin{table}[h!]\n  \\centering\n    \\begin{tabular}{cccccc}\n    \\hline\n    \\textbf{Type} &  $(Y | X=0,$ & $(Y |X=0, $ & $(Y | X=1, $ & $(Y | X=1, $ \\\\\n         & $M=0)$ & $M=1)$ & $M=0)$ & $M=1)$ \\\\  \\hline\n        1 \t\t\t&  0     & 0     & 0     & 0 \\\\\n    2 \t& 0     & 0     & 0     & 1 \\\\\n    3 \t& 0     & 0     & 1     & 0 \\\\\n    4 \t\t\t& 0     & 0     & 1     & 1 \\\\\n    5 \t& 0     & 1     & 0     & 0 \\\\\n    6 \t\t\t& 0     & 1     & 0     & 1 \\\\\n    7 \t& 0     & 1     & 1     & 0 \\\\\n    8 \t\t& 0     & 1     & 1     & 1 \\\\\n    9\t\t\t& 1     & 0     & 0     & 0 \\\\\n    10 \t& 1     & 0     & 0     & 1 \\\\\n    11\t\t\t& 1     & 0     & 1     & 0 \\\\\n    12\t\t& 1     & 0     & 1     & 1 \\\\\n    13\t\t\t& 1     & 1     & 0     & 0 \\\\\n    14\t\t& 1     & 1     & 0     & 1 \\\\\n    15\t\t& 1     & 1     & 1     & 0 \\\\\n    16\t\t\t& 1     & 1     & 1     & 1 \\\\\n    \\bottomrule\n    \\end{tabular}%\n   \\caption{$Y$'s 16 response types---values of $Q^Y$---given binary $X$ and $M$ as parents of $Y$}\n  \\label{typespaths}%\n\\end{table}% -->\n\n-------------------------------------------------------------------\n   **Type**    $(Y | X=0,$  $(Y |X=0,$   $(Y | X=1,$   $(Y | X=1,$  \n                  $M=0)$       $M=1)$       $M=0)$        $M=1)$    \n-------------  -----------  -----------  ------------  ------------\n      1             0            0             0             0      \n\n      2             0            0             0             1      \n\n      3             0            0             1             0      \n\n      4             0            0             1             1      \n\n      5             0            1             0             0      \n\n      6             0            1             0             1      \n\n      7             0            1             1             0      \n\n      8             0            1             1             1      \n\n      9             1            0             0             0      \n\n      10            1            0             0             1      \n\n      11            1            0             1             0      \n\n      12            1            0             1             1      \n\n      13            1            1             0             0      \n\n      14            1            1             0             1      \n\n      15            1            1             1             0      \n\n      16            1            1             1             1      \n-------------------------------------------------------------------\n\nTable: (\\#tab:typespaths)$Y$'s 16 response types---values of $Q^Y$---given binary $X$ and $M$ as parents of $Y$\n\nUnder these two values of $Q^Y$---response-types $2$ and $6$---we will see a positive effect of $X$ on $Y$ for which the $M$ path is causally necessary as long as $X$ also has a positive effect on $M$. These two response types are also different from one another in an interesting way. In type $6$, $X$'s effect on $Y$ runs strictly through $M$: if $M$ were to change from $0$ to $1$ *without* $X$ changing, $Y$ would still change from $0$ to $1$. $X$ is not per se causally important for $Y$, except insofar as it affects $M$. In a case of type $6$, then, anything else that similarly affects $M$ would generate the same effect on $Y$ as $X$ does. In type $2$, however, both $X$'s change to $1$ *and* the resulting change in $M$ are necessary to generate $Y$'s change to $1$; $X$'s causal effect thus operates both through the mediated and the unmediated pathway. Here $X$ itself matters in the counterfactual sense; for a case of type $2$, some other cause of $M$ would *not* generate the same effect on $Y$. \n\n\n\n\n<!-- The structural equation for $M$ will include $X$ and $Q_M$ as arguments. Thus, knowing the value of $M$ for any given value of $X$, conditional on a given structural equation for $M$, requires knowing $U_M$. The same logic operates for $U_Y$'s role in determining how $Y$ responds to a given change in $M$, conditional on $Y$'s structural equation.  -->\n\n<!-- Note that, as we saw with causal effects, it is also possible to imagine related estimands of the form \"does $X$ cause $Y$ in this case through $M$?\", \"did $X$ cause $Y$ in this case through $M$?\" (which requires knowledge of $X$), and \"how often does $X$ cause $Y$ though $M$ in a larger population?\" (which requires knowledge of the parameters that give rise to $U_Y$ and $U_M$).   -->\n\nWe can undertake the same exercise for sequence (2), in which $X$ first has a negative effect on $M$, or $Q^M=q^M_{10}$. Here we adjust the three queries for $Q^Y$ to take account of this negative effect. Thus, we adjust query 1a so that we are looking for $Y=0$ when $X=0$ and $M=1$. In query 2b, we look for $Y=1$ when $X=1$ and $M=0$. And for query 2, we want types in which $Y$ fails to shift to $1$ when $X$ shifts to $1$ but $M$ stays at $1$. Types $3$ and $11$ pass these three tests. \n\nIn sum, there are two combinations of values of $Q^M$ and $Q^Y$ that are consistent with the $X$'s positive effect on $Y$ in an $X=1, Y=1$ case having depended on the path running through $M$:\n\n* $Q^M=q^M_{01}$ and $Q^Y$ is of type $2$ or type $6$, or\n* $Q^M=q^M_{01}$ and $Q^Y$ is of type $3$ or type $11$\n\nAsking whether $X$'s positive effect on $Y$ ran through $M$ is thus to ask a question about the values of these exogenous nodes in the causal model. In some situations, $Q^M$ and $Q^Y$ may be substantively conceptualized and directly observable; in other situations, we will need to draw inferences about them from observable variables in the model. It is worth noting how different this formulation of the task is from widespread understandings of process tracing. Scholars commonly characterize process tracing as a method in which we determine whether a mechanism was operating by establishing whether the events lying along that path occurred. As a causal-model framework makes clear, finding out that $M=1$ (or $M=0$, for that matter) does not itself establish what was going on causally. Observing this intervening step does not by itself tell us what value $M$ *would* have taken on if $X$ had taken on a different value, or whether this would have changed $Y$'s value. We need instead to conceive of the problem of identifying pathways as one of figuring out the counterfactual response patterns of the variables along the causal chain. As we will explore later in the book, explicitly characterizing those response patterns as nodes in a causal model helps us think systematically about empirical strategies for drawing the relevant inferences.\n\n<!-- Such a  focus on causal paths does not restrict attention to questions of the form \"how did $X$ cause $Y$\" but more generally, \"what paths generated $Y$?\" Such questions may have answers of the form \"$Y=1$ occurred because $X=0$ led to $M=0$, which, when $Z=1$, gives rise to $Y=1$ and not because $X=1$  led to $M=1$, which, when $Z=0$ gives rise to $Y=1$.\" Such inquiries can focus on distinct sets of conditions that give rise to an outcome (\"equifinality\"), as in Qualitative Comparative Analysis (QCA). While QCA analysts sometimes refer to sets of conditions as \"paths\",  QCA does not generally involve explicit assessment of the causal steps linking conditions to outcomes. When examining paths in a causal-model framework, the analyst can address queries that involve drawing inferences about an entire chain linking $X$ to $Y$ or even an entire causal network. An understanding of a full causal network would, in turn, allow for any more specific estimand to be estimated. -->\n\n\n\n\n## A Running Example\n\nWe can more fully illustrate the definition of causal queries in terms of exogenous nodes on a graph by thinking through a simple causal model of a political process, one that we will return to in later parts of the book. We begin with two binary features of context. Consider, first, that a country may or may not have a free press ($X$). Second, the country's government may or may not be sensitive to public opinion ($S$).^[Government sensitivity here can be thought of as government sophistication (does it take the actions of others into account when making decisions?) or as a matter of preferences (does it have a dominant strategy to engage in corruption?).] Let us then stipulate what follows from these conditions. The government will engage in corruption ($C=1$) unless it is sensitive to public opinion and there is a free press. Moreover, if and only if there is both government corruption and a free press, the press will report on the corruption ($R=1$). Finally, the government will be removed from office ($Y=1) if it has acted corruptly and this gets reported in the press; otherwise, the government remains in office. \n\nAs a set of equations, this simple causal model may be written as follows:\n\n$\\begin{array}{ll}\nC = 1-X\\times S &  \\mbox{Whether the government is corrupt}\\\\\nR = C\\times X &  \\mbox{Whether the press reports on corruption}\\\\\nY = C\\times R & \\mbox{Whether the government is removed from office}\n\\end{array}$\n\nOne thing that these equations make clear is that the variables in our model function in various places as response-type nodes for one another. For instance, we can see from equation for $C$ that the causal effect of a free press ($X$) on corruption ($C$) depends on whether the government is sensitive to public opinion ($S$): $S$ determines $C$'s response to $X$ (as does $X$ for $S$'s effect on $C$). A similar relationship holds for $C$ and $X$ in their effect on $R$ and for $C$ and $R$ in their effect on $Y$. As we will see below, the model also implies more complex response-type relationships. We can, further, substitute through the causal processes to write down the functional equation for the outcome in terms of the two initial causal variables: $Y=(1-S)X$.^[Equivalently, in Boolean terms, where $Y$ stands for the occurrence of government removal, $Y= \\neg S \\land X$; and the function for the outcome \"government retained\" can be written  $\\neg Y = (S\\land X) \\lor (S\\land\\neg X) \\lor (\\neg S \\land \\neg X)$ or, equivalently, $\\neg Y = S + \\neg S \\neg X$.] \n\nLet us, further, allow our two primary causal variables---the existence of a free press and the existence of a sensitive government---to vary probabilistically. In particular, we represent the probability of a free press with the population parameter $\\pi^X$ and the probability of a sensitive government with the parameter $\\pi^S$. To generate draws with these probabilities, we then introduce two random variables with uniform distributions between 0 and 1, $U_X$ and $U_S$, and posit that we get a free press whenever $u_X < \\pi^X$ and a sensitive government whenever $u_S < \\pi^S$. \n\nThis gives the following equations for $X$ and $S$:\n\n$\\begin{array}{ll}\nX = \\mathbb{1}(u_X < \\pi^X) & \\mbox{Whether the press is free}\\\\\nS = \\mathbb{1}(u_S < \\pi^S) & \\mbox{Whether the government is sensitive}\\\\\n\\end{array}$\n\n\n<!-- where $\\pi^S$ and $\\pi^X$ are parameters governing the probability of $S$ and $X$, respectively, taking on the value of 1. -->\n\n<!-- To generate a probabilistic causal model, we also need distributions on $\\mathcal U = (U_S, U_X)$. These are given by:  -->\n\n<!-- $\\begin{array}{ll} -->\n<!-- u_S \\sim \\text{Unif}[0,1] &  \\mbox{Stochastic component of government type} \\\\ -->\n<!-- u_X \\sim \\text{Unif}[0,1] &  \\mbox{Stochastic component of press freedom} -->\n<!-- \\end{array}$ -->\n\nNote that in this model, only the most \"senior\" specified variables, $X$ and $S$, have a stochastic component (i.e., include a $U$ term in their function). All other endogenous variables are deterministic functions of other specified variables.\n\n<!-- Does that work? The term \"specified\" is quite useful here -- since otherwise really every variable except the U's is a deterministic function. Do we want to just come out and formally distinguish, earlier, between specified and unspecified variables? -->\n\n\n\n```{r, echo = FALSE}\nnames = c(\"S\", \"X\", \"C\", \"R\", \"Y\")\nM <- matrix(0, 5, 5)\nM[1, c(3)] <-1\nM[2, c(3,4)] <-1\nM[3, c(4,5)] <-1\nM[4, 5] <-1\nf_C <- function(V) 1- V[1]*V[2]\nf_R <- function(V) V[2]*V[3]\nf_Y <- function(V) V[3]*V[4]\n# Histories and effects:\n\nH <- function(do = c(0,0,NA,NA,NA)) {\n  do[3] <- ifelse(is.na(do[3]), f_C(do), do[3])\n  do[4] <- ifelse(is.na(do[4]), f_R(do), do[4])\n  do[5] <- ifelse(is.na(do[5]), f_Y(do), do[5])\n  do\n  }\n\nedges <- function(S,X) {\n  do0 <- c(S,X,NA, NA, NA)\n  H1   <- H(do0)\n  out <- sapply(1:5, function(i) {\n    do1 <- do0\n    do1[i] <- 1-H1[i]  # change value for i and put into do\n    1*(H1 != H(do1))\n  })\n  diag(out) <- 0\n  out}\n\n```\n\n\n<!-- **LET'S PUT THE PI'S ON THE BIG GRAPH (AS WELL AS THE U'S) SINCE THE PI'S ARE THE NODES THAT DEFINE THE AVERAGE CAUSAL EFFECT QUERY** -->\n\n\n```{r, echo = FALSE, fig.width = 11, fig.height = 11.5, fig.align=\"center\", out.width='\\\\textwidth', fig.cap = \"\\\\label{fig:running} The main panel shows a simple causal model. $S$ and $X$ are stochastic, other variables determined by their parents, as shown in bottom right panel. Other panels show four possible histories that can arise depending on values taken by $S$ and $X$, along with causal relations in each case. The equations for $S$ and $X$ are written with indicator variables, which take a value of 1 whenever the $u$ value is less than the $\\\\pi$ value.\", fig.align=\"center\", warning = FALSE}\n\nlayout(matrix(c(1,1,2,\n                1,1,3,\n                4, 5,6), 3, 3, byrow = TRUE))\npar(mar=c(1,1,3.5,1))\n\nx = c(0,0, 1, 1, 2)\ny = c(2,0, 2, 0, 1)\n\nnames = c(\"S:\\nSensitive\\ngovernment\\n\\n\", \"\\nX:\\nFree Press\", \"C:\\n Corruption\", \"R:\\n Media report\", \"Y:\\nGovernment\\nreplaced\")\n\nhj_dag(x =  c(x, 0, 0),\n       y = c(y, 0.25, 1.75),\n       names = c(names, \" \", \" \"),\n       arcs = cbind( c(1,2,2, 3, 4, 3, 6, 7),\n                     c(3,3,4, 5, 5, 4, 2, 1)),\n       title = \"Free Press and Government Survival\",\n       add_functions = 0, \n       contraction = .15,\n       add_functions_text = \"Structural Equations: Y = CR, R = CX, C = 1-XS\",\n       padding = .2)\n\ntext(c(0,0), c(.25, 1.75), c(expression(paste(U[X])), expression(paste(U[S]))))\n\nnames = c(\"S\", \"X\", \"C\", \"R\", \"Y\")\n\nmyarcs <- list(\n       which(t(edges(0,0))==1, arr.ind = TRUE),\n       which(t(edges(1,0))==1, arr.ind = TRUE),\n       which(t(edges(0,1))==1, arr.ind = TRUE),\n       which(t(edges(1,1))==1, arr.ind = TRUE))\n\nmysolids <- list(H(c(0,0,NA,NA,NA)), \n                 H(c(1,0,NA,NA,NA)), \n                 H(c(0,1,NA,NA,NA)), \n                 H(c(1,1,NA,NA,NA)))\ntitles = c(\"A: No free press causes Y = 0\", \n           \"B: No free press is the actual cause\\nBut neither S nor X counterfactually cause Y=0\",\n           \"C: Both S = 0 and X = 1 cause Y = 1.\\n X = 1 is the notable cause.\", \n           \"D: Government sensitivity\\ncauses Y = 0\")\nfor(j in 1:4){\n\n    hj_dag(\n       x = x,\n       y = y,\n       names = names,\n       arcs = myarcs[[j]],\n       title = titles[[j]],\n       add_points = TRUE,\n       solids = c(mysolids[[j]]),\n       contraction = .15\n       )\n}\n\nframe(); box();\ntext(.1,seq(.95, .05, -.1), \n          c(\"Structural Equations:\",\n            \"  Y = CR\",\n            \"  C = 1 - XS\",\n            \"  R = CX\", \n            expression(paste(\"  S = 1(\", u[S]<pi^S,\")\")),\n            expression(paste(\"  X = 1(\", u[X]<pi^X,\")\")), \"  \",\n            \"P(U):\",\n            expression(paste(\"  \", U[S], \"~Unif[01]\")), \n            expression(paste(\"  \", U[X], \"~Unif[01]\")) \n            ), cex = 1.5, adj = 0)\ntitle(\"Structural model\")\n```\n\n\n<!-- In figure above, can we not put proper indicator variable notation in the structural equations for the U's? NOT EASY -->\n\nThe corresponding causal diagram for this model is shown in Figure \\ref{fig:running}. The first, largest graph explicitly includes the processes determining the two key causal variables variables (the $\\pi$ and $U$ terms). \n\nIn the other four panels, $A, B, C$, and $D$, we leave the $\\pi$ and $U$ terms implicit as they will not come into play in our analysis of these graphs. In these four panels, we show all possible ``realizations'' of the graph given the four possible contexts defined by the exogenous nodes, $S$ and $X$. We build each of the four possible by assessing outcomes and counterfactual relationships for each possible combination of $S$ and $X$ values. A hollow circle at a node indicates that the variable takes on a value of $0$ while a shaded circle indicates a value of $1$. The arrows indicate causal effects. More specifically, an arrow pointing from one variable to another indicates that a manipulation of the first variable would cause a change in the second variable, *given the values realized by all other variables that are not the first variable's descendants*. Unlike in a conventional DAG, we represent here both the direct effect of each variable on its child and each variable's indirect (mediated) causal effects on its descendants. As we can see from the various arrows in the panels, we can use a single, simple causal model to think through a wide range of causal relationships that might be of interest.^[Though similar, these graphs are not DAGS or natural beams (or submodels). The panels reflect outcomes conditional on the values of $S$ and $X$, but they are not themselves DAGs because they indicate the values taken by nodes and include arrows between two nodes when and only when one causes the other, directly or indirectly. To construct \"natural beams\" [@pearl2009causality 10.3], we fix a realization of root variables, $U$,  (here, $\\mathcal U = (S, X)$); then for each variable, $V_i$ we  partition $pa(V_i)$ into a set of \"engaged parents,\" $S$, and \"disengaged parents,\" with the property that (a) $f_i(S(u), \\overline{s}, u) = V_i(u)$ for *all* values of $\\overline{s}$ and (b) $f_i(s', \\overline{S}(u), u) \\neq V_i(u)$ for *some*  $s'$. Thus a natural beam  would connect a parent to a child if, given the particular history, the parent mattered for the child's outcome.] Since the values of all variables in a model are determined by the values of the exogenous nodes, this is equivalent to saying that the arrows show the causal effects that are operating each *context.*\n\nOne important feature of DAGs is immediately evident from a comparison of the DAG with subgraphs $A, B, C$, and $D$ in the figure. Consistent with the rules of DAG-construction, the DAG includes arrows between all variables that are under any circumstances directly causally related; but the inclusion of an arrow does not mean that two variables are *always* causally related. For instance, while the DAG (large graph) has an arrow running from $X$ to $R$, we can see from the subgraphs (where we deviate from the standard grammar of DAGs) that the causal effect is contingent on context: it is present only when $S=0$ (panels $A$ and $C$) but not when $S=1$. The arrows in a DAG represent dependencies that exist under *some*, but not necessarily under all, values of the exogenous variables.\n\nThese five graphs allow us to define all causal claims of interest. The graphs illustrate, in other words, how causal queries can be represented as the value of the exogenous nodes in a causal diagram. Let us consider each causal query in turn.\n\n**Case-level causal effect.** Working with the four subgraphs, we can show that the query, \"What is the effect of one variable on another in this case?\" is equivalent to asking about the values of the model's exogenous variables, $X$ and $S$. Consider, for instance, the query: Do media reports of corruption, $R$, have a causal effect on government removal from office, $Y$, in this case? Turning to the subgraphs, we can simply ask in which of these four graphs---in which context---$R$ has a causal effect on $Y$: where is there an arrow running from $R$ to $Y$?^[The subgraphs are derived from application of Equation \\@ref(eq:ate). We can work through the $R \\rightarrow Y$ relationship to demonstrate how this is done. Consider the effect of $R$ on $Y$ given $S=0, X=0$. This is the arrow between  $R$ and $Y$ in panel $A$. Removing the arrows pointing to $R$, the distribution over nodes when $R=r'$ is: $P(c,y | \\hat{r}=r', s =0, x=0)$. We are interested in $P(y=1| \\hat{r}=1,  s =0, x=0) - P(y=1 | \\hat{r}= 0,  s =0, x=0)$. The second term is easy as for all cases in which $r=0$, $y=0$; and so  $P(y=1|| \\hat{r}= 0)=0$. We focus then on  $P(y=1| \\hat{r}=1, s= 0, x= 0)$. Taking the marginal distribution, this can be written $\\sum_{c=0}^1P(y=1|r=1, c)P(c|s=0,x=0)$. From the structural equations, we know that $P(c=1|s=0,x=0)=1$ and that $P(y=1|r=1, c=1)=1$. So the marginal distribution is $P(y=1| \\hat{r}=1, s= 0, x= 0) = 1$; and the treatment effect of $R$ on $Y$, conditional on the characteristics of this case, is then 1. This positive effect is represented with the arrow from the $R=0$ node to the $Y=0$ node in panel $A$.] We can readily see that $R$ has an effect---a positive effect---on $Y$ in all configurations of exogenous node values (i.e., in all subgraphs) except when $X=1$ and $S=1$ (panel $D$); the absence of an arrow in panel $D$ indicates that $R$'s effect on $Y$ is 0 in that context. Thus, given our model, asking whether there is a case-level causal effect of $X$ on $Y$ is equivalent to asking whether either $S$ or $X$ or both are equal to $0$ in the case.\n\nAnother way to put the point is that $S$ and $X$ jointly determine a case's response type when it comes to the effect of $R$ on $Y$. Returning to our four response types, the graphs tell us that a case is a $b$ type (positive effect) with respect to the $R \\rightarrow Y$ relationship whenever at least one of $S$ or $X$ is $0$. If $S=X=1$, then a case is a $c$ type (no effect, with the outcome fixed at $Y=0$). \n\nWe can work through other relationships in the model similarly. For instance, does a free press have an effect on government removal in a case? See an $X$-to-$Y$ arrow only in panels $A$ and $C$, we can thus conclude that $X$ has a causal effect on $Y$ in (and only in) cases in which $S=0$. \n\nFor now, we are simply using the models to *define* a query about a case-level causal effect. This definition sets the stage for our discussion of research design---how one might go about empirically addressing this query---later in the book. We can point the way toward that discussion by noting making two broad points. If the presence of a free press and government's sensitivity to public opinion are observable, then estimating case-level causal effects will simply be a matter of measuring these two exogenous nodes (or, for some queries, just one of them). However, we will often be in a situation in which the nodes defining our causal query are not observable. Our models of the world often include concepts that are theoretically central to a causal logic but cannot be directly measured. Consider, for instance, government sensitivity to public opinion. When a model's exogenous variables are unobservable, then our research design may require using information from other, *observable* nodes to draw inferences about context. This is a key strategy of process tracing that we develop in later chapters.\n\n\n<!-- We need to resolve the contradiction between the above footnote and the previous one: one says they're not submodels, the other says they are. -->\n\n \n\n\n<!-- In this causal beam with binary variables,  whenever a unique path connects one node to another then the ancestor's node's condition is a cause of the descendant's condition. These case level causal relations cannot be read directly from the graph however if there are multiple paths or non dichotomous variables. To see why multiple paths prevent this inference, return to the boulder example of non transitivity described above; to see why inferences cannot be made along paths with non binary outcomes notice that $A$ and $B$ may be connected because some change in $A$ produces a change in $B$, though not necessarily *all* changes in $A$.    -->\n\n**Average causal effects.** Average causal effects are simply averages of case-level causal effects for the population. Since case-level causal effects are determined by the values of the exogenous nodes in cases, we need to average over the distribution of case-level contexts in the population. Put differently, the average causal effect of any variable on another will depends on how commonly the relevant case-level conditions---those in which the causal effect is and is not present---occur. In our current example, we have seen that the free press makes a difference to government survival if and only if the government is *non-sensitive* (panels $A$ and $C$): the non-sensitive government gets exposed as corrupt if and only if there is a free press while the sensitive government never gets replaced because it adjusts to the presence of a free press by eliminating corruption. Similarly, the sensitivity of the government (and the resulting level of corruption) matters only if there *is* a free press (panels $C$ and $D$). Without a free press, non-sensitive and, thus, corrupt governments do not get exposed and so stay on; with a free press, non-sensitive (and, thus, corrupt) governments get replaced. \n\nThus, the *average* effect of each of these initial causes on the outcome will depend on the probability with which the other cause is absent or present. To define a query about average causal effects, we need to examine the full probabilistic causal model as graphed in the large panel in Figure \\ref{fig:running}. What is the average causal effect of a free press ($X$) on government removal ($Y$)? As we have learned from the subgraphs, this effect is fully defined by the value of $S$. In particular, the effect of $X$ on $Y$ is equal to $1$ when $S=0$, and is equal to $0$ when $S=1$. As we've noted, we calculate the average causal effect by averaging causal effects over the distribution of the relevant exogenous variables -- which, here, is only $S$. In the probabilistic model, $S$ is a function of $\\pi^S$ and $U_S$. In particular, $S=1$ whenever $u_S < \\pi^S$. Since $U_S$ has a uniform distribution, this simply means that $S=1$ with probability $\\pi^S$; likewise, $S=0$ with probability $1-\\pi^S$.  Thus, we calculate $X$'s average causal effect on $Y$ by multiplying each causal effect by the probability of $S$'s taking on the value that generates that effect: $1 \\times (1-\\pi^S) + 0 \\times \\pi^S = 1-\\pi^S$. Put differently, the causal effect of a free press on government removal is equal to the commonness of insensitive governments in the relevant population of cases. \n\nWe have thus defined our causal query in terms of an exogenous variable, $\\pi^S$, in the probabilistic causal model. Note that, just as $S$ acts as a response-type variable for $X$'s effect on $Y$, querying $\\pi^S$ is equivalent to asking about the distribution of response types in the population. In our four-type framework, cases with $S=0$ are $b$ (positive effect) types with respect to the $X \\rightarrow Y$ relationship; cases with $S=1$ are $c$ types (no effect, $Y=0$). (There are, here, no $a$ or $d$ types.) Thus, $\\pi^S$ represents the share of $c$ types and $1-\\pi^S$ the share of $b$ types in the population, vis-a-vis $X$'s effect on $Y$. \n\nWe can follow the same procedure for all causal relationships in the model. Returning, for instance, to the effect of $R$ on $Y$, we learned from the subgraphs that $R$ has a causal effect of $1$ in panels $A,B$ and $C$---that is, whenever it is not the case that $X=1$ and $S=1$---and otherwise of $0$. Thus, the $R$'s average causal effect is the weighted average $1 \\times (1-\\pi^S \\times \\pi^X) + 0 \\times \\pi^S \\times \\pi^X$ = $1-\\pi^S \\times \\pi^X$. This is simply the probability of not having both $X=1$ and $S=1$. Here, then, we have defined the causal query in terms of two exogenous nodes in the probabilistic model, $\\pi^S$ and $\\pi^X$. ^[Likewise, the average causal effect of $R$ conditional on $S=1$ is $1-\\pi^X$ (the probability of ending up in panel B, rather than D); and the average causal effect of $R$ given $S=0$ is 1 (since it has an effect in both panels A and C).]\n\n**TO BECOME A LONG FOOTNOTE...**\nThese quantities can be calculated from the distributions in the same way as we calculated the case-level effects. Removing the arrows pointing to $R$, the distribution over nodes when $R=r'$---but this time not fixing $S$ and $X$---is $P(s,x,c,y | \\hat{r}=r')$. Again the key part is $P(y=1| \\hat{r}=1)$, which can be written $\\sum_x\\sum_s\\sum_c P(x)P(s)P(c|x,s)P(y|c, r= 1)$. Using the structural equations, this simplifies to $\\sum_x\\sum_s P(x)P(s)P(c=1|x,s) = P(x=0)P(s=0) + P(x=0)P(s=1) + P(x=1)P(s=0)$, or, $1-\\pi^S\\pi^X$.\n\nIn the same way, we can construct the average treatment effect for each of the exogenous variables:\n\n* $\\tau_X = E_S(Y(X=1|S)-Y(X=0|S)) = -(1-\\pi^S)$\n* $\\tau_S = E_X(Y(S=1|X)-Y(S=0|X)) = \\pi^X$]\n**LONG FOOTNOTE ENDING HERE**\n\nIn general, then, we can define queries about average causal effects as queries about the exogenous nodes that represent a causal model's probabilistic components. In the present example, probabilistic components enter only as determinants of the initial substantive causal variables. In other models, variables further downstream might also have stochastic components, a query about average causal effects might include thus further exogenous terms representing population-level distributions. Estimating average causal effects thus amounts drawing inferences about these nodes.^[Given the model, data will be useful for estimating average effects only if one is uncertain about the distributions of $S$ and $X$, which are a function of $U_S$ and $\\pi^S$ and $U_X$ and $\\pi^X$, respectively. In this example $\\pi^S$ and $\\pi^X$ are fixed in the model and so we do not learn anything about them from data. If however $\\pi^S$ and $\\pi^X$ are represented as nodes that are themselves produced by some other distribution -- such as a Beta distribution --- then the question of understanding average effects is the question of making inferences about these nodes.]\n\n<!-- I find the previous paragraph quite confusing in terms of what all of this says about learning about nodes. I would think we would want to set up the example so that we *can* use data to learn about the ATE and so that this runs through learning about root nodes.  -->\n\n**Actual cause.** Returning to a case-level query, the concept of an actual cause becomes useful when outcomes are overdetermined. Suppose there is a case with a sensitive government ($S=1$) and no free press ($X=0$), as in panel B. Then the *survival* of the government is over-determined: neither government sensitivity nor the free press is a counterfactual cause. (A lack of a free press is enough for even a corrupt government to survive; and sensitivity ensures non-corruption and, thus, survival even in the presence of a free press.) \n\nNevertheless, we can distinguish between the causes in terms of which one was an actual cause. Conditioning on there being corruption, which there actually was, the lack of a free press *is* a counterfactual cause of government survival: if there had been a free press, holding corruption constant, then the government would have been removed. This makes the lack of a free press an actual cause---that is, a counterfactual cause when some (or no) feature of what actually happened is kept fixed. However, there is no set of realized variable values that we can condition on to make the presence of a sensitive government a counterfactual cause; thus, it is not an actual cause.\n\nThe context---the values of the exogenous nodes in the subgraphs, $S$ and $X$---determines which variables will be actual causes through setting the realized values of all endogenous variables in the model and thus restricting the values on which conditioning is permitted for the determination of actual causes. Since corruption *is* present whenever $S=1$ and $X=0$, we are permitted in this context to condition on its presence, and the free press is an actual cause of government retention. In contrast, the sensitivity of the government is not an actual cause in this same context. Given no free press, there will always be corruption but no reporting on corruption, which makes government removal impossible, regardless of government sensitivity; thus, there is no subset of actual events that, when kept fixed, would make a change to a non-sensitive government result in the government's fall. In sum, asking whether a variable in a model was the actual cause of an outcome can equivalently be understood as asking about the values of the model's exogenous nodes. Answering that question will consist either of directly observing those exogenous conditions or drawing inferences about them from other, observable nodes.\n\n<!-- This example has the odd feature that the question of whether S or X is an actual cause is itself already conditional on the values of S and X.   -->\n\n**Notable cause.** In the event that that there is a non-sensitive government ($S=0$) and a free press ($X=1$), as in panel $C$, the government gets replaced and *both* of the two causes matter counterfactually for government replacement. (Absent either one, the government would survive.) Again, however, we can distinguish between them, this time on the basis of notable causation. The question, for identifying a notable cause, is how commonly the causal variable in question takes on its realized, as opposed to a counterfactual, value. Thus, like average causal effects, notable causation depends on *population-level* distributions---in the present example, on the parameters $\\pi^S$ and $\\pi^X$. If, for instance, governments are more frequently sensitive (the counterfactual) than non-sensitive (the actual value)---i.e., $\\pi^S > 0.5$---then the non-sensitive government is a notable cause. However, if free presses are rarer than non-sensitive governments---i.e.  $\\pi^X < 1-\\pi^S$---then the free press is a *more* notable cause than the non-sensitive government.\n\n<!-- Again, would be more consistent with out queries-as-root-nodes to show the pi's as nodes. -->\n\n**Causal Paths.** Note finally that different causal paths can give rise to the same outcome, where the different paths can be distinguished based on values of root nodes $S$ and $X$. For example, the government may be retained ($Y=0$) because there is no free press ($X=0$) and so no negative reporting on the government, regardless of the value of $S$; or because, there is a free press ($X=1$) and a sensitive government ($S=1$) takes account of this and does not engage in corruption. **\\color{red} To be improved to link more closely to our abstract discussion of paths as estimands.**\n\n\n\n<!-- What still bugs me a bit is that we don't have a way of showing, in this example, different causal paths for the same *causes* and outcomes. -->\n",
    "created" : 1502057911158.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1900608534",
    "id" : "6D8C9446",
    "lastKnownWriteTime" : 1502060660,
    "last_content_update" : 1502060660,
    "path" : "~/Dropbox/04_RA and TA work/02 Macartan/ProcessTracing/6 Book/ii/02-causal-estimands.Rmd",
    "project_path" : "02-causal-estimands.Rmd",
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}
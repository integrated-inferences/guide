# Theories as causal claims

```{r, include = FALSE} 
source("hj_dag.R")
source("function_perm.R")
library(hop)
source("function_tree.R")

```

**\color{blue}Goal: We introduce the idea of thinking of (applied) theoretical claims as causal models. Lower level models serve as a theory for a higher level model if the higher level model can be deduced from the lower level model. The empirical content of a lowe level model is the possible reduction in variance of the higher level model that it can provide.**


Theory means many things in different communities. Consider three usages.

**Theory as tautology.** The claim that the number of Nash equilibria is generically odd in finite games is a theoretical claim. Unless there are errors in the derivation of the result, the claim is true in the sense that the conclusions follow from the assumptions. There is no evidence to look for in the world to test the claim. The same can be said of the theoretical claims of many formal models in social sciences; they are theoretical deductions of the if-then variety [@clarke2012model]. Theory, in this sense, is true (by a large tautology).

**Theory as model.**  Although @clarke2012model argue for a separation of the ideas of model and theory, it is common for social scientists to use the terms interchangeably to denote an abstract representation of some part of the world that is of interest. For instance, a model may stipulate that outcome $X$ can have a positive effect on $Y$ because $X$ can cause $M$ and $M$ can cause $Y$.  One can read from a model how things work in the context of the model: for instance, if $M$ does not obtain, then under this model, $X$ does not cause $Y$.   One can use a model to make claims about the world only by assuming a mapping from elements in the model to elements in the worlds.  In this sense a  model is best thought of as an object [@clarke2012model]; whether the model itself is true or false is, in this usage, not a coherent question. 

**Theory as empirical claim.** In common usage, "a theory of" a phenomenon is a direct claim about the phenomenon, in the world. The claim that natural resources cause conflict is a theoretical claim of this form. The claim is certainly not true by definition, and empirical evidence can be used to assess it. In this claim, the *theory*, as usually understood, is certainly thin; the claim is no more than an empirical proposition, and it possesses no internal logic. Yet, more elaborate collections of empirical propositions are easily constructed. For instance: natural resources cause conflict because they can finance secessionist claims in resource rich areas.^[This latter claim does seem to possess something like a logic; though it does not take much to see that the logic is just a slightly more elaborate set of empirical claims. The outcomes do not follow  *logically* from the causes----there is no logical reason why, secessionist claims would cause conflict, but the theory---as a collection of claims---has implications similar to those in the model in the paragraph above: if there are no secessionist claims, then under this theory, natural resources are not causing conflict.] Theory in this sense can certainly be right or wrong.


The distinction between the last two accounts is sometimes confusing, and  @clarke2012model make a case for cleaning up the language on this front. In their account, drawing on @giere2010explaining, a theory might be best thought of as a set of models accompanied by hypotheses linking the model to the question of interest in the  world. 

<!-- , with these claims,  perhaps derived or inspired from some model via a statement that the model represents the world faithfully for some purpose.  -->

<!-- The key difference as we see it is between representations of a system, a model, and claims that the model itself represents another system---the world---in some ways. The difference betw  -->


## What is a theory? An applied answer.

In this book we will take a somewhat idealist position and assume that we are permanently inhabiting a world of models. We will say that a theory is an *explanation* of a model. That is, a theory of a model is another model from which the model of interest can be deduced, in combination with data. 

Take a syllogism. "All men are mortal" is a model, which coupled with the data "Socarates is a man" implies the model "Socrates is mortal." 

Similarly, take the functional equation $f_1: Y=X_1X_2$. Coupled with data $X_2=1$, $f_1$ implies the functional equation $f_2: Y=X_1$. 

We see this approach to models as following in the spirit of  @clarke2012model and  @giere2010explaining yet also as being consistent with the treatment of models in the literature on probabilistic causal models with which this book is centrally engaged. A nice feature is that it preserves a close associated between theory and explanation and it incorporates naturally the notion of deduction without requiring that models themselves arestatements of the  *if-then* variety.

More formally, we will say that a causal model (probabilistic or functional), $M^\prime$, is a *theory* of $M$ if $M$ can be derived from $M^\prime$. In such cases we  will refer to $M$ as a *higher*-level model relative to $M^\prime$, and to $M^\prime$ as a *lower*-level model relative to $M$.^[This definition differs somewhat from that given in @pearl2009causality (p207): there a theory is a (functional) causal model and a restriction over $\times_j \mathcal{R}(U_j)$, that is, over the collection of contexts envisionable. Our definition also considers probabilistic models as theories, allowing statements such as ''the average effect of $X$ on $Y$ is 0.5.''] 

Higher-level models can be generated from lower-level models in two ways, both of which are consistent with common understandings of what it is for a set of claims to constitute or, conversely, derive from a ''theory.''


1. Aggregating nodes: A higher-level model $M^\prime$, can be a representation of $M$ in which multiple nodes in $M^\prime$ have been aggregated into a single node or in which one or more nodes have been dropped. Conversely, $M$, can be theorized by a lower-level model, $M^\prime$, in which new nodes have been added and existing nodes split.

For instance, suppose we start with $M$ as represented in  Figure \ref{fig:K}(a). We can then offer the graph $M'$ in panel (b) as a *theory* of $M$. Informally, we have added a step in the causal chain between $X$ and $Y$, a familiar mode of theorization. However, to see why and when adding a node may be helpful for inference we have to formalize how the two models relate to each other.
<!-- We should have a 4-panel figure to go with this, esp to get the U business straight for readers: -->


```{r, echo = FALSE, fig.width = 7, fig.height = 5, fig.align="center", out.width='.7\\textwidth', fig.cap = "\\label{fig:K} A model with one explanatory  variable (top left), two lower level models that can imply it, and one model that does not."}

par(mfrow = c(2,2))
par(mar=c(1.5,1.5,3.5,1.5))
hj_dag(x = c(1,1,2,2),
       y = c(1,2,1,2),
       names = c(
         expression(paste(X)),
         expression(paste(U[X])),  
         expression(paste("Y")),  
         expression(paste(U[Y]^{higher}))),
       arcs = cbind( c(2,1, 4),
                     c(1,3, 3)),
       title = "(a) Simplest X causes Y graph",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)


hj_dag(x = c(1,1,2,2, 1.5, 1.5),
       y = c(1,2,1,2, 1  , 2),
       names = c(
         expression(paste(X)),
         expression(paste(U[X])),  
         expression(paste("Y")),  
         expression(paste(U[Y]^{lower})),
         expression(paste(K)),
         expression(paste(U[K])) 
         ),
       arcs = cbind( c(2,1, 4, 6, 5),
                     c(1,5, 3, 5, 3)),
       title = "(b) Lower level graph 1:\nMediator specified",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)

hj_dag(x = c(1,1,2,2, 1.5, 1.5),
       y = c(1,2,1,2, 1.5, 2),
       names = c(
         expression(paste(X)),
         expression(paste(U[X])),  
         expression(paste("Y")),  
         expression(paste(U[Y]^{lower})),
         expression(paste(K)),
         expression(paste(U[K])) 
         ),
       arcs = cbind( c(2,1, 4, 6, 5),
                     c(1,3, 3, 5, 3)),
       title = "(c) Lower level graph 2:\nOrthogonal second cause",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)

hj_dag(x = c(1,1,2,2, 1.5, 1.5),
       y = c(1,2,1,2, 1.5, 2),
       names = c(
         expression(paste(X)),
         expression(paste(U[X])),  
         expression(paste("Y")),  
         expression(paste(U[Y]^{lower})),
         expression(paste(K)),
         expression(paste(U[K])) 
         ),
       arcs = cbind( c(2,1, 4, 6, 5, 5),
                     c(1,3, 3, 5, 3, 1)),
       title = "(d) An incompatible graph",
       add_functions = 0, 
       contraction = .16, 
       padding = .2
)

```



In the model $M$, $Y$ is a function of just $X$ and a disturbance $U_Y$, the latter representing all things other than $X$ than can affect $Y$. When we add $K$, $X$ now does not directly affect $Y$ but only does so via $K$. Further, in the general case, we would explicitly model $X$ as acting on $K$ "with error" by modeling $K$ as a function of both $X$ and $U_K$. As we emphasize further below, it is in fact only this "error" in the $X\rightarrow K$ link that makes the addition of $K$ potentially informative as a matter of research design: if $K$ were a deterministic function of $X$ only, then knowledge of $X$ would provide full knowledge of $K$, and nothing could be learned from observing $K$. What $U_K$ represents, then, is that part of the original $U_Y$ that is a consequence of some variable other than $X$ operating at the *first* step of the causal chain from $X$ to $Y$. We are thus taking that part of $Y$ not determined by $X$ and splitting it in two: into a non-$X$ input into $K$ and a non-$K$ (and thus also non-$X$) input into $Y$. Addition and splitting thus go hand-in-hand: the *insertion* of a mediator between $X$ and $Y$ generally also involves the *splitting* of $Y$'s unspecified parent ($U_Y$).   Importantly, we distinguish between the $U_Y$'s at the two levels by referring to $U_Y^{lower}$ in $M^\prime$ and $U_Y^{higher}$ in $M$. 

Consider next the model in  Figure \ref{fig:K}(c), which also implies the higher-level theory in panel $(a)$. But the logical relationship between models $(a)$ and $(c)$ is somewhat different. Here the lower level theory  *specifies* one or more of the conditions comprising $U_Y^{higher}$. As we have now extracted $U^K$ from $U_Y^{higher}$, the unspecified term pointing into $Y$ is now relabeled $U_Y^{lower}$ because it represents a different distribution. $M^\prime$ is a *theory* of $M$ in that it, in a sense, helps explain the dependencies of $Y$ on $X$ more fully than does $M$.

To turn the situation around, when we move up a level and eliminate a node, we must be careful to preserve all causal dependencies among remaining nodes. In particular, all of the eliminated node's parents become parents of all of that node's children. Thus, for instance in $M'$, since $K$ is a function of both $X$ and $U_K$, in a higher-level model omitting $K$, both $X$ and $U_K$ become parents of $K$'s child, $Y$. Recall that $U_K$ represents the part of $K$ not fully determined by $X$; thus, to retain all causal determinants of $Y$ in the graph, $U_K$ must (along with $X$) be carried forward as a parent of $Y$ when $K$ is removed. Rather than drawing two separate $U$ terms going into $Y$, however, we simply represent the combined root as $U_Y^{higher}$, with the ''higher'' signaling the aggregation of roots. 

Nodes with no parents in $\mathcal{U}\cup\mathcal{V}$ cannot be eliminated as this would entail a loss of information. The graph in Figure \ref{fig:K}(d) illustrates the importance of this. Here $K$ is a cause of both $X$ and $Y$, in other words it is a possible confounder. A higher-level graph that does not include $K$ still requires a $U_K$ node pointing into both $K$ and $Y$ to capture the fact that there is a confounder.  

In Figure \ref{fig:running_subs}, we show the permissible reductions of our running example (from Figure \ref{fig:running}). We can think of these reductions as the full set of simpler claims (involving at least two nodes) than can be derived from the lower-level theory. In each subgraph, we mark eliminated nodes in grey. Those nodes that are circled must be replaced with $U$ terms. The arrows represent the causal dependencies that must be preserved. Note, for instance, that neither $S$ (because it has a spouse) nor $X$ (because it has multiple children) can be simply eliminated; each must be replaced with an unspecified variable. Also, the higher-level graph with nodes missing can contain edges that do not appear in the lower-level graph: eliminating $D$, for instance, forces an edge running from $X$ to $Y$, just as eliminating $K$ produces a $S\rightarrow Y$ arrow. The simplest elimination is of $Y$ itself since it does not encode any feature of dependencies (not conditional on $Y$ itself) between other variables.
<!-- ^[Put differently, and in language that we introduce below, colliding arrowheads do not represent a path in DAG analysis.]  -->

We can also read Figure \ref{fig:running_subs} as telling us the set of claims for which the lower-level graph in Figure \ref{fig:running} can serve as a theory. For each reduction, there may be  other possible lower-level graphs consistent with it. 

One  effect of elimination is to render seemingly deterministic relations effectively probabilistic. For example, in the lower level graph $C$ is a deterministic function of $X$ and $S$. But in higher level graphs it can depend probabilistically on one of these: in submodel 21, $C$  depends probabilistically on  $X$ since $S$ is now a stochastic disturbance; in 34 $C$ depends probabilistically on $S$. This illustrates how unobserved or unidentified  features render a model "as-if" stochastic. Conversely, models that exclude this form of uncertainty implicitly claim model-completeness.    

Aggregation of nodes may also take the form of "encapsulated conditional probability distributions" [@koller2009probabilistic] where in a system of nodes, $\{Z_i\}$  is represented by a single node, $Z$,  that takes the parents of  $\{Z_i\}$ not in $\{Z_i\}$ as parents to $Z$ and issues the children of $(Z_i)$ that are not  in $(Z_i)$ as children.

Consider a second manner in which a higher level model  can be deduced from a lower level model, this time in conjunction with data (or more broadly. ancillary claims):

2. A higher level model may be formed by conditioning on values of nodes in a lower level model. Conversely, a higher-level functional model, $M$, can be theorized via a lower-level $M^\prime$ in which conditions shaping the operation of the causal effect in $M$, unspecified in $M$, are now specified. 


To illustrate this approach, consider again the graphs in Figure \ref{fig:K}. Above we described how the graph in  panel (a) can be produced by aggregating $U_Y^{lower}$ and $U_K$ from panel (c). An alternative possibility is to simplify by conditioning: we derive a higher-level graph from $M^\prime$ by fixing the value of $K$. For instance, if $Y=XK+U_Y^{lower}$ in $M'$, then at $K=1$, we have the submodel $M_k$ in which $Y=X+U_Y^{lower}$. Note that, in generating a submodel by conditioning on $K$, we retain the term $U_Y^{lower}$ as we have not added causal force into $Y$'s unspecified parent.

As we will see, thinking about models as conditionally nested within one another can be empirically useful in providing a way for  analysts to more fully specify incomplete higher-level claims by reference to lower-level models within which they are implicitly embedded and thus to make explicit unspecified conditions on which the higher-level relationships depend.


```{r, echo = FALSE, fig.width = 12, fig.height = 13.5, , fig.cap = "\\label{fig:running_subs} Higher level models derived from the model in Figure \\ref{fig:running}. Nodes that are eliminated are marked in grey; circles denote root nodes that are replaced in subgraphs by unidentified variables. (A circled node pointing into two children could equivalently be indicated as undirected edge connecting the children.) Note that $C$, $D$, and $Y$ are deterministic functions of $X$ and $S$ in this example. "}
par(mfrow = c(5,5))
par(mar=c(1,1,3.5,1))
x = c(0,0, 1, 1, 2)
y = c(2,0, 2, 0, 1)
names = c("S", "X", "C", "D", "Y")

M <- matrix(0, 5, 5)
M[1, c(3)] <-1
M[2, c(3,4)] <-1
M[3, c(4,5)] <-1
M[4, 5] <-1

matrix_remove <- function(M, remove = NULL){
  M2 <- M
if(!is.null(remove)) {
  for(j in remove) {M2 <- (M2 + outer(M[, j], M[j,]))}  # connect parents to children
  for(j in remove) {if(sum(M2[,j]>0)) {M2[j,] <- 0; M2[,j] <- 0}}  # disconnect non-roots (roots may have to be renamed)
}
M2[M2>1] <- 1
M2
}

removes <- perm(c(2,2,2,2,2))[-1,]
removes <- removes[rowSums(removes)<=3,]
removes <- removes==1
removes <- removes[order(rowSums(removes) - (1:nrow(removes))/100),]
for(j in 1:nrow(removes)){
GO <- (1:5)[removes[j,]]
    hj_dag(
       x = x,
       y = y,
       names = names,
       arcs = which(matrix_remove(M, GO)==1, arr.ind = TRUE),
       add_points = FALSE,
       solids = c(mysolids[[j]]),
       )
text(x[GO],y[GO],names[GO], col = "grey")
title(j, adj=0)
for(i in 1:ncol(M)) if((sum(M[,i])==0) & (i%in% GO)) points(x[i], y[i], cex = 3, col = "grey")
}
```


Note that the mapping from theories to higher-level claims may not be one-to-one. A single theory can support multiple higher-level theories. Moreover, a single higher-level relation can be supported by multiple, possibly incompatible lower-level theories. To illustrate, consider two theories:

\begin{itemize}
\item[$L_1$:] $X_1 \rightarrow X_2  \rightarrow Y$ 
\item[$L_2$:] $X_1 \rightarrow Y  \leftarrow X_2$ 
\end{itemize}

These two theories record different relations of conditional independence: in $L_2$, $X_1$ and $X_2$ are independent, but they are not independent in $L_1$. Also, in $L_1$, $X_1$ is independent of $Y$ conditional on $X_2$; but this is not the case in $L_2$. Now consider the following higher-level models:

\begin{itemize}
\item[$H_1$:] $X_1 \rightarrow Y$
\item[$H_2$:] $X_2 \rightarrow Y$
\item[$H_3$:] $X_1 \rightarrow X_2$
\end{itemize}

Both $H_1$ and $H_2$ are consistent with both $L_1$ and $L_2$. However, $H_3$ can be supported only by $L_1$ and not by $L_2$. In addition, the *conditional* higher-level model $((X_1 \rightarrow Y)|X_2 = x_2)$ can be supported by model $L_2$ but not by model $L_1$. 

Thus multiple (possibly *incompatible*) theories can usually be proposed to explain any given causal effect; and any given theory implies multiple (necessarily *compatible*) causal effects. This suggests that there is no generic sense in which a lower-level theory is more or less general than a higher-level theory. For example, a higher-level theory that is formed by conditioning on a node in a lower-level theory is less general in that it makes sense of fewer cases. On the other hand, a higher-level theory that is formed by aggregating nodes may be more general in that it is consistent with multiple lower-level theories that explain the relationships it contains, even if these lower-level theories are not consistent with each other.

Perhaps surprisingly, in this treatment, the theoretical support for a causal model is itself just another causal model: a set of beliefs about structural relations between variables. Thus, a theory is an object that is formally similar to an empirical claim. 

<!-- Would like to clarify last sentence above. -->

The approach can even handle theoretical propositions in the form of structural causal models, as described above, that make no immediate empirical claims but still have "empirical content" in the sense of being able to inform *conditional* claims. The claim "if $X$ then $Y$" says nothing about $P(Y)$, but it says a lot if $P(X)$ is known. 

<!-- This next paragraph is hard to follow. The u's come out of nowhere. What's a mapping from R1 to R1? Generally seems like the points could be made more simply. -->

This approach allows for an assessment of two features sometimes considered important to assess empirical content of a theory: the level of *universality* of a theory and the degree of *precision* of a theory [@glockner2011empirical]. For instance, consider a theory over $X_1, X_2, A, B, Y$ that specified $X_1, X_2 \rightarrow Y \leftarrow A, B, g$ with functional equations:

$$Y = \left\{ \begin{array}{ccc} 
A + BX_1 & \text{ if } & X_2 = 1\\   
  g(X_1) &\text{ if } & X_2 = 0 \end{array} \right.$$ 

where the domain of $g$, $\mathcal{R}(g)$, is the set of all functions that map from $\mathbb{R}^1$ to $\mathbb{R}^1$, and the ranges of $A$ and $B$ are the real number line. Say the distributions over $A, B, X_1, X_2$, and  $g$ are not specified. Then the theory makes a precise claim conditional on $u_1, u_2, X_1, X_2$, and  $g$. But since the distribution over $\mathcal{R}(g)$ is not provided by the theory, the theory only claims knowledge of a functional form for $Y$ for those cases in which $X_2=1$. Thus in this case the *universality* of the theory for the claim "$Y$ is a linear function of $X$," is  $P(X_2=1)$. This is the domain over which the theory has something to say about this proposition. Note that in this case the universality is not  provided by the theory, but is rather an external proposition that depends on additional data. The *precision* of the theory depends both on the claim of interest and the distribution of root variables. For example, the precision of the theory for the causal effect of $X_1$ on $Y$ when $X_2=1$ depends on the distribution of $B$: the theory is more precise about this causal effect the less uncertainty there is about the value of $B$. Moreover, a theory that specified that $B$ has large variance would be making a precise claim about causal *heterogeneity*, even if it was imprecise about the causal effect. Again this feature cannot be read from the theory without access to ancillary information that the theory itself does not provide.

<!-- AJ comments: -->

<!-- - Not clear what the "this approach" is that this is illustrating a feature of. Expressing theory as structural equations?  -->

<!-- - There seems to be a more specific point here than just that we can assess universality and precision. In fact, it's actually that we *can't* assess these things purely from structural models. We need probabilistic models or information on variable values, no? -->

<!-- - Universality seems an odd term for a concept that is continuous. Generality? Coverage? -->

<!-- - There was switching between precision and specificity. I've gone with precision, but could see either. -->



<!-- Better to replace type with variables.  -->

<!-- For example rather thatn $X \rightarrow Y \leftarrow Q$ where $Q$ takes on values of the four tyes.  -->
<!-- ROUGH NOTES -->
<!-- Say $Y = AX +B(1- X)$ in which case  uncertainty over functional forms, or  uncertainty about undefuned causal types  can be reconceptualized as uncertainty around positive and negative drivers -->


<!-- Drawing on different theories for subcomponents; eg theory of human decision making.  -->
<!-- More or less specified theories.  -->

<!-- Encapsulated CPDs are one way to wrap subtheories. Two identical DAGS could have two distinct theoretical underpinnings in the sense of having different encapsulated CPDs.    -->

<!-- When is one theory more general than another? When is one more specified than another? -->

<!-- If you specify a more detailed theory, the theory has more testable implications, but it is less general.  -->

<!-- Theory T'' is implied by theory T' if V'' is a subset of V' and the relations in T'' are implied by the relations in the reduced set T'' -->

<!-- For example:  -->
<!-- T'': A = 1 with prob .5, B = 1 with prob .5, if A = 1; 0 otherwise, C = 1 if B = 1 -->
<!-- T'': A = 1 with prob .5, C = 1 with prob .5, if A = 1; 0 otherwise -->
<!-- Different  subparts of theories may be implied by distinct theories provided implication relations satisfied -->
<!-- eg T'' part 1 may be implied by T, and T' implied by T'''', but T'' and T'''' not mutually consistent -->

<!-- Theory T'' explains more than T' if it identifies more causal relations -->
<!-- Theory T'' is more fertile  than T' if it implies more theories (same?) -->
<!-- Theory T'' has more observable implications that Theory T' if.... -->
<!-- Theory T'' is more falsifiable than theory T'.... -->
<!-- Theory T'' is more general (wide scope) than theory T' if its conditioning set is a  subset of theory T''s conditioning set -->
<!-- Theory T'' is more parsimonious  than theory T' if it predicts the same or more causal relations  with fewer nodes -->
<!-- Theory T'' is more complete (fully specified) than theory T' if it has lower prior variance (?) -->

<!-- Key -- there is no distinction between aleatory and epistemic uncertainty. It is all epistemic.  -->
<!-- eg the goalie might well randomize, but if we do not know which way she will go it is because we do not know  -->
<!-- what randomizing device she is using -->
<!-- Key: priors specified over all relations -->
<!-- Possibly need that nodes have classes -- eg utility -- that are used for some lo level theories -->
<!-- Question; ultimately is htere only one net and the universe is a realization of it? -->


Functional (but not probabilistic)  causal models allow for the representation of logically derived relations between nodes without implying any unconditional empirical claims; that is, all claims may be of the  *if-then* variety, as is typical for example of propositions derived from game theoretic models. The process of connecting such models to the empirical claims can be thought of as the embedding of these incomplete models within larger structures. 

Consider for example the claim that in normal form games,  players play Nash equilibrium.  This claim in itself is not a tautology; that is, it is not a result. It can be contrasted for example with the *analytic result* that when rational players play a game and players have common knowledge of the game structure and of player rationality they will only play ''rationalizable'' strategies. Even still, the Nash claim does provide a set of analytically derived functional equations that relate nodes that describe game forms to actions  taken, and from actions to utilities. Representation as a causal graph can make explicit what conditional independencies are assumed in the move from analytic results to empirical claims. For example, are actions independent of the game form conditional on beliefs about the game form; are utilities independent of expectations conditional on actions, and so on.

We give an example of one such model below when we turn to extensive-form games for a lower-level theory that supports our running example.  

### Assessing the gains from a theory
The  observation that theories vary in their precision points to a method for describing the learning that is attributable to a lower-level theory relative to a higher level theory. When a lower-level theory represents a disaggregation, the lower-level theory identifies a set of potentially observable variables that are not listed by the the higher-level theory. This allows one to assess the gains in precision (for some collection of unobserved variables) that can arise from  learning the values of additional observables in the lower-level theory. 

Suppose that the contribution of a lower-level theory is to allow for inferences from new data $K$ about some set of query variables $Q$, after we have already observed variables $W$ from the higher-level model.  If we use the expected  squared error from the mean posterior estimate as a measure of precision for collection  $Q$, then we have a measure of loss:

$$E_{k, q} \left(\left( \int q' P(q' | k, w)dq' - q\right)^2\right)$$
where the expectation is taken over the joint distribution of $K$ and $Q$, given $W$. This is an expected loss---or the *Bayes risk*. The inner term $P(q'|k, w)$ is the posterior distribution on $q'$ given observation of $k$ and $w$. 

Another way to think of the gains is as the expected reduction in the variance of the Bayesian posterior: how certain do you expect you will be after you make use of this new information?

In fact these two quantities are equivalent  (see for example @scharf1991statistical). Moreover, it is easy to see that whenever inferences are sensitive to $K$, the expected variance of the posterior will be lower than  the variance of the prior. This can be seen from the law of total variance, written here to highlight the gains from observation of $K$, given what is already known from observation of $W$.^[A similar expression can be given for the expected posterior variance from learning $K$ in addition to $W$ when $W$ is not yet known. See, for example, Proposition 3 in @geweke2014analysis.]  
$$Var(Q|W) = E_{K|W}(Var(Q|K,W)) +Var_{K|W}(E(Q|K,W))$$

The contribution of a theory can then be defined as the mean reduction in Bayes risk:

$$\text{Gains from theory} = 1- \frac{E_{K|W}(Var(Q|K,W))}{Var(Q|W)}$$

This is a kind of $R^2$ measure (see also @gelman2006bayesian). 

Other loss functions could be used, including functions that take account of the costs of collecting additional data,^[Further, one might call into question the value of a theory if the gains in precision depend upon data that are practically impossible to gather.] or to the risks associated with false diagnoses.^[For instance, in @heckerman1991toward, an objective function is generated using  expected utility gains from diagnoses generated based on new information over diagnoses based on what is believed already. In their treatment [@heckerman1991toward, Equation 6],  the expected value of new information $K$, given existing information $W$ is: $\sum{K}P(K|W)( EU(d(Q,W,K)|W, K) - EU(d(Q, W)|W, K))$ where $EU$ is expected utility and $d$ is the optimal inference (diagnosis) given available data. Note that the diagnosis can take account of $K$ when it is observed, but the expected utility depends on $K$ whether or not it is observed, as $K$ carries information about the state of interest.] 

For illustration say that it is known that $X=1, Y=1$ and that, given this information (playing the role of $W$), the posterior probability that a unit is of type $b$ (and not type $d$) is $p$. Say then that a theory specifies that $K$ will take a value 1  with probability $\phi_j$ if the unit is of type $j$. Then what is the value added of this theory? Define $Q$ here as the query regarding whether the unit is a $b$ type. Then the prior variance, $Var(Q|W)$, is simply $p(1-p)^2 +(1-p)p^2 = p(1-p)$. 

<!-- Would be best to  write down the theory as a structural equation that has phi_j as p(K=1|j) -->

To calculate $E_{K|W}(Var(Q|K,W))$, note that the posterior if $K$ is observed is $\frac{\phi_bp}{\phi_bp+\phi_d(1-p)}$. Let us call this $\hat{q}_K$, and the belief when $K$ is not observed $\hat{q}_{\overline{K}}$.
In that case the  *expected error* is: 

$$\text{Expected Error} = p\phi_b\left(1-\hat{q}_K\right)^2+(1-p)\phi_d\hat{q}_K^2+p(1-\phi_b)\left(1-\hat{q}_{\overline{K}}\right)^2+(1-p)(1-\phi_d)\hat{q}_{\overline{K}}^2$$

where the four terms are the errors when $K$ is seen for a $b$ type, when $K$ is seen for a $d$ type, when $K$ is not seen for a $b$ type, and when $K$ is not see for a $d$ type.


Defining $\rho_K = (p\phi_b+(1-p)\phi_d)$ as the probability of observing $K$ given the prior, we can write the posterior variance as:

$$\text{Expected Posterior Variance} = \rho_K\hat{q}_K(1-\hat{q}_K)+(1-\rho_K)\hat{q}_{\overline{K}}(1-\hat{q}_{\overline{K}})$$


<!-- Making use of the fact that $\rho_K\hat{q}_K = ({\phi_bp+\phi_d(1-p)})\frac{\phi_bp}{\phi_bp+\phi_d(1-p)} = \phi_bp$ and similarly  -->
<!-- $(1-\rho_K)\hat{q}_{\overline{K}} = (1-\phi_b)p$, this can be written in terms of primitives as: -->

With a little manipulation, both of these expressions simplify to:

$$\text{Expected Posterior Variance} =p(1-p)\left(\frac{\phi_b\phi_d}{\phi_bp+\phi_d(1-p)} + \frac{(1-\phi_b)(1-\phi_d)}{(1-\phi_b)p+(1-\phi_d)(1-p)}\right)$$


The gains are then:

$$\text{Gains} =1- \frac{\phi_b\phi_d}{\phi_bp+\phi_d(1-p)} - \frac{(1-\phi_b)(1-\phi_d)}{(1-\phi_b)p+(1-\phi_d)(1-p)}$$

Other natural measures of gains from theory might include the simple correlation between $K$ and $Q$, or entropy-based measures (see @zhang2003properties for many more possibilities). 

For this problem the correlation is given by (see appendix):

$$\rho_{KQ} = \frac{(\phi_b+\phi_d)(1-2p)(p(1-p))^{.5}}{
(p\phi_b+(1-p)\phi_d)(1-(p\phi_b+(1-p)\phi_d)))^{.5}}$$

One might also use a measure of "mutual information" from information theory:

$$I(Q,K) = \sum_q \sum_k P(q,k)\log\left(\frac{P(q,k)}{P(q)P(k)}\right)$$

<!-- here: -->


<!-- \begin{equation} -->
<!-- \begin{aligned} -->
<!-- I(Q,K) ={} & p\phi_b\log\left(\frac{\phi_b}{p\phi_b+(1-p)\phi_d}\right)+ (1-p)\phi_d\log\left(\frac{\phi_d}{p\phi_b+(1-p)\phi_d}\right) \\ -->
<!--       & +p(1-\phi_b)\log\left(\frac{1-\phi_b}{1-p\phi_b-(1-p)\phi_d}\right)+ -->
<!-- (1-p)(1-\phi_d)\log\left(\frac{1-\phi_d}{1-p\phi_b-(1-p)\phi_d}\right) -->
<!-- \end{aligned} -->
<!-- \end{equation} -->

To express this mutual information as a share of variation explained, we could divide $I(Q,K)$ by the entropy of $Q$, $H(Q)$ where $H(Q) = -\sum_qP(q)\log(P(q))$. The resulting ratio can  be interpreted as 1 minus the ratio of the entropy of $Q$ conditional (on $K$) to the unconditional entropy of $Q$.

For this example, Figure \ref{fig:probative_value} shows gains as a function of $\phi_b$ given a fixed value of $\phi_d$. The figure also shows other possible measures of probative value, with, in this case, the reduction in entropy tracking the reduced posterior variance closely. 

```{r, echo = FALSE, fig.width = 7, fig.height = 5,  fig.align="center", out.width='.7\\textwidth', fig.cap = "\\label{fig:probative_value} The solid line shows gains in precision (reduced posterior variance) for different values of $\\phi_b$ given $\\phi_d=0.25$ and $p=.5$ for the example given in the text. Additional measures of probative value are also provided including $|\\phi_b - \\phi_d|$, the correlation of $K$ and $Q$, and the reduction in entropy in $Q$ due to mutual information in $Q$ and $K$."}



gains = function(p, phi_b, phi_d){
  1- (phi_b*phi_d)/(phi_b*p +phi_d*(1-p)) - (1-phi_b)*(1-phi_d)/((1-phi_b)*p+(1-phi_d)*(1-p))
}

corr_qk <- function(p, phi_b, phi_d){
  ((phi_b-phi_d)*(p*(1-p))^{.5})/
  (((p*phi_b+(1-p)*phi_d)*(1-(p*phi_b+(1-p)*phi_d)))^{.5})
  }
# Mutual Information
mi_qk <- function(p, phi_b, phi_d, base = 2){
 p*phi_b*        log({phi_b}   / {p*phi_b+(1-p)*phi_d}, base = base)+
(1-p)*phi_d*     log({phi_d}   / {p*phi_b+(1-p)*phi_d}, base = base)+
p*(1-phi_b)*     log({1-phi_b} / {1-p*phi_b-(1-p)*phi_d}, base = base)+
(1-p)*(1-phi_d)* log({1-phi_d}/ {1-p*phi_b-(1-p)*phi_d}, base = base)
  }

norm_mi_qk <-  function(p, phi_b, phi_d, base = 2){
    -mi_qk(p, phi_b, phi_d, base = base)/(p*log(p, base = base)+(1-p)*log(1-p, base = base))}

phi_b = seq(0,1,.01)

plot(phi_b, gains(.75, phi_b, .25), type = "l", xlab = expression(paste(phi[b])), ylab = "Probative Value")
  points(phi_b, abs(corr_qk(.75, phi_b, .25)), type = "l", lty=2)
  points(phi_b, (norm_mi_qk(.75, phi_b, .25)), type = "l", lty = 3)
  points(phi_b, abs(phi_b - .25), type = "l", lty = 4)
  title("Reduced posterior variance, correlation, mutual information")
text(.8, c(.15, .25, .3, .62, .41), c("I(K,Q)/H(Q)","(Reduced posterior variance)", "Gains",  expression(paste(abs(phi[b]-phi[d]))), "Cor(K,Q)"))

#plot(abs(corr_qk(.75, phi_b, .25)), gains(.75, phi_b, .25), type = "l", xlab = "Probative value")
#lines(abs(phi_b - .25), gains(.75, phi_b, .25), type = "l", lty=2)
#lines(norm_mi_qk(.75, phi_b, .25), gains(.75, phi_b, .25), type = "l", lty=2)


```

## Illustration of simple theories of moderation and mediation

### Mediation as Theory {#medtheory}
We begin with a simple theory: there are two binary variables, $X$ and $Y$, and $X$ causes $Y$ (probabilistically). This theory, such as it is, is represented in Figure \ref{fig:K}(a) above.

Although simple, one could imagine many structural equations representing this relationship. For example if $u_Y^{higher}$ is distributed normally and $Y$ takes on the value 1 if $bX+u_Y^{higher}$ is above some threshold, we have a probit model. In a very general formulation we can let $u_Y^{higher}$ be a variable that selects among four different causal types represented with the notation $t_{ij}$: we read the subscripts to mean that a unit of type $t_{ij}$ has outcome $i$ when $X=0$ and $j$ when $X=1$. Then let $u_Y^{higher}$ have a multinomial distribution over the four values of  $t_{ij}$ with event probabilities  $\lambda_{ij}^{higher}$. Note also that in this graph $X$ is independent of $u_Y^{higher}$, which means that it is as if $X$ is randomly assigned; for example, let $u_X\sim \text{Unif}[0,1]$ and $X = \mathbb{1}(u_K<\pi^K)$.^[The types here map directly into the four types, $a$, $b$, $c$, $d$, used in @humphreys2015mixing and into principal strata employed by Rubin and others. The literature on probabilistic models also refers to such strata as "canonical partitions" or "equivalence classes." Note that this model is not completely general as the multinomial distribution assumes that errors are iid.]


The functional equation for $Y$ is then given by: 
$$Y(x, t_{ij}^{higher}) = \left\{ \begin{array}{cc}  
i & \text{ if } x=0 \\ j & \text{ if } x=1 \end{array}  \right.$$

Now consider a  theory that specifies a mediating variable between  $X$ and $Y$. This theory is depicted in Figure \ref{fig:K}(b) above.

The lower-level functional equations are formally similar though now each unit's outcome (given $X$) depends on two event probabilities: one that determines type with respect to the effect of $X$ on $K$ ($t_{ij}^{K}$), and one with respect to the effect of $K$ on $Y$ ($t_{ij}^{Y}$):

$$Y(K, t_{ij}^{Y}) = \left\{ \begin{array}{cc}  
i & \text{ if } K=0 \\ j & \text{ if } K=1 \end{array}  \right.$$
$$K(X, t_{ij}^{K}) = \left\{ \begin{array}{cc}  
i & \text{ if } X=0 \\ j & \text{ if } X=1 \end{array}  \right.$$

Thus, in the lower-level model, there are sixteen types that derive from the cross product of two independent random terms.

Critically, one can derive the higher-level types from the lower level types, and beliefs about the higher level types from beliefs about the lower level types. For example, using the nomenclature in @humphreys2015mixing:

\begin{eqnarray*}
\text{adverse: }t_{10}^{high} &=& t_{01}^{K}\&t_{10}^{Y} \text{ or } t_{10}^{K}\&t_{01}^{Y} \\
\text{beneficial: }t_{01}^{high} &=& t_{01}^{K}\&t_{01}^{Y} \text{ or }  t_{10}^{K}\&t_{10}^{Y} \\
\text{chronic: } t_{00}^{high} &=& t_{00}^{Y} \text{ or }  t_{00}^{K}\&t_{01}^{Y} \text{ or }  t_{11}^{K}\&t_{10}^{Y}\\
\text{destined: }t_{11}^{high} &=& t_{11}^{Y} \text{ or }  t_{00}^{K}\&t_{10}^{Y} \text{ or }  t_{11}^{K}\&t_{01}^{Y}
\end{eqnarray*}

In the same way, the higher-level probabilities are implied by the lower level probabilities.

\begin{eqnarray*}
\text{adverse: }\lambda_{10}^{high} &=& \lambda_{01}^{K}\lambda_{10}^{Y} + \lambda_{10}^{K}\lambda_{01}^{Y} \\
\text{beneficial: }\lambda_{01}^{high} &=& \lambda_{01}^{K}\lambda_{01}^{Y} + \lambda_{10}^{K}\lambda_{10}^{Y} \\
\text{chronic: } \lambda_{00}^{high} &=& \lambda_{00}^{Y} + \lambda_{00}^{K}\lambda_{01}^{Y} + \lambda_{11}^{K}\lambda_{10}^{Y}\\
\text{destined: }\lambda_{11}^{high} &=& \lambda_{11}^{Y} + \lambda_{00}^{K}\lambda_{10}^{Y} + \lambda_{11}^{K}\lambda_{01}^{Y}
\end{eqnarray*}

Importantly, even without specifying a distribution over $U_K$ or $U_Y^{lower}$, a lower-level structural model could be informative by restricting the *ranges* of  $U_K$ or $U_Y^{lower}$. For instance, a lower level theory that imposed a monotonicity condition (no adverse effects) might exclude $t^K_{10}$ and $t^y_{10}$---that is, increasing $X$ never reduces $K$, and increasing $K$ never reduces $Y$. 

We return  to this example below and show how observation  of $K$ can yield inference on causal estimands when  the theory places this kind of a structure on relationships.

### Moderation as Theory {#modtheory}

Now consider an alternative lower-level theory. This theory is represented in  Figure \ref{fig:K}(c) above, in which  $K$ is assumed to be a second cause of $Y$. This graph  contains the substantive assumption that $K$ is orthogonal to $X$ as well as the assumption that $X$ is as-if randomly assigned. 

In this graph $u_K$ determines the value of $K$. For example, let $u_K\sim \text{Unif}[0,1]$ and $K = \mathbb{1}(u_K<\pi^K)$. Now $u_Y^{lower}$ is more complex as it determines the mapping from two binary variables into $\{0,1\}$. With this structure, $u_Y^{lower}$ selects among 16 causal types. Let $t_{ij}^{gh}$ denote a unit that has outcome $i$ when $X=0, K=0$, $j$ when $X=1, K=0$, $g$ when $X=0, K=1$, $h$ when $X=1, K=1$.  We let $u_Y^{lower}$ in this graph denote a multinomial distribution over the sixteen values of  $t_{ij}^{gh}$ with event probabilities  $\lambda_{ij}^{gh}$.

<!-- I changed abcd scripts above to ghij and made corresponding (I think) changes below. I don't care what it is but abcd obviously could be confusing in this context. -->

The sixteen types are illustrated in Table \@ref(tab:types2X) in the appendices.

Again, the types in the higher level mapping are functions of the types in the lower-level mapping. For example,  a unit has type $t_{01}$ in the higher level model if $K=1$ and it is of type $t_{00}^{01},t_{10}^{01},t_{01}^{01}$, or $t_{11}^{01}$, or if $K=0$ and it is of type $\lambda_{01}^{00},\lambda_{01}^{10},\lambda_{01}^{01}$, or $\lambda_{01}^{11}$. 

We write this as:

$$t_{01} =  ((K=1) \land (t^{lower} \in \{t_{00}^{01} \cup t_{10}^{01} \cup  t_{01}^{01} \cup t_{11}^{01} \}) \lor  ((K=0) \land (t^{lower} \in \{\lambda_{01}^{00} \cup \lambda_{01}^{10} \cup \lambda_{01}^{01} \cup \lambda_{01}^{11}\})$$

In the same way, the probability of type $t_{01}$ can be written in terms of the parameters of the lower-level graph.  Importantly, the parameters of the higher-level distribution  $u_Y^{higher}$ depend on both $u_K$ and $u_Y^{lower}$. Thus, unlike the mediation case above, the probative value depends on the likelihood of an *observable* event occurring. Specifically, the share of a given higher-level type is given by:

$$\lambda_{ij} = P(u_Y^{higher} = t_{ij}) = \pi^K\left(\lambda_{00}^{gh}+\lambda_{10}^{gh}+\lambda_{01}^{gh}+\lambda_{11}^{gh}\right)
+
(1-\pi^K)\left(\lambda_{ij}^{00}+\lambda_{ij}^{10}+\lambda_{ij}^{01}+\lambda_{ij}^{11}\right)$$

For example:

$$\lambda_{00} = P(u_Y^{higher} = t_{00}) = \pi^K\left(\lambda_{00}^{00}+\lambda_{10}^{00}+\lambda_{01}^{00}+\lambda_{11}^{00}\right)
+
(1-\pi^K)\left(\lambda_{00}^{00}+\lambda_{00}^{10}+\lambda_{00}^{01}+\lambda_{00}^{11}\right)$$


Conditional probabilities follow in the usual way. Consider, for instance, the case where it is known that $X=Y=1$ and so the posterior probability of type $t_{01}$ is simply $P(i \in t_{01} | X=Y=1) = \frac{\lambda_{01}}{\lambda_{01}+\lambda_{11}}$. Note that $\pi^x$ does not appear here as this $X$ is orthogonal to $u_Y$. The probability of type $t_{01}$, knowing that $X=Y=1$, can be written in terms of the parameters of the $u$ distributions in the lower-level graph. 

$$P(i \in t_{01} | X=Y=1) = \frac{
\pi^K\left(\lambda_{00}^{01}+\lambda_{10}^{01}+\lambda_{01}^{01}+\lambda_{11}^{01}\right)
+
(1-\pi^K)\left(\lambda_{01}^{00}+\lambda_{01}^{10}+\lambda_{01}^{01}+\lambda_{01}^{11}\right)
}{
\sum_{i = 0}^1\left(\pi^K\left(\lambda_{00}^{i1}+\lambda_{10}^{i1}+\lambda_{01}^{i1}+\lambda_{11}^{i1}\right)
+
(1-\pi^K)\left(\lambda_{i1}^{00}+\lambda_{i1}^{10}+\lambda_{i1}^{01}+\lambda_{i1}^{11}\right)
\right)}$$

We return below to this example and describe how the lower-level model can be used to generate inferences on relations implied by the higher level model. 




## Illustration of a Mapping from a Game to a DAG

Our running example supports a set of higher level models, but it can also  be *implied* by a lower level models. Here we illustrate with an example in which the lower level model is a game theoretic model, together with a solution.^[Such representations have been discussed as multi agent influence diagrams, for example in @koller2003multi or @white2009settable on "settable systems"--- an extension of the "influence diagrams" described by @dawid2002influence.] 

In Figure \ref{fig:tree} we show a game in which nature first decides on the type of the media and the politician -- is it a media that values reporting on corruption or not? Is the politician one who has a dominant strategy to engage in corruption or one who is sensitive to the risks of media exposure? In the example the payoffs to all players are fully specified, though for illustration we include parameter $b$ in the voter's payoffs which captures utility gains from sacking a politician that has had a negative story written about them *whether or not they actually engaged in corruption*. A somewhat less specific, though more easily defended, theory would not specify particular numbers as in the figure, but rather assume ranges on payoffs that have the same strategic implications.  

The theory is then the  game plus a solution to the game. Here for a solution the theory specifies subgame perfect equilibrium.

In the subgame perfect  equilibrium of the game; marked out on the game tree (for the case  $b=0$) the sensitive politicians do not engage in corruption when there is a free press -- otherwise they do; a free press writes up any acts of corruption, voters throw out the politician if indeed she is corrupt and this corruption is reported by the press.  

As with any structural model, the theory says what will happen but also what *would* happen if things that should not happen happen. 

```{r game1, echo=FALSE, fig.width = 15, fig.height = 12, fig.cap = "\\label{fig:tree} A Game Tree. Solid lines represent choices on the (unique) equilibrium path of the subgames starting after nature's move for the case in which  $b=0$."}

H <-  matrix(c(rep("O", 32), 
rep("X=1, S=1", 8), rep("X=1, S=0", 8), rep("X=0, S=1", 8), rep("X=0, S=0",8 ),
rep(rep(c("C","NC"), each = 4 ), 4),
rep(rep(c("R","NR"), each = 2 ), 8),
rep(c("Y","NY"), 16)), 
 32)[32:1,]

in.history = function(action) rowSums(H==action)>0

P <- cbind(rep(1, 32), 
           rep(2, 32), 
           rep(3, 32), 
           rep(4, 32))[32:1,]
U <- matrix(NA, 32, 4)
U[,2] <- in.history("C") -   
          2*in.history("Y") + 
          2*(in.history("X=0, S=0")+ in.history("X=1, S=0"))*in.history("C") 


# Media gains only when it does reliable story 
U[,3] <- in.history("NR") +   
          2*in.history("R")*in.history("C")*(in.history("X=1, S=0")+ in.history("X=1, S=1")) 


# Voters prefer firing if reports on corrupt politician
U[,4] <- in.history("NY") +   
          2*in.history("Y")*in.history("C")*in.history("R") 

        
gt_tree(H,U,P, player.names = c("Nature", "Gov", "Media", "Voters"),         
  mark.branches=((ncol(H)-1):2),
  print.utilities = c(FALSE, TRUE, TRUE, TRUE),
  force_solution = TRUE, warnings = FALSE)

text(6.6, (1:32)[in.history("Y") & in.history("R")]- .02, expression(italic(+b)) , cex = 1.2) 

```



To draw this  equilibrium as a DAG we include nodes for every action taken, nodes for features that determine the game being played, and the utilities at the end of the game. 

If equilibrium claims are justified by claims about the beliefs of actors then these could also appear as nodes. To be clear however these are not required to represent the game  or the equilibrium, though they can capture assumed logics underlying the equilibrium choice. For instance a theorist might claim that humans are wired so that whenever they are playing a "Stag Hunt" game they play "defect." The game and this solution can be represented on a DAG without reference to the  beliefs of actors about the action of other players. However, if the *justification* for the equilibrium involves optimization given the beliefs of other players, a lower level DAG could represent this by having a node for the  game description that points to beliefs about the actions of others, that then points to choices. In a game with dominant strategies, in contrast, there would be no arrows from these beliefs to actions.

For our running example, nodes could usefully include the politician's expectations, since the government's actions depend on expectations of the actions of others. However, given the game there is no gain from  including the media's expectations of the voter's actions since in this case the media's actions do not depend on expectations of the voters actions then these expectations should be included.  

In Figure \ref{fig:gamedag} we provide two examples of DAGs that illustrate lower level models that support our running example. 

The upper  graph gives a DAG reflecting equilibrium play in the game described in Figure \ref{fig:tree}. Note that in this game there is an arrow between $C$ and $Y$ even though $Y$ does not depend on $C$ for some values of $b$---this is because conditional independence requires that two variables are independent for *all* values of the conditioning set. For simplicity also we mark $S$ and $X$, along with $b$ as features that affect which subgame is being played---taking the subgames starting after Nature's move. Note that the government's expectations of responses by others matters, but the expectations of other players do not matter given this game and solution. Note that the utilities appear twice in a sense. They appear in the subgame node, as they are part of the definition of the game--though here they are the utilities that players expect at each terminal node; when they appear at the end of the DAG they are the utilities that actually arise (in theory at least). 

The lower level DAG  is very low and much more general, representing the theory that in three player games of complete information, players engage in backwards induction and choose the actions that they expect to maximize utility given their beliefs about the actions of others. The DAG assumes that players know what game is being played ("Game"), though this could also be included for more fundamental justification of behavioral predictions. Each action is taken as a function of the beliefs about the game, the expectations about the actions of others, and knowledge of play to date. The functional equations---not shown---are given by optimization and belief formation assuming optimization by others.  


```{r, echo = FALSE, fig.width = 12, fig.height = 10, out.width='\\textwidth', fig.cap = "\\label{fig:gamedag} The upper panel shows a causal graph that describes  relations between nodes suggested by analysis of  the  game  in Figure \\ref{fig:tree} and which can imply the causal graph of  Figure \\ref{fig:running}. The game itself  (or beliefs about the game) appear as a node, which are in turn determined by exogneous factors.   The lower panel represents a still lower level and more general theory ``players use backwards induction in three step games of complete information.''", fig.align="center", warning = FALSE}

par(mfrow = c(2,1))
par(mar=c(1,1,3.5,1))


x = c(0, 1, 2, 2,  3,  3, 4, 5)
y = c(0, 0, 2, -2, 2, -2.5, -2, 0)

names = c("S, X, b",                                        #1 
          "Subgame",                                           #2 
          "E: Gov's Beliefs\nabout responses by\n Media and Voters",    #3
          "Corruption",                                       #4
          "",            #5
          "Report",                                       #6
          "Remove\nGovernment",                                       #7
          "Utilities"                          #8
)

hj_dag(x =  x,
       y = y,
       names = c(names, " ", " "),
       arcs = cbind( c(1,rep(2,5)  ,3, c(4,6,7),  4, 4, 6),
                     c(2,3:4, 6:8,      4,  rep(8, 3), 6, 7, 7)),
       title = "Lower DAG: Backwards induction in a game with 3 players  with one  move  each",
       contraction = .22,
       padding = .5)



x = c(0, 1, 2, 2,  3,  3, 4, 5)
y = c(0, 0, 2, -2, 2, -2.5, -2, 0)

names = c("Context",                                        #1 
          "Game",                                           #2 
          "1's Beliefs\nabout actions \n 2|1 and 3|2,1",    #3
          "Action 1",                                       #4
          "2's Beliefs\nabout actions \n 3|2,1",            #5
          "Action 2",                                       #6
          "Action 3",                                       #7
          "Utilities"                          #8
)

hj_dag(x =  x,
       y = y,
       names = c(names, " ", " "),
       arcs = cbind( c(1,rep(2,6)  ,3, 5, c(4,6,7),  4, 4, 6),
                     c(2,3:8,      4, 6, rep(8, 3), 6, 7, 7)),
       title = "Still lower: Backwards induction, 3 player game with one  move for each player",
       contraction = .2,
       padding = .5)




```

These lower level graphs can themselves provide clues for assessing relations in the higher level graphs. For instance, the lower level model might specify that the value of $b$ in the game affects the actions of the government only through their beliefs about the behavior of voters, $E$. These beliefs may themselves have a stochastic component, $U_E$. Thus  $b$ high  might be thought to reduce the effect of media on corruption. For instance if $b \in \mathbb{R}_+$, we have $C= 1-FG(1-\mathbb{1}(b>1))$. If $X$ is unobserved and one is interested in whether $S=0$ caused corruption, knowledge of $b$ is informative. It is a root node in the causal estimand. If $b>1$ then $S=0$ did not cause corruption. However if $b$ matters only because of its effect on $E$ then the query depends on $U_E$.  In this case, while knowing $b$ is informative about whether $S=0$ caused $C=1$, knowing $E$ from the lower level graph is more informative.

Note that the  model we have examined here involves no terms for $U_C$, $U_R$ and $U_Y$---that is, shocks to outcomes given action. Yet clearly any of these could exist. One could imagine a version of this game with "trembling hands," such that errors are always made with some small probability, giving rise to a much richer set of predictions.  These can be  represented in the game tree as moves by nature between actions chosen and outcomes realized. Importantly in a strategic environment such noise could give rise to different types of conditional independence. For instance say that a Free Press only published its report on corruption with  probability $\pi^R$, then with $\pi^R$ high enough the sensitive government might decide it is worth engaging in corruption even if there is a free press; in this case the arrow from $X$ to $C$ would be removed. Interestingly in this case as the error rate rises, $R$ becomes less likely, meaning that the effect of a $S$ on $Y$ becomes gradually weaker (since governments that are not sensitive become  more likely to survive) and then drops to 0 as sensitive governments start acting just like nonsensitive governments. 
